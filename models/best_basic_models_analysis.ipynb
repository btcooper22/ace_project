{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   },
   "source": [
    "# Initial Models: Analysis and Features\n",
    "\n",
    "The following is a short analysis of the models that have performed best on\n",
    "the initial ACE dataset, and an explanation of the features of the data the\n",
    "model highlights as explanatory / predictive of the need for hospital treatment\n",
    "\n",
    "## Data Preparation Methods\n",
    "\n",
    "As part of the training pipeline, I have tried a number of different\n",
    "approaches to data preparation. These are worth mentioning quickly, as they\n",
    "have a huge bearing on the performance of each model. The discussion can be\n",
    "sub-divided into two major considerations:\n",
    "\n",
    "### 1. Categorical Encoding Methods:\n",
    "\n",
    "Machine learning methods require categorical data to be represented\n",
    "numerically for it to be interpretable. There are a number of ways of doing this, some of which are not possible in this setting because of the small amount of training data. I've focussed on two approaches:\n",
    "\n",
    "**One Hot Encoding** - Each categorical feature is split into\n",
    "individual categories and these categories are assigned a binary value, a\n",
    "1 indicating the feature is present and 0 not present. For example, if we had\n",
    " the following data on the time of referral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "  Referral Time\n1       morning\n2     afternoon\n3       morning\n4       evening",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>morning</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afternoon</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>morning</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>evening</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time\": [\"morning\", \"afternoon\", \"morning\", \"evening\"],\n",
    "}, index=[1,2,3,4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "could be one-hot encoded as follows:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-52106fca2cb7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-4-52106fca2cb7>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    could be one hot encoded as follows:\u001B[0m\n\u001B[0m          ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   Referral Time Morning  Referral Time Afternoon  Referral Time Evening\n1                      1                        0                      0\n2                      0                        1                      0\n3                      1                        0                      0\n4                      0                        0                      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time Morning</th>\n      <th>Referral Time Afternoon</th>\n      <th>Referral Time Evening</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time Morning\": [1, 0, 1, 0],\n",
    "    \"Referral Time Afternoon\": [0, 1, 0, 0],\n",
    "    \"Referral Time Evening\": [0, 0, 0, 1],\n",
    "}, index=[1,2,3,4])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An issue with one-hot encoding is the creation of a large number of extra\n",
    "features (one for each level of each categorical feature) i.e. the above took\n",
    " one category and made it into three. This crates a very \"sparse\" dataset\n",
    " (contains a lot of zeros that don't add much info) and can result in a very\n",
    " sparse model of the data i.e. a tree model that has to make hundreds\n",
    " of yes / no decisions on different binary categories before it can make a\n",
    " prediction. An alternative to this approach is to encode each category with\n",
    " a numerical representation of its value.\n",
    "\n",
    "**Mean encoding / Feature encoding**\n",
    "\n",
    "\n",
    " Target encoding takes the target feature, in this case the need for hospital\n",
    "  treatment, and encodes each categorical feature with the mean / proportion\n",
    "  that applies to the individual \"levels\" of that category. Using the above example, we would calculate the proportion of referrals made in the morning / afternoon / evening that required hospital treatment, and use those proportions as numerical representations of the features. For example, if 15% of children referred in the morning required hospital treatment, and 5% and 18% for the kids referred in the afternoon and evening required hospital treatment, then the feature would look like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  Referral Time  Target Encoded Referral Time\n1       morning                          0.15\n2     afternoon                          0.05\n3       morning                          0.18\n4       evening                          0.15",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time</th>\n      <th>Target Encoded Referral Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>morning</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afternoon</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>morning</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>evening</td>\n      <td>0.15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time\": [\"morning\", \"afternoon\", \"morning\", \"evening\"],\n",
    "    \"Target Encoded Referral Time\": [.15, .05, .18,\n",
    "                                   .15],\n",
    "}, index=[1,2,3,4])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: One must be careful when using this approach, that \"leakage\" isn't\n",
    "introduced\n",
    " into the dataset - that is, information about the target feature for that\n",
    " example being included in the explanatory variables for the same example.\n",
    " This can be avoided by ensuring that the target value for each example is\n",
    " left out when calculating its encodings.\n",
    "\n",
    "### 2. Balancing Positive / Negative Examples:\n",
    "\n",
    "The ACE dataset is heavily imbalanced i.e. only 16.5% of examples require\n",
    "hospital treatment. Left as is, models can easily achieve high (83.5%)\n",
    "accuracy by simply predicting ALL children can be treated by ACE. This\n",
    "wouldn't be a very useful model!\n",
    "\n",
    "To avoid this, efforts need to be made to balance the predictions made by\n",
    "each model. Again, I have used two basic approaches to achieve this:\n",
    "\n",
    "**1. Weighting Labels**:\n",
    "\n",
    "The penalty a model is given for making an incorrect prediction can be\n",
    "weighted to penalise the minority label incorrect guesses more\n",
    "heavily. This discourages the model from simply guessing the majority label\n",
    "over and over, as it gets a heavier penalty when it does so incorrectly. The\n",
    "weight is usually chosen to be proportional to the imbalance i.e. if there\n",
    "are 5 times more negative examples than positive, then an incorrect negative\n",
    "guess is penalised 5 times more than an incorrect positive.\n",
    "\n",
    "**2. SMOTE - Synthetic Minority Oversampling TEchnique**\n",
    "\n",
    "This uses a statistical model to create synthetic examples from the minority\n",
    "label to balance the number of positive / negative examples 50/50. The\n",
    "simplest form of oversampling is to simply duplicate the minority examples\n",
    "over and over. SMOTE uses interpolation between the different minority\n",
    "examples to create synthetic examples that resemble the distribution of the\n",
    "originals.\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "The following metrics are used to measure model performance:\n",
    "\n",
    "* **True Positive / False Positive / True Negative / False Negative**: Fairly\n",
    "self\n",
    " explanatory. A true positive in this context is an example a model correctly\n",
    "  states requires hospital treatment, a true negative is an example the model\n",
    "   states needs hospital treatment when it doesn't, and so on....\n",
    "* **Accuracy**: Again fairly self explanatory. The proportion of\n",
    " correct predictions\n",
    "* **Precision**: the proportion of positive guesses that are\n",
    "correct i.e. if a model has a precision of 75%, 3 out of every 4 times it\n",
    "predicts that hospital treatment is needed it is correct.\n",
    "* **Recall**: the proportion of positive examples in the dataset that the\n",
    "model correctly predicts i.e. if there are 50 examples requiring hospital\n",
    "treatment and the model correctly identifies 40 of them, it has an 80% recall.\n",
    "* **ROC/AUC**: this is a measure of the tradeoff between precision and\n",
    "recall, but is a little complex to define here. A 0.5 ROC/AUC is\n",
    "representative of random chance and 1 is a perfect model.\n",
    "* **F1 Score**: the f1 score is another measure of the tradeoff between precision and recall. It is a weighted average of the two and ranges from 0 (worst) to 1 (perfect)\n",
    "\n",
    "## Models and Performance:\n",
    "\n",
    "Different iterations of each model were evaluated based on a technique called\n",
    "cross\n",
    "validation -\n",
    "this samples the training data and holds back a certain proportion to test\n",
    "model predictions, ensuring the model is never evaluated on examples it has\n",
    "already seen. These scores were used to select the best parameters for each\n",
    "model, and as a guide to overall performance. Overall test scores were taken\n",
    "from a holdout test set that was not used at any point during the training\n",
    "process to ensure an unbiased estimate of how well a model can generalise to\n",
    "new data.\n",
    "\n",
    "The best models were trained with **one-hot encoded** data with **weighted labels** used to balance the dataset. It is likely that the mean / target encoded data, and the synthetic examples were causing the models to overfit to the training data and thus not generalise well to new examples not seen in training. The following are the scores for the one-hot encoded examples with weighted labels :"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}