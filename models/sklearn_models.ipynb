{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (make_scorer, confusion_matrix, precision_score,\n",
    "                             f1_score, roc_auc_score, accuracy_score,\n",
    "                             recall_score)\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/samrelins/Documents/LIDA/ace_project/')\n",
    "from src.data_prep import *\n",
    "from src.train_test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Models: Analysis and Features\n",
    "\n",
    "The below is a short(ish) script and summary of the data preparation and\n",
    "modelling experiments I have performed. This is to be followed by a more in-depth analysis of a small selection of the best performing models - although, as we'll see, none of the models show impressive predictive accuracy at this point.\n",
    "\n",
    "## Data Preparation Methods\n",
    "\n",
    "The ACE dataset presents a couple of prominent considerations that need to be\n",
    " accounted for when preparing the data for modeling:\n",
    "\n",
    "### 1. Categorical Encoding Methods:\n",
    "\n",
    "Machine learning methods require categorical data to be represented\n",
    "numerically for it to be interpretable. There are a number of ways to\n",
    "approach\n",
    "this, some of which are not possible in this setting because of the small amount of training data. I've focussed on two approaches:\n",
    "\n",
    "**One Hot Encoding** - Each categorical feature is split into\n",
    "individual categories and these categories are assigned a binary value, a\n",
    "1 indicating the feature is present and 0 not present. For example, if we had\n",
    " the following data on the time of referral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Referral Time\n1       morning\n2     afternoon\n3       morning\n4       evening",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>morning</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afternoon</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>morning</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>evening</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time\": [\"morning\", \"afternoon\", \"morning\", \"evening\"],\n",
    "}, index=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could be one-hot encoded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Referral Time Morning  Referral Time Afternoon  Referral Time Evening\n1                      1                        0                      0\n2                      0                        1                      0\n3                      1                        0                      0\n4                      0                        0                      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time Morning</th>\n      <th>Referral Time Afternoon</th>\n      <th>Referral Time Evening</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time Morning\": [1, 0, 1, 0],\n",
    "    \"Referral Time Afternoon\": [0, 1, 0, 0],\n",
    "    \"Referral Time Evening\": [0, 0, 0, 1],\n",
    "}, index=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue with one-hot encoding is the creation of a large number of extra\n",
    "features (one for each level of each categorical feature) i.e. the above took\n",
    " one category and made it into three. This crates a very \"sparse\" dataset\n",
    " (contains a lot of zeros that don't add much info) and can result in a very\n",
    " sparse model of the data i.e. a tree model that has to make hundreds\n",
    " of yes / no decisions on different binary categories before it can make a\n",
    " prediction. An alternative to this approach is to encode each category with\n",
    " a numerical representation of its value.\n",
    "\n",
    "**Mean encoding / Feature encoding**\n",
    "\n",
    "\n",
    " Target encoding takes the target feature, in this case the need for hospital\n",
    "  treatment, and encodes each categorical feature with the mean / proportion\n",
    "  that applies to the individual \"levels\" of that category. Using the above example, we would calculate the proportion of referrals made in the morning / afternoon / evening that required hospital treatment, and use those proportions as numerical representations of the features. For example, if 15% of children referred in the morning required hospital treatment, and 5% and 18% for the kids referred in the afternoon and evening required hospital treatment, then the feature would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Referral Time  Target Encoded Referral Time\n1       morning                          0.15\n2     afternoon                          0.05\n3       morning                          0.18\n4       evening                          0.15",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time</th>\n      <th>Target Encoded Referral Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>morning</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afternoon</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>morning</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>evening</td>\n      <td>0.15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time\": [\"morning\", \"afternoon\", \"morning\", \"evening\"],\n",
    "    \"Target Encoded Referral Time\": [.15, .05, .18,\n",
    "                                   .15],\n",
    "}, index=[1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: One must be careful when using this approach, that \"leakage\" isn't\n",
    "introduced\n",
    " into the dataset - that is, information about the target feature for that\n",
    " example being included in the explanatory variables for the same example.\n",
    " This can be avoided by ensuring that the target value for each example is\n",
    " left out when calculating its encodings.\n",
    "\n",
    "Target encoding fixes the sparcity issue - each categorical feature remains\n",
    "one feature rather than expanding, but it often results in overfitting - that\n",
    " is, when the model maps too closely to the examples it has seen in training\n",
    " and doesn't then generalise well when given new data.\n",
    "\n",
    "### 2. Balancing Positive / Negative Examples:\n",
    "\n",
    "The ACE dataset is heavily imbalanced i.e. only 16.5% of examples require\n",
    "hospital treatment. Left as is, models can easily achieve high (83.5%)\n",
    "accuracy by simply predicting ALL children can be treated by ACE. This\n",
    "wouldn't be a very useful model!\n",
    "\n",
    "To avoid this, efforts need to be made to balance the predictions made by\n",
    "each model. Again, I have used two basic approaches to achieve this:\n",
    "\n",
    "**1. Weighting Labels**:\n",
    "\n",
    "The penalty a model is given for making an incorrect prediction can be\n",
    "weighted to penalise the minority label incorrect guesses more\n",
    "heavily. This discourages the model from simply guessing the majority label\n",
    "over and over, as it gets a heavier penalty when it gets one of the minority\n",
    "examples wrong.\n",
    " The\n",
    "weight is usually chosen to be proportional to the imbalance i.e. if there\n",
    "are 5 times more negative examples (children that can be treated by ACE) than\n",
    "positive (children that require hospital treatment), then\n",
    " an incorrect\n",
    "negative\n",
    "guess is penalised 5 times more than an incorrect positive.\n",
    "\n",
    "**2. SMOTE - Synthetic Minority Oversampling TEchnique**\n",
    "\n",
    "This uses a statistical model to create synthetic examples from the minority\n",
    "label to balance the number of positive / negative examples to 50/50. The\n",
    "simplest form of oversampling is to simply duplicate the minority examples\n",
    "over and over. SMOTE uses interpolation between the different minority\n",
    "examples to create synthetic examples that roughly preserve the distribution of\n",
    " the\n",
    "original examples.\n",
    "\n",
    "### Data Preparation Pipeline\n",
    "\n",
    "I've spent some time developing a \"pipeline\" or group of functions that can\n",
    "automatically apply the above encoding and balancing techniques to the\n",
    "data \"at the flick of a switch\". This means that, during training, the\n",
    "different data preparation methods can be easily and consistently\n",
    "applied to the data at runtime, without having to store many different versions\n",
    " of the ace dataset. The importance of this will become clear in the discussion of model\n",
    "evaluation and cross validation below. Given the general utility of these\n",
    "functions, and the fact they are fairly verbose, I have extracted them into a\n",
    " separate module - `data_prep.py` in which script and detailed documentation\n",
    " can be found.\n",
    "\n",
    "A quick summary of the pipeline functions is as follows:\n",
    "\n",
    "* `clean_data`: converts raw excel / csv data into a more python friendly format\n",
    "* `fill_nas`: fills missing values in dataset with group means\n",
    "* `add_features`: add the extra categorical features discussed in the data\n",
    "analysis\n",
    "* `return_train_test`: divides the dataset into consistent train and test\n",
    "dataframes\n",
    "* `add_synthetic_examples`: generates SMOTE (synthetic) examples and adds to\n",
    "dataset\n",
    "* `encode_and_scale`: applies various categorical encoding techniques and\n",
    "min/max scales the data (for modelling techniques that require scaled data)\n",
    "\n",
    "## Model Training and Evaluating Performance:\n",
    "\n",
    "Having considered data preparation, we now need to define models to predict\n",
    "the\n",
    "hospital /\n",
    " community outcomes.\n",
    "\n",
    "### Models and Parameters\n",
    "\n",
    "The modelling techniques used are too numerous to attempt any discussion\n",
    "here, but further exploration of the most successful techniques will be\n",
    "included in\n",
    " the\n",
    "more detailed discussion that will follow. Each modelling technique\n",
    "includes a number of\n",
    "parameters or \"assumptions\" that need to be specified when defining the model\n",
    " , and have a downstream effect on performance and prediction\n",
    " accuracy. To simplify and compartmentalise each of the models we wish to\n",
    " test along with its parameters (an extension of Ruaridh's work), we have\n",
    " a number of functions that return a model and a \"parameter grid\" of\n",
    "  each of the parameters we wish to test.\n",
    "\n",
    "Note: each function has a \"balanced\" argument allowing for the\n",
    "calculation and use of balanced weights, which is not implemented in the\n",
    "parameters of some models (hence the separate `scaled` keyword argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def return_knn_params(balanced=False):\n",
    "    clf = KNeighborsClassifier()\n",
    "    # param_grid = {'n_neighbors': np.arange(1,10),\n",
    "    #                 'weights': ['uniform','distance'],\n",
    "    #                 'p': [1,2],\n",
    "    #                 'metric':['minkowski','euclidean','manhattan'],\n",
    "    #                 'n_jobs':[-2]}\n",
    "    param_grid = {'n_neighbors': [3],\n",
    "                    'weights': ['uniform'],\n",
    "                    'p': [1,],\n",
    "                    'metric':['minkowski'],\n",
    "                    'n_jobs':[-2]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for nearest neighbours\")\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def return_svm_params(balanced=False):\n",
    "    clf = SVC()\n",
    "    # param_grid = {'kernel': ['linear','rbf'],\n",
    "    #               'C': np.logspace(2,4,2), # np.logspace(2,5,6)\n",
    "    #               'gamma': np.logspace(-4,0.5,1)} # np.logspace(-4,0.5,10)}\n",
    "    param_grid = {'kernel': ['linear'],\n",
    "                  'C': [0.1], # np.logspace(2,5,6)\n",
    "                  'gamma': [0.1]} # np.logspace(-4,0.5,10)}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "def return_gaussian_process_params(balanced=False):\n",
    "    clf = GaussianProcessClassifier(random_state=0, n_jobs=-2)\n",
    "    kernels = [mul * RBF(length_scale)\n",
    "                    for mul in np.arange(0.5, 2.5, 0.5)\n",
    "                    for length_scale in np.arange(0.5, 2.5, 0.5)]\n",
    "    param_grid = {'kernel': kernels,\n",
    "                  'n_jobs': [-2]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for Gaussian Process\")\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def return_random_forest_params(balanced=False):\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    # param_grid = {'max_depth': [4, 6, 10, 14, 20],\n",
    "    #               'n_estimators': [30, 100, 130, 300],\n",
    "    #               'min_samples_split': [2, 3, 10, 13, 30],\n",
    "    #               'max_features': [0.3, 0.4, 0.5, \"auto\"],\n",
    "    #               'n_jobs': [-2]}\n",
    "    param_grid = {'max_depth': [4],\n",
    "                  'n_estimators': [30],\n",
    "                  'min_samples_split': [2],\n",
    "                  'max_features': [0.3],\n",
    "                  'n_jobs': [-2]}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def return_grad_boost_params(balanced=False):\n",
    "    clf = GradientBoostingClassifier(n_estimators=100,random_state=0)\n",
    "    # param_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "    #               'n_estimators': [30, 100, 130, 300],\n",
    "    #               'max_depth': [4, 6, 10, 14, 20],\n",
    "    #               'min_samples_split': [3, 10, 13, 30],\n",
    "    #               'max_features': [x for x in np.linspace(0.2,0.4,4)]}\n",
    "    param_grid = {'learning_rate': [0.1],\n",
    "                  'n_estimators': [30],\n",
    "                  'max_depth': [4],\n",
    "                  'min_samples_split': [3],\n",
    "                  'max_features': [3]}\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": False,\n",
    "            \"weight_y\": balanced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def return_ada_boost_params(balanced=False):\n",
    "    clf = AdaBoostClassifier(random_state=0)\n",
    "    # param_grid = {'n_estimators': [30, 100, 130, 300],\n",
    "    #               'learning_rate': [0.001,0.01,0.1,0.2,0.5]}\n",
    "    param_grid = {'n_estimators': [30],\n",
    "                  'learning_rate': [0.001]}\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": False,\n",
    "            \"weight_y\":balanced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def return_naive_bayes_params(balanced=False):\n",
    "    clf = GaussianNB()\n",
    "    # param_grid = {'var_smoothing':  np.logspace(-11,-3,9,base=10)}\n",
    "    param_grid = {'var_smoothing': [0.3]}\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": False,\n",
    "            \"weight_y\":balanced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def return_lr_params(balanced=False):\n",
    "    clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "    # param_grid = {'penalty' : ['l2'],\n",
    "    #               'solver': [\"liblinear\"],\n",
    "    #               'C' : np.logspace(-4, 4, 20)}\n",
    "    param_grid = {'penalty' : ['l2'],\n",
    "                  'solver': [\"liblinear\"],\n",
    "                  'C' : [0.1]}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "def return_qda_params(balanced=False):\n",
    "    clf = QuadraticDiscriminantAnalysis()\n",
    "    # param_grid = {'reg_param':  [0.0, 0.01, 0.03, 0.1, 0.3]}\n",
    "    param_grid = {'reg_param':  [0.0]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for QDA\")\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def return_lda_params(balanced=False):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    # param_grid = {'solver':  [\"svd\", \"lsqr\", \"eigen\"],\n",
    "    #               \"shrinkage\": [None, \"auto\", 0.1, 0.3, 0.8, 1]}\n",
    "    param_grid = {'solver':  [\"svd\"],\n",
    "                  \"shrinkage\": [None]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for LDA\")\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cross Validation and Grid Search\n",
    "\n",
    "For those keeping score, we now have several different data preparation techniques,\n",
    "models. We can't use them all simultaneously (well - technically that is\n",
    "actually possible but would be a needlessly complex solution) and so we need\n",
    "to decide on the best combination. To evaluate each possible permutation of\n",
    "data preparation / model / parameters we\n",
    "can use a technique called \"cross validation\": dividing the\n",
    "training data into k\n",
    "groups, training the model on k-1 of these groups leaving one group aside, and\n",
    " evaluating the model's predictions against the group held aside. Doing this\n",
    " ensures the model is never evaluated on examples it has already seen. This\n",
    " technique is used in conjunction with a parameter optimisation method called\n",
    "  \"grid search\" - this iterates through each possible combination of the\n",
    "  specified parameters, scoring each individually. The best combination of\n",
    "  parameters can then be established.\n",
    "\n",
    "There are several \"out of the box\" implementations of these methods that can\n",
    "be applied in most use cases. However, these functions require you to specify\n",
    " a pipeline that can be compartmentalised into distinct stages and applied\n",
    " across the whole training dataset. This isn't practical in this case as:\n",
    "\n",
    " * The synthesizing and encoding stages can't be divorced from one another,\n",
    " otherwise the SMOTE examples may extrapolate between target-encoded\n",
    " features to produce nonsense examples\n",
    " * Only the training splits can include synthetic data - otherwise model\n",
    " performance will be based in part on its ability to predict synthetic data\n",
    " and will result in a biased cross validation score\n",
    "\n",
    "Therefore, I've spent some time developing a custom cross validation and grid\n",
    " search loop that produces unbiased validation scores. The function includes\n",
    " the different data preparation techniques and pipeline functions discussed\n",
    " above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import (make_scorer, confusion_matrix, precision_score,\n",
    "                             f1_score, roc_auc_score, accuracy_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/samrelins/Documents/LIDA/ace_project/')\n",
    "from src.data_prep import *\n",
    "\n",
    "# custom scoring functions for CV loop\n",
    "true_neg = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0][0])\n",
    "false_neg = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[1][0])\n",
    "true_pos = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[1][1])\n",
    "false_pos = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0][1])\n",
    "precision = make_scorer(precision_score, zero_division=0)\n",
    "\n",
    "# dict of scoring functions\n",
    "SCORING = {\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score),\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"true_pos\": true_pos,\n",
    "    \"true_neg\": true_neg,\n",
    "    \"false_pos\": false_pos,\n",
    "    \"false_neg\": false_neg\n",
    "}\n",
    "\n",
    "\n",
    "def score_classifier(clf, X, y):\n",
    "    \"\"\"\n",
    "    Scores a classifier against metrics in SCORING dict\n",
    "\n",
    "    :param clf: (object: sklearn classifier) classifier to be scored\n",
    "    :param X: (object: pandas DataFrame) matrix of training vectors\n",
    "    :param y: (object: pandas Series) vector of target labels\n",
    "    :return: (dict) group of {score function name: score} pairs\n",
    "    \"\"\"\n",
    "\n",
    "    scores = {}\n",
    "    for name, scorer in SCORING.items():\n",
    "        scores[name] = scorer(clf, X, y)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def cv_score_classifier(clf, X_train, y_train, params,\n",
    "                        cat_encoder=\"one_hot\",\n",
    "                        add_synthetic=False,\n",
    "                        scaled=False,\n",
    "                        n_splits=3,\n",
    "                        weight_y=False):\n",
    "    \"\"\"\n",
    "    Custom CV loop to score classifier functions\n",
    "\n",
    "    Implemented to account for SMOTE example generation and target encoding -\n",
    "    both should only be performed on training data and not validation data -\n",
    "    not possible to achieve this separation using the sklearn pipeline and\n",
    "    GridSearchCV.\n",
    "\n",
    "    :param clf: (object: sklearn classifier) classifier to train and score\n",
    "    :param X_train: (object: pandas DataFrame) Explanatory Training data\n",
    "    :param y_train: (object: pandas Series) Training data labels\n",
    "    :param params: (dict) parameters for classifier\n",
    "    :param cat_encoder: (str: \"one_hot\") categorical encoder for data\n",
    "    either \"one_hot\" / \"target\"\n",
    "    :param add_synthetic: (bool: False) set True to add SMOTE examples before\n",
    "    training\n",
    "    :param scaled: (bool: False) set True to scale numeric features\n",
    "    :param n_splits: (int: 3) number of splilts for CV loop\n",
    "    :param weight_y: (bool: False) set True if clf requires sample_weights\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # create splits for CV loop\n",
    "    splitter = StratifiedKFold(n_splits=n_splits, random_state=1)\n",
    "    splits = list(splitter.split(X_train, y_train))\n",
    "\n",
    "    total_cv_scores = {} # dict to store cumulative CV scores\n",
    "    for train_idxs, val_idxs  in splits:\n",
    "        # divide data into train and validation sets for this cv loop\n",
    "        cv_X_train, cv_y_train, X_val, y_val = (X_train.iloc[train_idxs],\n",
    "                                                y_train.iloc[train_idxs],\n",
    "                                                X_train.iloc[val_idxs],\n",
    "                                                y_train.iloc[val_idxs])\n",
    "\n",
    "        if add_synthetic: # add SMOTE examples to balance data if required\n",
    "            cv_X_train, cv_y_train = add_synthetic_examples(cv_X_train, cv_y_train)\n",
    "\n",
    "        # encode categorical features and scale numeric if required\n",
    "        cv_X_train, X_val, = encode_and_scale(\n",
    "            cv_X_train, cv_y_train, X_val,\n",
    "            cat_encoder=cat_encoder,\n",
    "            scaled=scaled)\n",
    "\n",
    "        if weight_y:\n",
    "            # calculate array of weights for y labels\n",
    "            pos_weight, neg_weight = compute_class_weight(\n",
    "                class_weight=\"balanced\",\n",
    "                classes=[1,0],\n",
    "                y=cv_y_train)\n",
    "            y_weights = cv_y_train.apply(lambda y: pos_weight if y else neg_weight)\n",
    "            # train model using parameters, weights and cv loop data\n",
    "            cv_clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(cv_X_train, cv_y_train, sample_weight=y_weights))\n",
    "        else:\n",
    "            # train model using parameters and cv loop data\n",
    "            cv_clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(cv_X_train, cv_y_train))\n",
    "\n",
    "        # score classifier on cv validation set and add scores to total\n",
    "        scores = score_classifier(cv_clf, X_val, y_val)\n",
    "        if total_cv_scores:\n",
    "            for key, value in scores.items():\n",
    "                total_cv_scores[key] += value\n",
    "        else:\n",
    "            total_cv_scores = scores\n",
    "\n",
    "    mean_cv_scores = {}\n",
    "    for key, value in total_cv_scores.items():\n",
    "        mean_cv_scores[key] = value / n_splits\n",
    "\n",
    "    return mean_cv_scores\n",
    "\n",
    "\n",
    "def param_search_classifier(param_grid, **kwargs):\n",
    "    \"\"\"\n",
    "    custom param grid search to compliment cv_score_classifier function\n",
    "\n",
    "    :param param_grid: (dict) parameters on which to perform grid search\n",
    "    :param kwargs: arguments for cv_score_classifier function\n",
    "    :return: (dict: best_scores, dict: best_params) scores and parameters for\n",
    "    highest scoring model\n",
    "    \"\"\"\n",
    "    param_grid = ParameterGrid(param_grid)\n",
    "    # variable to store best param combo and relevant scores\n",
    "    best_scores = {}\n",
    "    best_params = {}\n",
    "    for params in param_grid:\n",
    "        mean_cv_scores = cv_score_classifier(params=params,\n",
    "                                             **kwargs)\n",
    "        if not best_scores:\n",
    "            best_scores = mean_cv_scores\n",
    "            best_params = params\n",
    "        elif mean_cv_scores[\"f1\"] > best_scores[\"f1\"]:\n",
    "            best_scores = mean_cv_scores\n",
    "            best_params = params\n",
    "\n",
    "    return best_scores, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Metrics\n",
    "\n",
    "The cross validation loop outputs the following metrics, used to measure\n",
    "model performance:\n",
    "\n",
    "* **True Positive / False Positive / True Negative / False Negative**: Fairly\n",
    "self\n",
    " explanatory. A true positive in this context is an example a model correctly\n",
    "  states requires hospital treatment, a true negative is an example the model\n",
    "   states needs hospital treatment when it doesn't, and so on....\n",
    "* **Accuracy**: Again fairly self explanatory. The proportion of\n",
    " correct predictions\n",
    "* **Precision**: the proportion of positive guesses that are\n",
    "correct i.e. if a model has a precision of 75%, 3 out of every 4 times it\n",
    "predicts that hospital treatment is needed it is correct.\n",
    "* **Recall**: the proportion of positive examples in the dataset that the\n",
    "model correctly predicts i.e. if there are 50 examples requiring hospital\n",
    "treatment and the model correctly identifies 40 of them, it has an 80% recall.\n",
    "* **ROC/AUC**: this is a measure of the tradeoff between precision and\n",
    "recall, but is a little complex to define here. A 0.5 ROC/AUC is\n",
    "representative of random chance and 1 is a perfect model.\n",
    "* **F1 Score**: the f1 score is another measure of the tradeoff between precision and recall. It is a weighted average of the two and ranges from 0 (worst) to 1 (perfect)\n",
    "\n",
    "## Tests\n",
    "\n",
    "The following is the (perhaps long awaited!) output from the training /\n",
    "validation. Scores are broken down by data preparation methods and then model\n",
    " type - the best performing model for each is selected from the cv loop and\n",
    " displayed in the results:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required features missing to run <function add_free_text_features at 0x7fe766f93e50>\n",
      "==================================================\n",
      "Data Prep: one_hot_balanced\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "no available balancing technique for nearest neighbours\n",
      "==================================================\n",
      "Testing KNeighborsClassifier() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "==================================================\n",
      "Testing SVC() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "no available balancing technique for Gaussian Process\n",
      "==================================================\n",
      "Testing GaussianProcessClassifier(n_jobs=-2, random_state=0) classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "==================================================\n",
      "Testing RandomForestClassifier() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "==================================================\n",
      "Testing GradientBoostingClassifier(random_state=0) classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "==================================================\n",
      "Testing AdaBoostClassifier(random_state=0) classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "==================================================\n",
      "Testing GaussianNB() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "==================================================\n",
      "Testing LogisticRegression(max_iter=10000, random_state=0) classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "no available balancing technique for QDA\n",
      "==================================================\n",
      "Testing QuadraticDiscriminantAnalysis() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "==================================================\n",
      "Data Prep: target_balanced\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "no available balancing technique for nearest neighbours\n",
      "==================================================\n",
      "Testing KNeighborsClassifier() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "==================================================\n",
      "Testing SVC() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "no available balancing technique for Gaussian Process\n",
      "==================================================\n",
      "Testing GaussianProcessClassifier(n_jobs=-2, random_state=0) classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "==================================================\n",
      "Testing RandomForestClassifier() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "==================================================\n",
      "Testing GradientBoostingClassifier(random_state=0) classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "==================================================\n",
      "Testing AdaBoostClassifier(random_state=0) classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "==================================================\n",
      "Testing GaussianNB() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "==================================================\n",
      "Testing LogisticRegression(max_iter=10000, random_state=0) classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "no available balancing technique for QDA\n",
      "==================================================\n",
      "Testing QuadraticDiscriminantAnalysis() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "==================================================\n",
      "Data Prep: one_hot_resampled\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "==================================================\n",
      "Testing KNeighborsClassifier() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "==================================================\n",
      "Testing SVC() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "==================================================\n",
      "Testing GaussianProcessClassifier(n_jobs=-2, random_state=0) classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "==================================================\n",
      "Testing RandomForestClassifier() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "==================================================\n",
      "Testing GradientBoostingClassifier(random_state=0) classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "==================================================\n",
      "Testing AdaBoostClassifier(random_state=0) classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "==================================================\n",
      "Testing GaussianNB() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "==================================================\n",
      "Testing LogisticRegression(max_iter=10000, random_state=0) classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "==================================================\n",
      "Testing QuadraticDiscriminantAnalysis() classifier with one_hot encoded features.\n",
      "==================================================\n",
      "done.\n",
      "==================================================\n",
      "Data Prep: target_resampled\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "==================================================\n",
      "Testing KNeighborsClassifier() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "==================================================\n",
      "Testing SVC() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "==================================================\n",
      "Testing GaussianProcessClassifier(n_jobs=-2, random_state=0) classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "==================================================\n",
      "Testing RandomForestClassifier() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "==================================================\n",
      "Testing GradientBoostingClassifier(random_state=0) classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "==================================================\n",
      "Testing AdaBoostClassifier(random_state=0) classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "==================================================\n",
      "Testing GaussianNB() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "==================================================\n",
      "Testing LogisticRegression(max_iter=10000, random_state=0) classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "==================================================\n",
      "Testing QuadraticDiscriminantAnalysis() classifier with target encoded features.\n",
      "==================================================\n",
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.21it/s]\n",
      "100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.77it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.67it/s]\n",
      "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.37it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "100%|██████████| 16/16 [00:18<00:00,  1.17s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "techniques_dict = {'K Nearest Neighbours': return_knn_params,\n",
    "                   'Support Vector Machines': return_svm_params,\n",
    "                   'Gaussian Process': return_gaussian_process_params,\n",
    "                   'Random Forest Classifier': return_random_forest_params,\n",
    "                   'Gradient Boosting Classifier': return_grad_boost_params,\n",
    "                   'Ada Boost classifier': return_ada_boost_params,\n",
    "                   'Gaussian Naieve Bayes': return_naive_bayes_params,\n",
    "                   'Logistic Regression': return_lr_params,\n",
    "                   'Quadratic Discriminant Analysis': return_qda_params}\n",
    "\n",
    "data_prep_types = [\"one_hot_balanced\", \"target_balanced\",\n",
    "                    \"one_hot_resampled\", \"target_resampled\"]\n",
    "\n",
    "# data_loc = \"/Users/samrelins/Documents/LIDA/ace_project/data/ace_data_orig.csv\"\n",
    "data_loc = \"/Users/samrelins/Documents/LIDA/ace_project/data/ace_data_extra.csv\"\n",
    "ace_data_orig = pd.read_csv(data_loc)\n",
    "ace_data_orig.drop([\"medical_history\", \"examination_summary\",\n",
    "                    \"recommendation\"],\n",
    "                   axis=1, inplace=True)\n",
    "X_train, y_train, X_test, y_test = return_train_test(ace_data_orig)\n",
    "\n",
    "best_params = {}\n",
    "best_model_scores = {}\n",
    "\n",
    "for data_prep_type in data_prep_types:\n",
    "    ### uncomment this and other print statements for output of loop progress\n",
    "    print(50* '=')\n",
    "    print(f\"Data Prep: {data_prep_type}\")\n",
    "    print(50* '=')\n",
    "\n",
    "    cat_encoder = \"one_hot\" if \"one_hot\" in data_prep_type else \"target\"\n",
    "    balanced = True if \"balanced\" in data_prep_type else False\n",
    "    resample = \"undersample\" if \"undersample\" in data_prep_type else None\n",
    "    resample = \"smote\" if \"smote\" in data_prep_type else resample\n",
    "\n",
    "    scores_list = []\n",
    "    best_loop_params = {}\n",
    "    for model_name, model_params_f in techniques_dict.items():\n",
    "        print(f\"fitting {model_name}......\")\n",
    "        model_best_scores, model_best_params = param_search_classifier(\n",
    "            **model_params_f(balanced=balanced),\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            cat_encoder=cat_encoder,\n",
    "            resample=resample,\n",
    "            verbose=False)\n",
    "        scores_list.append(model_best_scores)\n",
    "        best_loop_params[model_name] = model_best_params\n",
    "        print(\"done.\")\n",
    "\n",
    "    best_model_scores[data_prep_type] = pd.DataFrame(scores_list,\n",
    "                                                     index=techniques_dict.keys())\n",
    "    best_params[data_prep_type] = best_loop_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>One Hot Balanced"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fe768132370>",
      "text/html": "<style  type=\"text/css\" >\n#T_74f8c_row0_col0,#T_74f8c_row0_col1,#T_74f8c_row0_col2,#T_74f8c_row0_col3,#T_74f8c_row0_col4,#T_74f8c_row0_col5,#T_74f8c_row0_col6,#T_74f8c_row0_col7,#T_74f8c_row0_col8,#T_74f8c_row0_col9,#T_74f8c_row0_col10,#T_74f8c_row0_col11,#T_74f8c_row0_col12,#T_74f8c_row0_col13,#T_74f8c_row0_col14,#T_74f8c_row0_col15,#T_74f8c_row0_col16,#T_74f8c_row0_col17,#T_74f8c_row1_col0,#T_74f8c_row1_col1,#T_74f8c_row1_col2,#T_74f8c_row1_col3,#T_74f8c_row1_col4,#T_74f8c_row1_col5,#T_74f8c_row1_col6,#T_74f8c_row1_col7,#T_74f8c_row1_col8,#T_74f8c_row1_col9,#T_74f8c_row1_col10,#T_74f8c_row1_col11,#T_74f8c_row1_col12,#T_74f8c_row1_col13,#T_74f8c_row1_col14,#T_74f8c_row1_col15,#T_74f8c_row1_col16,#T_74f8c_row1_col17,#T_74f8c_row2_col0,#T_74f8c_row2_col1,#T_74f8c_row2_col2,#T_74f8c_row2_col3,#T_74f8c_row2_col4,#T_74f8c_row2_col5,#T_74f8c_row2_col6,#T_74f8c_row2_col7,#T_74f8c_row2_col8,#T_74f8c_row2_col9,#T_74f8c_row2_col10,#T_74f8c_row2_col11,#T_74f8c_row2_col12,#T_74f8c_row2_col13,#T_74f8c_row2_col14,#T_74f8c_row2_col15,#T_74f8c_row2_col16,#T_74f8c_row2_col17,#T_74f8c_row3_col0,#T_74f8c_row3_col1,#T_74f8c_row3_col2,#T_74f8c_row3_col3,#T_74f8c_row3_col4,#T_74f8c_row3_col5,#T_74f8c_row3_col6,#T_74f8c_row3_col7,#T_74f8c_row3_col8,#T_74f8c_row3_col9,#T_74f8c_row3_col10,#T_74f8c_row3_col11,#T_74f8c_row3_col12,#T_74f8c_row3_col13,#T_74f8c_row3_col14,#T_74f8c_row3_col15,#T_74f8c_row3_col16,#T_74f8c_row3_col17,#T_74f8c_row4_col0,#T_74f8c_row4_col1,#T_74f8c_row4_col2,#T_74f8c_row4_col3,#T_74f8c_row4_col4,#T_74f8c_row4_col5,#T_74f8c_row4_col6,#T_74f8c_row4_col7,#T_74f8c_row4_col8,#T_74f8c_row4_col9,#T_74f8c_row4_col10,#T_74f8c_row4_col11,#T_74f8c_row4_col12,#T_74f8c_row4_col13,#T_74f8c_row4_col14,#T_74f8c_row4_col15,#T_74f8c_row4_col16,#T_74f8c_row4_col17,#T_74f8c_row5_col0,#T_74f8c_row5_col1,#T_74f8c_row5_col2,#T_74f8c_row5_col3,#T_74f8c_row5_col4,#T_74f8c_row5_col5,#T_74f8c_row5_col6,#T_74f8c_row5_col7,#T_74f8c_row5_col8,#T_74f8c_row5_col9,#T_74f8c_row5_col10,#T_74f8c_row5_col11,#T_74f8c_row5_col12,#T_74f8c_row5_col13,#T_74f8c_row5_col14,#T_74f8c_row5_col15,#T_74f8c_row5_col16,#T_74f8c_row5_col17,#T_74f8c_row6_col0,#T_74f8c_row6_col1,#T_74f8c_row6_col2,#T_74f8c_row6_col3,#T_74f8c_row6_col4,#T_74f8c_row6_col5,#T_74f8c_row6_col6,#T_74f8c_row6_col7,#T_74f8c_row6_col8,#T_74f8c_row6_col9,#T_74f8c_row6_col10,#T_74f8c_row6_col11,#T_74f8c_row6_col12,#T_74f8c_row6_col13,#T_74f8c_row6_col14,#T_74f8c_row6_col15,#T_74f8c_row6_col16,#T_74f8c_row6_col17,#T_74f8c_row7_col0,#T_74f8c_row7_col1,#T_74f8c_row7_col2,#T_74f8c_row7_col3,#T_74f8c_row7_col4,#T_74f8c_row7_col5,#T_74f8c_row7_col6,#T_74f8c_row7_col7,#T_74f8c_row7_col8,#T_74f8c_row7_col9,#T_74f8c_row7_col10,#T_74f8c_row7_col11,#T_74f8c_row7_col12,#T_74f8c_row7_col13,#T_74f8c_row7_col14,#T_74f8c_row7_col15,#T_74f8c_row7_col16,#T_74f8c_row7_col17,#T_74f8c_row8_col0,#T_74f8c_row8_col1,#T_74f8c_row8_col2,#T_74f8c_row8_col3,#T_74f8c_row8_col4,#T_74f8c_row8_col5,#T_74f8c_row8_col6,#T_74f8c_row8_col7,#T_74f8c_row8_col8,#T_74f8c_row8_col9,#T_74f8c_row8_col10,#T_74f8c_row8_col11,#T_74f8c_row8_col12,#T_74f8c_row8_col13,#T_74f8c_row8_col14,#T_74f8c_row8_col15,#T_74f8c_row8_col16,#T_74f8c_row8_col17{\n            background-color: ;\n        }</style><table id=\"T_74f8c_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >mean_f1</th>        <th class=\"col_heading level0 col1\" >std_f1</th>        <th class=\"col_heading level0 col2\" >mean_roc_auc</th>        <th class=\"col_heading level0 col3\" >std_roc_auc</th>        <th class=\"col_heading level0 col4\" >mean_accuracy</th>        <th class=\"col_heading level0 col5\" >std_accuracy</th>        <th class=\"col_heading level0 col6\" >mean_recall</th>        <th class=\"col_heading level0 col7\" >std_recall</th>        <th class=\"col_heading level0 col8\" >mean_precision</th>        <th class=\"col_heading level0 col9\" >std_precision</th>        <th class=\"col_heading level0 col10\" >mean_true_pos</th>        <th class=\"col_heading level0 col11\" >std_true_pos</th>        <th class=\"col_heading level0 col12\" >mean_true_neg</th>        <th class=\"col_heading level0 col13\" >std_true_neg</th>        <th class=\"col_heading level0 col14\" >mean_false_pos</th>        <th class=\"col_heading level0 col15\" >std_false_pos</th>        <th class=\"col_heading level0 col16\" >mean_false_neg</th>        <th class=\"col_heading level0 col17\" >std_false_neg</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_74f8c_level0_row0\" class=\"row_heading level0 row0\" >K Nearest Neighbours</th>\n                        <td id=\"T_74f8c_row0_col0\" class=\"data row0 col0\" >0.144819</td>\n                        <td id=\"T_74f8c_row0_col1\" class=\"data row0 col1\" >0.042427</td>\n                        <td id=\"T_74f8c_row0_col2\" class=\"data row0 col2\" >0.518926</td>\n                        <td id=\"T_74f8c_row0_col3\" class=\"data row0 col3\" >0.009520</td>\n                        <td id=\"T_74f8c_row0_col4\" class=\"data row0 col4\" >0.792049</td>\n                        <td id=\"T_74f8c_row0_col5\" class=\"data row0 col5\" >0.021624</td>\n                        <td id=\"T_74f8c_row0_col6\" class=\"data row0 col6\" >0.111111</td>\n                        <td id=\"T_74f8c_row0_col7\" class=\"data row0 col7\" >0.045361</td>\n                        <td id=\"T_74f8c_row0_col8\" class=\"data row0 col8\" >0.233333</td>\n                        <td id=\"T_74f8c_row0_col9\" class=\"data row0 col9\" >0.037495</td>\n                        <td id=\"T_74f8c_row0_col10\" class=\"data row0 col10\" >2.000000</td>\n                        <td id=\"T_74f8c_row0_col11\" class=\"data row0 col11\" >0.816497</td>\n                        <td id=\"T_74f8c_row0_col12\" class=\"data row0 col12\" >84.333333</td>\n                        <td id=\"T_74f8c_row0_col13\" class=\"data row0 col13\" >3.091206</td>\n                        <td id=\"T_74f8c_row0_col14\" class=\"data row0 col14\" >6.666667</td>\n                        <td id=\"T_74f8c_row0_col15\" class=\"data row0 col15\" >3.091206</td>\n                        <td id=\"T_74f8c_row0_col16\" class=\"data row0 col16\" >16.000000</td>\n                        <td id=\"T_74f8c_row0_col17\" class=\"data row0 col17\" >0.816497</td>\n            </tr>\n            <tr>\n                        <th id=\"T_74f8c_level0_row1\" class=\"row_heading level0 row1\" >Support Vector Machines</th>\n                        <td id=\"T_74f8c_row1_col0\" class=\"data row1 col0\" >0.223449</td>\n                        <td id=\"T_74f8c_row1_col1\" class=\"data row1 col1\" >0.073343</td>\n                        <td id=\"T_74f8c_row1_col2\" class=\"data row1 col2\" >0.498575</td>\n                        <td id=\"T_74f8c_row1_col3\" class=\"data row1 col3\" >0.052939</td>\n                        <td id=\"T_74f8c_row1_col4\" class=\"data row1 col4\" >0.559633</td>\n                        <td id=\"T_74f8c_row1_col5\" class=\"data row1 col5\" >0.027008</td>\n                        <td id=\"T_74f8c_row1_col6\" class=\"data row1 col6\" >0.407407</td>\n                        <td id=\"T_74f8c_row1_col7\" class=\"data row1 col7\" >0.171734</td>\n                        <td id=\"T_74f8c_row1_col8\" class=\"data row1 col8\" >0.155368</td>\n                        <td id=\"T_74f8c_row1_col9\" class=\"data row1 col9\" >0.043618</td>\n                        <td id=\"T_74f8c_row1_col10\" class=\"data row1 col10\" >7.333333</td>\n                        <td id=\"T_74f8c_row1_col11\" class=\"data row1 col11\" >3.091206</td>\n                        <td id=\"T_74f8c_row1_col12\" class=\"data row1 col12\" >53.666667</td>\n                        <td id=\"T_74f8c_row1_col13\" class=\"data row1 col13\" >6.018490</td>\n                        <td id=\"T_74f8c_row1_col14\" class=\"data row1 col14\" >37.333333</td>\n                        <td id=\"T_74f8c_row1_col15\" class=\"data row1 col15\" >6.018490</td>\n                        <td id=\"T_74f8c_row1_col16\" class=\"data row1 col16\" >10.666667</td>\n                        <td id=\"T_74f8c_row1_col17\" class=\"data row1 col17\" >3.091206</td>\n            </tr>\n            <tr>\n                        <th id=\"T_74f8c_level0_row2\" class=\"row_heading level0 row2\" >Gaussian Process</th>\n                        <td id=\"T_74f8c_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col2\" class=\"data row2 col2\" >0.500000</td>\n                        <td id=\"T_74f8c_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col4\" class=\"data row2 col4\" >0.834862</td>\n                        <td id=\"T_74f8c_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col10\" class=\"data row2 col10\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col11\" class=\"data row2 col11\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col12\" class=\"data row2 col12\" >91.000000</td>\n                        <td id=\"T_74f8c_row2_col13\" class=\"data row2 col13\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col14\" class=\"data row2 col14\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col15\" class=\"data row2 col15\" >0.000000</td>\n                        <td id=\"T_74f8c_row2_col16\" class=\"data row2 col16\" >18.000000</td>\n                        <td id=\"T_74f8c_row2_col17\" class=\"data row2 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_74f8c_level0_row3\" class=\"row_heading level0 row3\" >Random Forest Classifier</th>\n                        <td id=\"T_74f8c_row3_col0\" class=\"data row3 col0\" >0.125483</td>\n                        <td id=\"T_74f8c_row3_col1\" class=\"data row3 col1\" >0.091246</td>\n                        <td id=\"T_74f8c_row3_col2\" class=\"data row3 col2\" >0.498779</td>\n                        <td id=\"T_74f8c_row3_col3\" class=\"data row3 col3\" >0.036341</td>\n                        <td id=\"T_74f8c_row3_col4\" class=\"data row3 col4\" >0.758410</td>\n                        <td id=\"T_74f8c_row3_col5\" class=\"data row3 col5\" >0.033778</td>\n                        <td id=\"T_74f8c_row3_col6\" class=\"data row3 col6\" >0.111111</td>\n                        <td id=\"T_74f8c_row3_col7\" class=\"data row3 col7\" >0.078567</td>\n                        <td id=\"T_74f8c_row3_col8\" class=\"data row3 col8\" >0.152632</td>\n                        <td id=\"T_74f8c_row3_col9\" class=\"data row3 col9\" >0.122531</td>\n                        <td id=\"T_74f8c_row3_col10\" class=\"data row3 col10\" >2.000000</td>\n                        <td id=\"T_74f8c_row3_col11\" class=\"data row3 col11\" >1.414214</td>\n                        <td id=\"T_74f8c_row3_col12\" class=\"data row3 col12\" >80.666667</td>\n                        <td id=\"T_74f8c_row3_col13\" class=\"data row3 col13\" >4.027682</td>\n                        <td id=\"T_74f8c_row3_col14\" class=\"data row3 col14\" >10.333333</td>\n                        <td id=\"T_74f8c_row3_col15\" class=\"data row3 col15\" >4.027682</td>\n                        <td id=\"T_74f8c_row3_col16\" class=\"data row3 col16\" >16.000000</td>\n                        <td id=\"T_74f8c_row3_col17\" class=\"data row3 col17\" >1.414214</td>\n            </tr>\n            <tr>\n                        <th id=\"T_74f8c_level0_row4\" class=\"row_heading level0 row4\" >Gradient Boosting Classifier</th>\n                        <td id=\"T_74f8c_row4_col0\" class=\"data row4 col0\" >0.158322</td>\n                        <td id=\"T_74f8c_row4_col1\" class=\"data row4 col1\" >0.042224</td>\n                        <td id=\"T_74f8c_row4_col2\" class=\"data row4 col2\" >0.484534</td>\n                        <td id=\"T_74f8c_row4_col3\" class=\"data row4 col3\" >0.022682</td>\n                        <td id=\"T_74f8c_row4_col4\" class=\"data row4 col4\" >0.685015</td>\n                        <td id=\"T_74f8c_row4_col5\" class=\"data row4 col5\" >0.035400</td>\n                        <td id=\"T_74f8c_row4_col6\" class=\"data row4 col6\" >0.185185</td>\n                        <td id=\"T_74f8c_row4_col7\" class=\"data row4 col7\" >0.069290</td>\n                        <td id=\"T_74f8c_row4_col8\" class=\"data row4 col8\" >0.142430</td>\n                        <td id=\"T_74f8c_row4_col9\" class=\"data row4 col9\" >0.026586</td>\n                        <td id=\"T_74f8c_row4_col10\" class=\"data row4 col10\" >3.333333</td>\n                        <td id=\"T_74f8c_row4_col11\" class=\"data row4 col11\" >1.247219</td>\n                        <td id=\"T_74f8c_row4_col12\" class=\"data row4 col12\" >71.333333</td>\n                        <td id=\"T_74f8c_row4_col13\" class=\"data row4 col13\" >4.714045</td>\n                        <td id=\"T_74f8c_row4_col14\" class=\"data row4 col14\" >19.666667</td>\n                        <td id=\"T_74f8c_row4_col15\" class=\"data row4 col15\" >4.714045</td>\n                        <td id=\"T_74f8c_row4_col16\" class=\"data row4 col16\" >14.666667</td>\n                        <td id=\"T_74f8c_row4_col17\" class=\"data row4 col17\" >1.247219</td>\n            </tr>\n            <tr>\n                        <th id=\"T_74f8c_level0_row5\" class=\"row_heading level0 row5\" >Ada Boost classifier</th>\n                        <td id=\"T_74f8c_row5_col0\" class=\"data row5 col0\" >0.249982</td>\n                        <td id=\"T_74f8c_row5_col1\" class=\"data row5 col1\" >0.043867</td>\n                        <td id=\"T_74f8c_row5_col2\" class=\"data row5 col2\" >0.517399</td>\n                        <td id=\"T_74f8c_row5_col3\" class=\"data row5 col3\" >0.042994</td>\n                        <td id=\"T_74f8c_row5_col4\" class=\"data row5 col4\" >0.529052</td>\n                        <td id=\"T_74f8c_row5_col5\" class=\"data row5 col5\" >0.134487</td>\n                        <td id=\"T_74f8c_row5_col6\" class=\"data row5 col6\" >0.500000</td>\n                        <td id=\"T_74f8c_row5_col7\" class=\"data row5 col7\" >0.207870</td>\n                        <td id=\"T_74f8c_row5_col8\" class=\"data row5 col8\" >0.178900</td>\n                        <td id=\"T_74f8c_row5_col9\" class=\"data row5 col9\" >0.021130</td>\n                        <td id=\"T_74f8c_row5_col10\" class=\"data row5 col10\" >9.000000</td>\n                        <td id=\"T_74f8c_row5_col11\" class=\"data row5 col11\" >3.741657</td>\n                        <td id=\"T_74f8c_row5_col12\" class=\"data row5 col12\" >48.666667</td>\n                        <td id=\"T_74f8c_row5_col13\" class=\"data row5 col13\" >17.987650</td>\n                        <td id=\"T_74f8c_row5_col14\" class=\"data row5 col14\" >42.333333</td>\n                        <td id=\"T_74f8c_row5_col15\" class=\"data row5 col15\" >17.987650</td>\n                        <td id=\"T_74f8c_row5_col16\" class=\"data row5 col16\" >9.000000</td>\n                        <td id=\"T_74f8c_row5_col17\" class=\"data row5 col17\" >3.741657</td>\n            </tr>\n            <tr>\n                        <th id=\"T_74f8c_level0_row6\" class=\"row_heading level0 row6\" >Gaussian Naieve Bayes</th>\n                        <td id=\"T_74f8c_row6_col0\" class=\"data row6 col0\" >0.242619</td>\n                        <td id=\"T_74f8c_row6_col1\" class=\"data row6 col1\" >0.031142</td>\n                        <td id=\"T_74f8c_row6_col2\" class=\"data row6 col2\" >0.508038</td>\n                        <td id=\"T_74f8c_row6_col3\" class=\"data row6 col3\" >0.024087</td>\n                        <td id=\"T_74f8c_row6_col4\" class=\"data row6 col4\" >0.538226</td>\n                        <td id=\"T_74f8c_row6_col5\" class=\"data row6 col5\" >0.114424</td>\n                        <td id=\"T_74f8c_row6_col6\" class=\"data row6 col6\" >0.462963</td>\n                        <td id=\"T_74f8c_row6_col7\" class=\"data row6 col7\" >0.183324</td>\n                        <td id=\"T_74f8c_row6_col8\" class=\"data row6 col8\" >0.171074</td>\n                        <td id=\"T_74f8c_row6_col9\" class=\"data row6 col9\" >0.019338</td>\n                        <td id=\"T_74f8c_row6_col10\" class=\"data row6 col10\" >8.333333</td>\n                        <td id=\"T_74f8c_row6_col11\" class=\"data row6 col11\" >3.299832</td>\n                        <td id=\"T_74f8c_row6_col12\" class=\"data row6 col12\" >50.333333</td>\n                        <td id=\"T_74f8c_row6_col13\" class=\"data row6 col13\" >15.627611</td>\n                        <td id=\"T_74f8c_row6_col14\" class=\"data row6 col14\" >40.666667</td>\n                        <td id=\"T_74f8c_row6_col15\" class=\"data row6 col15\" >15.627611</td>\n                        <td id=\"T_74f8c_row6_col16\" class=\"data row6 col16\" >9.666667</td>\n                        <td id=\"T_74f8c_row6_col17\" class=\"data row6 col17\" >3.299832</td>\n            </tr>\n            <tr>\n                        <th id=\"T_74f8c_level0_row7\" class=\"row_heading level0 row7\" >Logistic Regression</th>\n                        <td id=\"T_74f8c_row7_col0\" class=\"data row7 col0\" >0.218896</td>\n                        <td id=\"T_74f8c_row7_col1\" class=\"data row7 col1\" >0.062146</td>\n                        <td id=\"T_74f8c_row7_col2\" class=\"data row7 col2\" >0.503765</td>\n                        <td id=\"T_74f8c_row7_col3\" class=\"data row7 col3\" >0.042312</td>\n                        <td id=\"T_74f8c_row7_col4\" class=\"data row7 col4\" >0.605505</td>\n                        <td id=\"T_74f8c_row7_col5\" class=\"data row7 col5\" >0.052436</td>\n                        <td id=\"T_74f8c_row7_col6\" class=\"data row7 col6\" >0.351852</td>\n                        <td id=\"T_74f8c_row7_col7\" class=\"data row7 col7\" >0.138580</td>\n                        <td id=\"T_74f8c_row7_col8\" class=\"data row7 col8\" >0.162159</td>\n                        <td id=\"T_74f8c_row7_col9\" class=\"data row7 col9\" >0.035133</td>\n                        <td id=\"T_74f8c_row7_col10\" class=\"data row7 col10\" >6.333333</td>\n                        <td id=\"T_74f8c_row7_col11\" class=\"data row7 col11\" >2.494438</td>\n                        <td id=\"T_74f8c_row7_col12\" class=\"data row7 col12\" >59.666667</td>\n                        <td id=\"T_74f8c_row7_col13\" class=\"data row7 col13\" >7.586538</td>\n                        <td id=\"T_74f8c_row7_col14\" class=\"data row7 col14\" >31.333333</td>\n                        <td id=\"T_74f8c_row7_col15\" class=\"data row7 col15\" >7.586538</td>\n                        <td id=\"T_74f8c_row7_col16\" class=\"data row7 col16\" >11.666667</td>\n                        <td id=\"T_74f8c_row7_col17\" class=\"data row7 col17\" >2.494438</td>\n            </tr>\n            <tr>\n                        <th id=\"T_74f8c_level0_row8\" class=\"row_heading level0 row8\" >Quadratic Discriminant Analysis</th>\n                        <td id=\"T_74f8c_row8_col0\" class=\"data row8 col0\" >0.027778</td>\n                        <td id=\"T_74f8c_row8_col1\" class=\"data row8 col1\" >0.039284</td>\n                        <td id=\"T_74f8c_row8_col2\" class=\"data row8 col2\" >0.494607</td>\n                        <td id=\"T_74f8c_row8_col3\" class=\"data row8 col3\" >0.004611</td>\n                        <td id=\"T_74f8c_row8_col4\" class=\"data row8 col4\" >0.813456</td>\n                        <td id=\"T_74f8c_row8_col5\" class=\"data row8 col5\" >0.011442</td>\n                        <td id=\"T_74f8c_row8_col6\" class=\"data row8 col6\" >0.018519</td>\n                        <td id=\"T_74f8c_row8_col7\" class=\"data row8 col7\" >0.026189</td>\n                        <td id=\"T_74f8c_row8_col8\" class=\"data row8 col8\" >0.055556</td>\n                        <td id=\"T_74f8c_row8_col9\" class=\"data row8 col9\" >0.078567</td>\n                        <td id=\"T_74f8c_row8_col10\" class=\"data row8 col10\" >0.333333</td>\n                        <td id=\"T_74f8c_row8_col11\" class=\"data row8 col11\" >0.471405</td>\n                        <td id=\"T_74f8c_row8_col12\" class=\"data row8 col12\" >88.333333</td>\n                        <td id=\"T_74f8c_row8_col13\" class=\"data row8 col13\" >1.699673</td>\n                        <td id=\"T_74f8c_row8_col14\" class=\"data row8 col14\" >2.666667</td>\n                        <td id=\"T_74f8c_row8_col15\" class=\"data row8 col15\" >1.699673</td>\n                        <td id=\"T_74f8c_row8_col16\" class=\"data row8 col16\" >17.666667</td>\n                        <td id=\"T_74f8c_row8_col17\" class=\"data row8 col17\" >0.471405</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>Target Balanced"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fe768130040>",
      "text/html": "<style  type=\"text/css\" >\n#T_7dab3_row0_col0,#T_7dab3_row0_col1,#T_7dab3_row0_col2,#T_7dab3_row0_col3,#T_7dab3_row0_col4,#T_7dab3_row0_col5,#T_7dab3_row0_col6,#T_7dab3_row0_col7,#T_7dab3_row0_col8,#T_7dab3_row0_col9,#T_7dab3_row0_col10,#T_7dab3_row0_col11,#T_7dab3_row0_col12,#T_7dab3_row0_col13,#T_7dab3_row0_col14,#T_7dab3_row0_col15,#T_7dab3_row0_col16,#T_7dab3_row0_col17,#T_7dab3_row1_col0,#T_7dab3_row1_col1,#T_7dab3_row1_col2,#T_7dab3_row1_col3,#T_7dab3_row1_col4,#T_7dab3_row1_col5,#T_7dab3_row1_col6,#T_7dab3_row1_col7,#T_7dab3_row1_col8,#T_7dab3_row1_col9,#T_7dab3_row1_col10,#T_7dab3_row1_col11,#T_7dab3_row1_col12,#T_7dab3_row1_col13,#T_7dab3_row1_col14,#T_7dab3_row1_col15,#T_7dab3_row1_col16,#T_7dab3_row1_col17,#T_7dab3_row2_col0,#T_7dab3_row2_col1,#T_7dab3_row2_col2,#T_7dab3_row2_col3,#T_7dab3_row2_col4,#T_7dab3_row2_col5,#T_7dab3_row2_col6,#T_7dab3_row2_col7,#T_7dab3_row2_col8,#T_7dab3_row2_col9,#T_7dab3_row2_col10,#T_7dab3_row2_col11,#T_7dab3_row2_col12,#T_7dab3_row2_col13,#T_7dab3_row2_col14,#T_7dab3_row2_col15,#T_7dab3_row2_col16,#T_7dab3_row2_col17,#T_7dab3_row3_col0,#T_7dab3_row3_col1,#T_7dab3_row3_col2,#T_7dab3_row3_col3,#T_7dab3_row3_col4,#T_7dab3_row3_col5,#T_7dab3_row3_col6,#T_7dab3_row3_col7,#T_7dab3_row3_col8,#T_7dab3_row3_col9,#T_7dab3_row3_col10,#T_7dab3_row3_col11,#T_7dab3_row3_col12,#T_7dab3_row3_col13,#T_7dab3_row3_col14,#T_7dab3_row3_col15,#T_7dab3_row3_col16,#T_7dab3_row3_col17,#T_7dab3_row4_col0,#T_7dab3_row4_col1,#T_7dab3_row4_col2,#T_7dab3_row4_col3,#T_7dab3_row4_col4,#T_7dab3_row4_col5,#T_7dab3_row4_col6,#T_7dab3_row4_col7,#T_7dab3_row4_col8,#T_7dab3_row4_col9,#T_7dab3_row4_col10,#T_7dab3_row4_col11,#T_7dab3_row4_col12,#T_7dab3_row4_col13,#T_7dab3_row4_col14,#T_7dab3_row4_col15,#T_7dab3_row4_col16,#T_7dab3_row4_col17,#T_7dab3_row5_col0,#T_7dab3_row5_col1,#T_7dab3_row5_col2,#T_7dab3_row5_col3,#T_7dab3_row5_col4,#T_7dab3_row5_col5,#T_7dab3_row5_col6,#T_7dab3_row5_col7,#T_7dab3_row5_col8,#T_7dab3_row5_col9,#T_7dab3_row5_col10,#T_7dab3_row5_col11,#T_7dab3_row5_col12,#T_7dab3_row5_col13,#T_7dab3_row5_col14,#T_7dab3_row5_col15,#T_7dab3_row5_col16,#T_7dab3_row5_col17,#T_7dab3_row6_col0,#T_7dab3_row6_col1,#T_7dab3_row6_col2,#T_7dab3_row6_col3,#T_7dab3_row6_col4,#T_7dab3_row6_col5,#T_7dab3_row6_col6,#T_7dab3_row6_col7,#T_7dab3_row6_col8,#T_7dab3_row6_col9,#T_7dab3_row6_col10,#T_7dab3_row6_col11,#T_7dab3_row6_col12,#T_7dab3_row6_col13,#T_7dab3_row6_col14,#T_7dab3_row6_col15,#T_7dab3_row6_col16,#T_7dab3_row6_col17,#T_7dab3_row7_col0,#T_7dab3_row7_col1,#T_7dab3_row7_col2,#T_7dab3_row7_col3,#T_7dab3_row7_col4,#T_7dab3_row7_col5,#T_7dab3_row7_col6,#T_7dab3_row7_col7,#T_7dab3_row7_col8,#T_7dab3_row7_col9,#T_7dab3_row7_col10,#T_7dab3_row7_col11,#T_7dab3_row7_col12,#T_7dab3_row7_col13,#T_7dab3_row7_col14,#T_7dab3_row7_col15,#T_7dab3_row7_col16,#T_7dab3_row7_col17,#T_7dab3_row8_col0,#T_7dab3_row8_col1,#T_7dab3_row8_col2,#T_7dab3_row8_col3,#T_7dab3_row8_col4,#T_7dab3_row8_col5,#T_7dab3_row8_col6,#T_7dab3_row8_col7,#T_7dab3_row8_col8,#T_7dab3_row8_col9,#T_7dab3_row8_col10,#T_7dab3_row8_col11,#T_7dab3_row8_col12,#T_7dab3_row8_col13,#T_7dab3_row8_col14,#T_7dab3_row8_col15,#T_7dab3_row8_col16,#T_7dab3_row8_col17{\n            background-color: ;\n        }</style><table id=\"T_7dab3_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >mean_f1</th>        <th class=\"col_heading level0 col1\" >std_f1</th>        <th class=\"col_heading level0 col2\" >mean_roc_auc</th>        <th class=\"col_heading level0 col3\" >std_roc_auc</th>        <th class=\"col_heading level0 col4\" >mean_accuracy</th>        <th class=\"col_heading level0 col5\" >std_accuracy</th>        <th class=\"col_heading level0 col6\" >mean_recall</th>        <th class=\"col_heading level0 col7\" >std_recall</th>        <th class=\"col_heading level0 col8\" >mean_precision</th>        <th class=\"col_heading level0 col9\" >std_precision</th>        <th class=\"col_heading level0 col10\" >mean_true_pos</th>        <th class=\"col_heading level0 col11\" >std_true_pos</th>        <th class=\"col_heading level0 col12\" >mean_true_neg</th>        <th class=\"col_heading level0 col13\" >std_true_neg</th>        <th class=\"col_heading level0 col14\" >mean_false_pos</th>        <th class=\"col_heading level0 col15\" >std_false_pos</th>        <th class=\"col_heading level0 col16\" >mean_false_neg</th>        <th class=\"col_heading level0 col17\" >std_false_neg</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_7dab3_level0_row0\" class=\"row_heading level0 row0\" >K Nearest Neighbours</th>\n                        <td id=\"T_7dab3_row0_col0\" class=\"data row0 col0\" >0.045977</td>\n                        <td id=\"T_7dab3_row0_col1\" class=\"data row0 col1\" >0.065021</td>\n                        <td id=\"T_7dab3_row0_col2\" class=\"data row0 col2\" >0.483720</td>\n                        <td id=\"T_7dab3_row0_col3\" class=\"data row0 col3\" >0.015829</td>\n                        <td id=\"T_7dab3_row0_col4\" class=\"data row0 col4\" >0.782875</td>\n                        <td id=\"T_7dab3_row0_col5\" class=\"data row0 col5\" >0.008650</td>\n                        <td id=\"T_7dab3_row0_col6\" class=\"data row0 col6\" >0.037037</td>\n                        <td id=\"T_7dab3_row0_col7\" class=\"data row0 col7\" >0.052378</td>\n                        <td id=\"T_7dab3_row0_col8\" class=\"data row0 col8\" >0.060606</td>\n                        <td id=\"T_7dab3_row0_col9\" class=\"data row0 col9\" >0.085710</td>\n                        <td id=\"T_7dab3_row0_col10\" class=\"data row0 col10\" >0.666667</td>\n                        <td id=\"T_7dab3_row0_col11\" class=\"data row0 col11\" >0.942809</td>\n                        <td id=\"T_7dab3_row0_col12\" class=\"data row0 col12\" >84.666667</td>\n                        <td id=\"T_7dab3_row0_col13\" class=\"data row0 col13\" >1.885618</td>\n                        <td id=\"T_7dab3_row0_col14\" class=\"data row0 col14\" >6.333333</td>\n                        <td id=\"T_7dab3_row0_col15\" class=\"data row0 col15\" >1.885618</td>\n                        <td id=\"T_7dab3_row0_col16\" class=\"data row0 col16\" >17.333333</td>\n                        <td id=\"T_7dab3_row0_col17\" class=\"data row0 col17\" >0.942809</td>\n            </tr>\n            <tr>\n                        <th id=\"T_7dab3_level0_row1\" class=\"row_heading level0 row1\" >Support Vector Machines</th>\n                        <td id=\"T_7dab3_row1_col0\" class=\"data row1 col0\" >0.300911</td>\n                        <td id=\"T_7dab3_row1_col1\" class=\"data row1 col1\" >0.025771</td>\n                        <td id=\"T_7dab3_row1_col2\" class=\"data row1 col2\" >0.550061</td>\n                        <td id=\"T_7dab3_row1_col3\" class=\"data row1 col3\" >0.043348</td>\n                        <td id=\"T_7dab3_row1_col4\" class=\"data row1 col4\" >0.397554</td>\n                        <td id=\"T_7dab3_row1_col5\" class=\"data row1 col5\" >0.131960</td>\n                        <td id=\"T_7dab3_row1_col6\" class=\"data row1 col6\" >0.777778</td>\n                        <td id=\"T_7dab3_row1_col7\" class=\"data row1 col7\" >0.136083</td>\n                        <td id=\"T_7dab3_row1_col8\" class=\"data row1 col8\" >0.188462</td>\n                        <td id=\"T_7dab3_row1_col9\" class=\"data row1 col9\" >0.020217</td>\n                        <td id=\"T_7dab3_row1_col10\" class=\"data row1 col10\" >14.000000</td>\n                        <td id=\"T_7dab3_row1_col11\" class=\"data row1 col11\" >2.449490</td>\n                        <td id=\"T_7dab3_row1_col12\" class=\"data row1 col12\" >29.333333</td>\n                        <td id=\"T_7dab3_row1_col13\" class=\"data row1 col13\" >16.519349</td>\n                        <td id=\"T_7dab3_row1_col14\" class=\"data row1 col14\" >61.666667</td>\n                        <td id=\"T_7dab3_row1_col15\" class=\"data row1 col15\" >16.519349</td>\n                        <td id=\"T_7dab3_row1_col16\" class=\"data row1 col16\" >4.000000</td>\n                        <td id=\"T_7dab3_row1_col17\" class=\"data row1 col17\" >2.449490</td>\n            </tr>\n            <tr>\n                        <th id=\"T_7dab3_level0_row2\" class=\"row_heading level0 row2\" >Gaussian Process</th>\n                        <td id=\"T_7dab3_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n                        <td id=\"T_7dab3_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n                        <td id=\"T_7dab3_row2_col2\" class=\"data row2 col2\" >0.494505</td>\n                        <td id=\"T_7dab3_row2_col3\" class=\"data row2 col3\" >0.004486</td>\n                        <td id=\"T_7dab3_row2_col4\" class=\"data row2 col4\" >0.825688</td>\n                        <td id=\"T_7dab3_row2_col5\" class=\"data row2 col5\" >0.007491</td>\n                        <td id=\"T_7dab3_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n                        <td id=\"T_7dab3_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n                        <td id=\"T_7dab3_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n                        <td id=\"T_7dab3_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n                        <td id=\"T_7dab3_row2_col10\" class=\"data row2 col10\" >0.000000</td>\n                        <td id=\"T_7dab3_row2_col11\" class=\"data row2 col11\" >0.000000</td>\n                        <td id=\"T_7dab3_row2_col12\" class=\"data row2 col12\" >90.000000</td>\n                        <td id=\"T_7dab3_row2_col13\" class=\"data row2 col13\" >0.816497</td>\n                        <td id=\"T_7dab3_row2_col14\" class=\"data row2 col14\" >1.000000</td>\n                        <td id=\"T_7dab3_row2_col15\" class=\"data row2 col15\" >0.816497</td>\n                        <td id=\"T_7dab3_row2_col16\" class=\"data row2 col16\" >18.000000</td>\n                        <td id=\"T_7dab3_row2_col17\" class=\"data row2 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_7dab3_level0_row3\" class=\"row_heading level0 row3\" >Random Forest Classifier</th>\n                        <td id=\"T_7dab3_row3_col0\" class=\"data row3 col0\" >0.107882</td>\n                        <td id=\"T_7dab3_row3_col1\" class=\"data row3 col1\" >0.027525</td>\n                        <td id=\"T_7dab3_row3_col2\" class=\"data row3 col2\" >0.485857</td>\n                        <td id=\"T_7dab3_row3_col3\" class=\"data row3 col3\" >0.014474</td>\n                        <td id=\"T_7dab3_row3_col4\" class=\"data row3 col4\" >0.749235</td>\n                        <td id=\"T_7dab3_row3_col5\" class=\"data row3 col5\" >0.024080</td>\n                        <td id=\"T_7dab3_row3_col6\" class=\"data row3 col6\" >0.092593</td>\n                        <td id=\"T_7dab3_row3_col7\" class=\"data row3 col7\" >0.026189</td>\n                        <td id=\"T_7dab3_row3_col8\" class=\"data row3 col8\" >0.133155</td>\n                        <td id=\"T_7dab3_row3_col9\" class=\"data row3 col9\" >0.035156</td>\n                        <td id=\"T_7dab3_row3_col10\" class=\"data row3 col10\" >1.666667</td>\n                        <td id=\"T_7dab3_row3_col11\" class=\"data row3 col11\" >0.471405</td>\n                        <td id=\"T_7dab3_row3_col12\" class=\"data row3 col12\" >80.000000</td>\n                        <td id=\"T_7dab3_row3_col13\" class=\"data row3 col13\" >2.828427</td>\n                        <td id=\"T_7dab3_row3_col14\" class=\"data row3 col14\" >11.000000</td>\n                        <td id=\"T_7dab3_row3_col15\" class=\"data row3 col15\" >2.828427</td>\n                        <td id=\"T_7dab3_row3_col16\" class=\"data row3 col16\" >16.333333</td>\n                        <td id=\"T_7dab3_row3_col17\" class=\"data row3 col17\" >0.471405</td>\n            </tr>\n            <tr>\n                        <th id=\"T_7dab3_level0_row4\" class=\"row_heading level0 row4\" >Gradient Boosting Classifier</th>\n                        <td id=\"T_7dab3_row4_col0\" class=\"data row4 col0\" >0.114472</td>\n                        <td id=\"T_7dab3_row4_col1\" class=\"data row4 col1\" >0.046862</td>\n                        <td id=\"T_7dab3_row4_col2\" class=\"data row4 col2\" >0.462251</td>\n                        <td id=\"T_7dab3_row4_col3\" class=\"data row4 col3\" >0.019814</td>\n                        <td id=\"T_7dab3_row4_col4\" class=\"data row4 col4\" >0.685015</td>\n                        <td id=\"T_7dab3_row4_col5\" class=\"data row4 col5\" >0.033778</td>\n                        <td id=\"T_7dab3_row4_col6\" class=\"data row4 col6\" >0.129630</td>\n                        <td id=\"T_7dab3_row4_col7\" class=\"data row4 col7\" >0.069290</td>\n                        <td id=\"T_7dab3_row4_col8\" class=\"data row4 col8\" >0.106162</td>\n                        <td id=\"T_7dab3_row4_col9\" class=\"data row4 col9\" >0.036172</td>\n                        <td id=\"T_7dab3_row4_col10\" class=\"data row4 col10\" >2.333333</td>\n                        <td id=\"T_7dab3_row4_col11\" class=\"data row4 col11\" >1.247219</td>\n                        <td id=\"T_7dab3_row4_col12\" class=\"data row4 col12\" >72.333333</td>\n                        <td id=\"T_7dab3_row4_col13\" class=\"data row4 col13\" >4.642796</td>\n                        <td id=\"T_7dab3_row4_col14\" class=\"data row4 col14\" >18.666667</td>\n                        <td id=\"T_7dab3_row4_col15\" class=\"data row4 col15\" >4.642796</td>\n                        <td id=\"T_7dab3_row4_col16\" class=\"data row4 col16\" >15.666667</td>\n                        <td id=\"T_7dab3_row4_col17\" class=\"data row4 col17\" >1.247219</td>\n            </tr>\n            <tr>\n                        <th id=\"T_7dab3_level0_row5\" class=\"row_heading level0 row5\" >Ada Boost classifier</th>\n                        <td id=\"T_7dab3_row5_col0\" class=\"data row5 col0\" >0.202363</td>\n                        <td id=\"T_7dab3_row5_col1\" class=\"data row5 col1\" >0.028085</td>\n                        <td id=\"T_7dab3_row5_col2\" class=\"data row5 col2\" >0.472629</td>\n                        <td id=\"T_7dab3_row5_col3\" class=\"data row5 col3\" >0.034001</td>\n                        <td id=\"T_7dab3_row5_col4\" class=\"data row5 col4\" >0.553517</td>\n                        <td id=\"T_7dab3_row5_col5\" class=\"data row5 col5\" >0.127637</td>\n                        <td id=\"T_7dab3_row5_col6\" class=\"data row5 col6\" >0.351852</td>\n                        <td id=\"T_7dab3_row5_col7\" class=\"data row5 col7\" >0.145815</td>\n                        <td id=\"T_7dab3_row5_col8\" class=\"data row5 col8\" >0.152926</td>\n                        <td id=\"T_7dab3_row5_col9\" class=\"data row5 col9\" >0.029276</td>\n                        <td id=\"T_7dab3_row5_col10\" class=\"data row5 col10\" >6.333333</td>\n                        <td id=\"T_7dab3_row5_col11\" class=\"data row5 col11\" >2.624669</td>\n                        <td id=\"T_7dab3_row5_col12\" class=\"data row5 col12\" >54.000000</td>\n                        <td id=\"T_7dab3_row5_col13\" class=\"data row5 col13\" >16.329932</td>\n                        <td id=\"T_7dab3_row5_col14\" class=\"data row5 col14\" >37.000000</td>\n                        <td id=\"T_7dab3_row5_col15\" class=\"data row5 col15\" >16.329932</td>\n                        <td id=\"T_7dab3_row5_col16\" class=\"data row5 col16\" >11.666667</td>\n                        <td id=\"T_7dab3_row5_col17\" class=\"data row5 col17\" >2.624669</td>\n            </tr>\n            <tr>\n                        <th id=\"T_7dab3_level0_row6\" class=\"row_heading level0 row6\" >Gaussian Naieve Bayes</th>\n                        <td id=\"T_7dab3_row6_col0\" class=\"data row6 col0\" >0.241628</td>\n                        <td id=\"T_7dab3_row6_col1\" class=\"data row6 col1\" >0.029976</td>\n                        <td id=\"T_7dab3_row6_col2\" class=\"data row6 col2\" >0.506207</td>\n                        <td id=\"T_7dab3_row6_col3\" class=\"data row6 col3\" >0.023269</td>\n                        <td id=\"T_7dab3_row6_col4\" class=\"data row6 col4\" >0.535168</td>\n                        <td id=\"T_7dab3_row6_col5\" class=\"data row6 col5\" >0.118519</td>\n                        <td id=\"T_7dab3_row6_col6\" class=\"data row6 col6\" >0.462963</td>\n                        <td id=\"T_7dab3_row6_col7\" class=\"data row6 col7\" >0.183324</td>\n                        <td id=\"T_7dab3_row6_col8\" class=\"data row6 col8\" >0.170314</td>\n                        <td id=\"T_7dab3_row6_col9\" class=\"data row6 col9\" >0.019279</td>\n                        <td id=\"T_7dab3_row6_col10\" class=\"data row6 col10\" >8.333333</td>\n                        <td id=\"T_7dab3_row6_col11\" class=\"data row6 col11\" >3.299832</td>\n                        <td id=\"T_7dab3_row6_col12\" class=\"data row6 col12\" >50.000000</td>\n                        <td id=\"T_7dab3_row6_col13\" class=\"data row6 col13\" >16.083117</td>\n                        <td id=\"T_7dab3_row6_col14\" class=\"data row6 col14\" >41.000000</td>\n                        <td id=\"T_7dab3_row6_col15\" class=\"data row6 col15\" >16.083117</td>\n                        <td id=\"T_7dab3_row6_col16\" class=\"data row6 col16\" >9.666667</td>\n                        <td id=\"T_7dab3_row6_col17\" class=\"data row6 col17\" >3.299832</td>\n            </tr>\n            <tr>\n                        <th id=\"T_7dab3_level0_row7\" class=\"row_heading level0 row7\" >Logistic Regression</th>\n                        <td id=\"T_7dab3_row7_col0\" class=\"data row7 col0\" >0.295990</td>\n                        <td id=\"T_7dab3_row7_col1\" class=\"data row7 col1\" >0.047610</td>\n                        <td id=\"T_7dab3_row7_col2\" class=\"data row7 col2\" >0.563492</td>\n                        <td id=\"T_7dab3_row7_col3\" class=\"data row7 col3\" >0.048091</td>\n                        <td id=\"T_7dab3_row7_col4\" class=\"data row7 col4\" >0.568807</td>\n                        <td id=\"T_7dab3_row7_col5\" class=\"data row7 col5\" >0.029963</td>\n                        <td id=\"T_7dab3_row7_col6\" class=\"data row7 col6\" >0.555556</td>\n                        <td id=\"T_7dab3_row7_col7\" class=\"data row7 col7\" >0.120014</td>\n                        <td id=\"T_7dab3_row7_col8\" class=\"data row7 col8\" >0.202453</td>\n                        <td id=\"T_7dab3_row7_col9\" class=\"data row7 col9\" >0.029643</td>\n                        <td id=\"T_7dab3_row7_col10\" class=\"data row7 col10\" >10.000000</td>\n                        <td id=\"T_7dab3_row7_col11\" class=\"data row7 col11\" >2.160247</td>\n                        <td id=\"T_7dab3_row7_col12\" class=\"data row7 col12\" >52.000000</td>\n                        <td id=\"T_7dab3_row7_col13\" class=\"data row7 col13\" >4.242641</td>\n                        <td id=\"T_7dab3_row7_col14\" class=\"data row7 col14\" >39.000000</td>\n                        <td id=\"T_7dab3_row7_col15\" class=\"data row7 col15\" >4.242641</td>\n                        <td id=\"T_7dab3_row7_col16\" class=\"data row7 col16\" >8.000000</td>\n                        <td id=\"T_7dab3_row7_col17\" class=\"data row7 col17\" >2.160247</td>\n            </tr>\n            <tr>\n                        <th id=\"T_7dab3_level0_row8\" class=\"row_heading level0 row8\" >Quadratic Discriminant Analysis</th>\n                        <td id=\"T_7dab3_row8_col0\" class=\"data row8 col0\" >0.141923</td>\n                        <td id=\"T_7dab3_row8_col1\" class=\"data row8 col1\" >0.077249</td>\n                        <td id=\"T_7dab3_row8_col2\" class=\"data row8 col2\" >0.446581</td>\n                        <td id=\"T_7dab3_row8_col3\" class=\"data row8 col3\" >0.050949</td>\n                        <td id=\"T_7dab3_row8_col4\" class=\"data row8 col4\" >0.559633</td>\n                        <td id=\"T_7dab3_row8_col5\" class=\"data row8 col5\" >0.156950</td>\n                        <td id=\"T_7dab3_row8_col6\" class=\"data row8 col6\" >0.277778</td>\n                        <td id=\"T_7dab3_row8_col7\" class=\"data row8 col7\" >0.240027</td>\n                        <td id=\"T_7dab3_row8_col8\" class=\"data row8 col8\" >0.113808</td>\n                        <td id=\"T_7dab3_row8_col9\" class=\"data row8 col9\" >0.034334</td>\n                        <td id=\"T_7dab3_row8_col10\" class=\"data row8 col10\" >5.000000</td>\n                        <td id=\"T_7dab3_row8_col11\" class=\"data row8 col11\" >4.320494</td>\n                        <td id=\"T_7dab3_row8_col12\" class=\"data row8 col12\" >56.000000</td>\n                        <td id=\"T_7dab3_row8_col13\" class=\"data row8 col13\" >20.928450</td>\n                        <td id=\"T_7dab3_row8_col14\" class=\"data row8 col14\" >35.000000</td>\n                        <td id=\"T_7dab3_row8_col15\" class=\"data row8 col15\" >20.928450</td>\n                        <td id=\"T_7dab3_row8_col16\" class=\"data row8 col16\" >13.000000</td>\n                        <td id=\"T_7dab3_row8_col17\" class=\"data row8 col17\" >4.320494</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>One Hot Resampled"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fe74cf0e880>",
      "text/html": "<style  type=\"text/css\" >\n#T_41ffa_row0_col0,#T_41ffa_row0_col1,#T_41ffa_row0_col2,#T_41ffa_row0_col3,#T_41ffa_row0_col4,#T_41ffa_row0_col5,#T_41ffa_row0_col6,#T_41ffa_row0_col7,#T_41ffa_row0_col8,#T_41ffa_row0_col9,#T_41ffa_row0_col10,#T_41ffa_row0_col11,#T_41ffa_row0_col12,#T_41ffa_row0_col13,#T_41ffa_row0_col14,#T_41ffa_row0_col15,#T_41ffa_row0_col16,#T_41ffa_row0_col17,#T_41ffa_row1_col0,#T_41ffa_row1_col1,#T_41ffa_row1_col2,#T_41ffa_row1_col3,#T_41ffa_row1_col4,#T_41ffa_row1_col5,#T_41ffa_row1_col6,#T_41ffa_row1_col7,#T_41ffa_row1_col8,#T_41ffa_row1_col9,#T_41ffa_row1_col10,#T_41ffa_row1_col11,#T_41ffa_row1_col12,#T_41ffa_row1_col13,#T_41ffa_row1_col14,#T_41ffa_row1_col15,#T_41ffa_row1_col16,#T_41ffa_row1_col17,#T_41ffa_row2_col0,#T_41ffa_row2_col1,#T_41ffa_row2_col2,#T_41ffa_row2_col3,#T_41ffa_row2_col4,#T_41ffa_row2_col5,#T_41ffa_row2_col6,#T_41ffa_row2_col7,#T_41ffa_row2_col8,#T_41ffa_row2_col9,#T_41ffa_row2_col10,#T_41ffa_row2_col11,#T_41ffa_row2_col12,#T_41ffa_row2_col13,#T_41ffa_row2_col14,#T_41ffa_row2_col15,#T_41ffa_row2_col16,#T_41ffa_row2_col17,#T_41ffa_row3_col0,#T_41ffa_row3_col1,#T_41ffa_row3_col2,#T_41ffa_row3_col3,#T_41ffa_row3_col4,#T_41ffa_row3_col5,#T_41ffa_row3_col6,#T_41ffa_row3_col7,#T_41ffa_row3_col8,#T_41ffa_row3_col9,#T_41ffa_row3_col10,#T_41ffa_row3_col11,#T_41ffa_row3_col12,#T_41ffa_row3_col13,#T_41ffa_row3_col14,#T_41ffa_row3_col15,#T_41ffa_row3_col16,#T_41ffa_row3_col17,#T_41ffa_row4_col0,#T_41ffa_row4_col1,#T_41ffa_row4_col2,#T_41ffa_row4_col3,#T_41ffa_row4_col4,#T_41ffa_row4_col5,#T_41ffa_row4_col6,#T_41ffa_row4_col7,#T_41ffa_row4_col8,#T_41ffa_row4_col9,#T_41ffa_row4_col10,#T_41ffa_row4_col11,#T_41ffa_row4_col12,#T_41ffa_row4_col13,#T_41ffa_row4_col14,#T_41ffa_row4_col15,#T_41ffa_row4_col16,#T_41ffa_row4_col17,#T_41ffa_row5_col0,#T_41ffa_row5_col1,#T_41ffa_row5_col2,#T_41ffa_row5_col3,#T_41ffa_row5_col4,#T_41ffa_row5_col5,#T_41ffa_row5_col6,#T_41ffa_row5_col7,#T_41ffa_row5_col8,#T_41ffa_row5_col9,#T_41ffa_row5_col10,#T_41ffa_row5_col11,#T_41ffa_row5_col12,#T_41ffa_row5_col13,#T_41ffa_row5_col14,#T_41ffa_row5_col15,#T_41ffa_row5_col16,#T_41ffa_row5_col17,#T_41ffa_row6_col0,#T_41ffa_row6_col1,#T_41ffa_row6_col2,#T_41ffa_row6_col3,#T_41ffa_row6_col4,#T_41ffa_row6_col5,#T_41ffa_row6_col6,#T_41ffa_row6_col7,#T_41ffa_row6_col8,#T_41ffa_row6_col9,#T_41ffa_row6_col10,#T_41ffa_row6_col11,#T_41ffa_row6_col12,#T_41ffa_row6_col13,#T_41ffa_row6_col14,#T_41ffa_row6_col15,#T_41ffa_row6_col16,#T_41ffa_row6_col17,#T_41ffa_row7_col0,#T_41ffa_row7_col1,#T_41ffa_row7_col2,#T_41ffa_row7_col3,#T_41ffa_row7_col4,#T_41ffa_row7_col5,#T_41ffa_row7_col6,#T_41ffa_row7_col7,#T_41ffa_row7_col8,#T_41ffa_row7_col9,#T_41ffa_row7_col10,#T_41ffa_row7_col11,#T_41ffa_row7_col12,#T_41ffa_row7_col13,#T_41ffa_row7_col14,#T_41ffa_row7_col15,#T_41ffa_row7_col16,#T_41ffa_row7_col17,#T_41ffa_row8_col0,#T_41ffa_row8_col1,#T_41ffa_row8_col2,#T_41ffa_row8_col3,#T_41ffa_row8_col4,#T_41ffa_row8_col5,#T_41ffa_row8_col6,#T_41ffa_row8_col7,#T_41ffa_row8_col8,#T_41ffa_row8_col9,#T_41ffa_row8_col10,#T_41ffa_row8_col11,#T_41ffa_row8_col12,#T_41ffa_row8_col13,#T_41ffa_row8_col14,#T_41ffa_row8_col15,#T_41ffa_row8_col16,#T_41ffa_row8_col17{\n            background-color: ;\n        }</style><table id=\"T_41ffa_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >mean_f1</th>        <th class=\"col_heading level0 col1\" >std_f1</th>        <th class=\"col_heading level0 col2\" >mean_roc_auc</th>        <th class=\"col_heading level0 col3\" >std_roc_auc</th>        <th class=\"col_heading level0 col4\" >mean_accuracy</th>        <th class=\"col_heading level0 col5\" >std_accuracy</th>        <th class=\"col_heading level0 col6\" >mean_recall</th>        <th class=\"col_heading level0 col7\" >std_recall</th>        <th class=\"col_heading level0 col8\" >mean_precision</th>        <th class=\"col_heading level0 col9\" >std_precision</th>        <th class=\"col_heading level0 col10\" >mean_true_pos</th>        <th class=\"col_heading level0 col11\" >std_true_pos</th>        <th class=\"col_heading level0 col12\" >mean_true_neg</th>        <th class=\"col_heading level0 col13\" >std_true_neg</th>        <th class=\"col_heading level0 col14\" >mean_false_pos</th>        <th class=\"col_heading level0 col15\" >std_false_pos</th>        <th class=\"col_heading level0 col16\" >mean_false_neg</th>        <th class=\"col_heading level0 col17\" >std_false_neg</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_41ffa_level0_row0\" class=\"row_heading level0 row0\" >K Nearest Neighbours</th>\n                        <td id=\"T_41ffa_row0_col0\" class=\"data row0 col0\" >0.144819</td>\n                        <td id=\"T_41ffa_row0_col1\" class=\"data row0 col1\" >0.042427</td>\n                        <td id=\"T_41ffa_row0_col2\" class=\"data row0 col2\" >0.518926</td>\n                        <td id=\"T_41ffa_row0_col3\" class=\"data row0 col3\" >0.009520</td>\n                        <td id=\"T_41ffa_row0_col4\" class=\"data row0 col4\" >0.792049</td>\n                        <td id=\"T_41ffa_row0_col5\" class=\"data row0 col5\" >0.021624</td>\n                        <td id=\"T_41ffa_row0_col6\" class=\"data row0 col6\" >0.111111</td>\n                        <td id=\"T_41ffa_row0_col7\" class=\"data row0 col7\" >0.045361</td>\n                        <td id=\"T_41ffa_row0_col8\" class=\"data row0 col8\" >0.233333</td>\n                        <td id=\"T_41ffa_row0_col9\" class=\"data row0 col9\" >0.037495</td>\n                        <td id=\"T_41ffa_row0_col10\" class=\"data row0 col10\" >2.000000</td>\n                        <td id=\"T_41ffa_row0_col11\" class=\"data row0 col11\" >0.816497</td>\n                        <td id=\"T_41ffa_row0_col12\" class=\"data row0 col12\" >84.333333</td>\n                        <td id=\"T_41ffa_row0_col13\" class=\"data row0 col13\" >3.091206</td>\n                        <td id=\"T_41ffa_row0_col14\" class=\"data row0 col14\" >6.666667</td>\n                        <td id=\"T_41ffa_row0_col15\" class=\"data row0 col15\" >3.091206</td>\n                        <td id=\"T_41ffa_row0_col16\" class=\"data row0 col16\" >16.000000</td>\n                        <td id=\"T_41ffa_row0_col17\" class=\"data row0 col17\" >0.816497</td>\n            </tr>\n            <tr>\n                        <th id=\"T_41ffa_level0_row1\" class=\"row_heading level0 row1\" >Support Vector Machines</th>\n                        <td id=\"T_41ffa_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col2\" class=\"data row1 col2\" >0.500000</td>\n                        <td id=\"T_41ffa_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col4\" class=\"data row1 col4\" >0.834862</td>\n                        <td id=\"T_41ffa_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col9\" class=\"data row1 col9\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col10\" class=\"data row1 col10\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col11\" class=\"data row1 col11\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col12\" class=\"data row1 col12\" >91.000000</td>\n                        <td id=\"T_41ffa_row1_col13\" class=\"data row1 col13\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col14\" class=\"data row1 col14\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col15\" class=\"data row1 col15\" >0.000000</td>\n                        <td id=\"T_41ffa_row1_col16\" class=\"data row1 col16\" >18.000000</td>\n                        <td id=\"T_41ffa_row1_col17\" class=\"data row1 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_41ffa_level0_row2\" class=\"row_heading level0 row2\" >Gaussian Process</th>\n                        <td id=\"T_41ffa_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col2\" class=\"data row2 col2\" >0.500000</td>\n                        <td id=\"T_41ffa_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col4\" class=\"data row2 col4\" >0.834862</td>\n                        <td id=\"T_41ffa_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col10\" class=\"data row2 col10\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col11\" class=\"data row2 col11\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col12\" class=\"data row2 col12\" >91.000000</td>\n                        <td id=\"T_41ffa_row2_col13\" class=\"data row2 col13\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col14\" class=\"data row2 col14\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col15\" class=\"data row2 col15\" >0.000000</td>\n                        <td id=\"T_41ffa_row2_col16\" class=\"data row2 col16\" >18.000000</td>\n                        <td id=\"T_41ffa_row2_col17\" class=\"data row2 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_41ffa_level0_row3\" class=\"row_heading level0 row3\" >Random Forest Classifier</th>\n                        <td id=\"T_41ffa_row3_col0\" class=\"data row3 col0\" >0.000000</td>\n                        <td id=\"T_41ffa_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n                        <td id=\"T_41ffa_row3_col2\" class=\"data row3 col2\" >0.496337</td>\n                        <td id=\"T_41ffa_row3_col3\" class=\"data row3 col3\" >0.005180</td>\n                        <td id=\"T_41ffa_row3_col4\" class=\"data row3 col4\" >0.828746</td>\n                        <td id=\"T_41ffa_row3_col5\" class=\"data row3 col5\" >0.008650</td>\n                        <td id=\"T_41ffa_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n                        <td id=\"T_41ffa_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n                        <td id=\"T_41ffa_row3_col8\" class=\"data row3 col8\" >0.000000</td>\n                        <td id=\"T_41ffa_row3_col9\" class=\"data row3 col9\" >0.000000</td>\n                        <td id=\"T_41ffa_row3_col10\" class=\"data row3 col10\" >0.000000</td>\n                        <td id=\"T_41ffa_row3_col11\" class=\"data row3 col11\" >0.000000</td>\n                        <td id=\"T_41ffa_row3_col12\" class=\"data row3 col12\" >90.333333</td>\n                        <td id=\"T_41ffa_row3_col13\" class=\"data row3 col13\" >0.942809</td>\n                        <td id=\"T_41ffa_row3_col14\" class=\"data row3 col14\" >0.666667</td>\n                        <td id=\"T_41ffa_row3_col15\" class=\"data row3 col15\" >0.942809</td>\n                        <td id=\"T_41ffa_row3_col16\" class=\"data row3 col16\" >18.000000</td>\n                        <td id=\"T_41ffa_row3_col17\" class=\"data row3 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_41ffa_level0_row4\" class=\"row_heading level0 row4\" >Gradient Boosting Classifier</th>\n                        <td id=\"T_41ffa_row4_col0\" class=\"data row4 col0\" >0.000000</td>\n                        <td id=\"T_41ffa_row4_col1\" class=\"data row4 col1\" >0.000000</td>\n                        <td id=\"T_41ffa_row4_col2\" class=\"data row4 col2\" >0.498168</td>\n                        <td id=\"T_41ffa_row4_col3\" class=\"data row4 col3\" >0.002590</td>\n                        <td id=\"T_41ffa_row4_col4\" class=\"data row4 col4\" >0.831804</td>\n                        <td id=\"T_41ffa_row4_col5\" class=\"data row4 col5\" >0.004325</td>\n                        <td id=\"T_41ffa_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n                        <td id=\"T_41ffa_row4_col7\" class=\"data row4 col7\" >0.000000</td>\n                        <td id=\"T_41ffa_row4_col8\" class=\"data row4 col8\" >0.000000</td>\n                        <td id=\"T_41ffa_row4_col9\" class=\"data row4 col9\" >0.000000</td>\n                        <td id=\"T_41ffa_row4_col10\" class=\"data row4 col10\" >0.000000</td>\n                        <td id=\"T_41ffa_row4_col11\" class=\"data row4 col11\" >0.000000</td>\n                        <td id=\"T_41ffa_row4_col12\" class=\"data row4 col12\" >90.666667</td>\n                        <td id=\"T_41ffa_row4_col13\" class=\"data row4 col13\" >0.471405</td>\n                        <td id=\"T_41ffa_row4_col14\" class=\"data row4 col14\" >0.333333</td>\n                        <td id=\"T_41ffa_row4_col15\" class=\"data row4 col15\" >0.471405</td>\n                        <td id=\"T_41ffa_row4_col16\" class=\"data row4 col16\" >18.000000</td>\n                        <td id=\"T_41ffa_row4_col17\" class=\"data row4 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_41ffa_level0_row5\" class=\"row_heading level0 row5\" >Ada Boost classifier</th>\n                        <td id=\"T_41ffa_row5_col0\" class=\"data row5 col0\" >0.000000</td>\n                        <td id=\"T_41ffa_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n                        <td id=\"T_41ffa_row5_col2\" class=\"data row5 col2\" >0.496337</td>\n                        <td id=\"T_41ffa_row5_col3\" class=\"data row5 col3\" >0.005180</td>\n                        <td id=\"T_41ffa_row5_col4\" class=\"data row5 col4\" >0.828746</td>\n                        <td id=\"T_41ffa_row5_col5\" class=\"data row5 col5\" >0.008650</td>\n                        <td id=\"T_41ffa_row5_col6\" class=\"data row5 col6\" >0.000000</td>\n                        <td id=\"T_41ffa_row5_col7\" class=\"data row5 col7\" >0.000000</td>\n                        <td id=\"T_41ffa_row5_col8\" class=\"data row5 col8\" >0.000000</td>\n                        <td id=\"T_41ffa_row5_col9\" class=\"data row5 col9\" >0.000000</td>\n                        <td id=\"T_41ffa_row5_col10\" class=\"data row5 col10\" >0.000000</td>\n                        <td id=\"T_41ffa_row5_col11\" class=\"data row5 col11\" >0.000000</td>\n                        <td id=\"T_41ffa_row5_col12\" class=\"data row5 col12\" >90.333333</td>\n                        <td id=\"T_41ffa_row5_col13\" class=\"data row5 col13\" >0.942809</td>\n                        <td id=\"T_41ffa_row5_col14\" class=\"data row5 col14\" >0.666667</td>\n                        <td id=\"T_41ffa_row5_col15\" class=\"data row5 col15\" >0.942809</td>\n                        <td id=\"T_41ffa_row5_col16\" class=\"data row5 col16\" >18.000000</td>\n                        <td id=\"T_41ffa_row5_col17\" class=\"data row5 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_41ffa_level0_row6\" class=\"row_heading level0 row6\" >Gaussian Naieve Bayes</th>\n                        <td id=\"T_41ffa_row6_col0\" class=\"data row6 col0\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col2\" class=\"data row6 col2\" >0.500000</td>\n                        <td id=\"T_41ffa_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col4\" class=\"data row6 col4\" >0.834862</td>\n                        <td id=\"T_41ffa_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col6\" class=\"data row6 col6\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col7\" class=\"data row6 col7\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col8\" class=\"data row6 col8\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col9\" class=\"data row6 col9\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col10\" class=\"data row6 col10\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col11\" class=\"data row6 col11\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col12\" class=\"data row6 col12\" >91.000000</td>\n                        <td id=\"T_41ffa_row6_col13\" class=\"data row6 col13\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col14\" class=\"data row6 col14\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col15\" class=\"data row6 col15\" >0.000000</td>\n                        <td id=\"T_41ffa_row6_col16\" class=\"data row6 col16\" >18.000000</td>\n                        <td id=\"T_41ffa_row6_col17\" class=\"data row6 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_41ffa_level0_row7\" class=\"row_heading level0 row7\" >Logistic Regression</th>\n                        <td id=\"T_41ffa_row7_col0\" class=\"data row7 col0\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col1\" class=\"data row7 col1\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col2\" class=\"data row7 col2\" >0.500000</td>\n                        <td id=\"T_41ffa_row7_col3\" class=\"data row7 col3\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col4\" class=\"data row7 col4\" >0.834862</td>\n                        <td id=\"T_41ffa_row7_col5\" class=\"data row7 col5\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col6\" class=\"data row7 col6\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col7\" class=\"data row7 col7\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col8\" class=\"data row7 col8\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col9\" class=\"data row7 col9\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col10\" class=\"data row7 col10\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col11\" class=\"data row7 col11\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col12\" class=\"data row7 col12\" >91.000000</td>\n                        <td id=\"T_41ffa_row7_col13\" class=\"data row7 col13\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col14\" class=\"data row7 col14\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col15\" class=\"data row7 col15\" >0.000000</td>\n                        <td id=\"T_41ffa_row7_col16\" class=\"data row7 col16\" >18.000000</td>\n                        <td id=\"T_41ffa_row7_col17\" class=\"data row7 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_41ffa_level0_row8\" class=\"row_heading level0 row8\" >Quadratic Discriminant Analysis</th>\n                        <td id=\"T_41ffa_row8_col0\" class=\"data row8 col0\" >0.027778</td>\n                        <td id=\"T_41ffa_row8_col1\" class=\"data row8 col1\" >0.039284</td>\n                        <td id=\"T_41ffa_row8_col2\" class=\"data row8 col2\" >0.494607</td>\n                        <td id=\"T_41ffa_row8_col3\" class=\"data row8 col3\" >0.004611</td>\n                        <td id=\"T_41ffa_row8_col4\" class=\"data row8 col4\" >0.813456</td>\n                        <td id=\"T_41ffa_row8_col5\" class=\"data row8 col5\" >0.011442</td>\n                        <td id=\"T_41ffa_row8_col6\" class=\"data row8 col6\" >0.018519</td>\n                        <td id=\"T_41ffa_row8_col7\" class=\"data row8 col7\" >0.026189</td>\n                        <td id=\"T_41ffa_row8_col8\" class=\"data row8 col8\" >0.055556</td>\n                        <td id=\"T_41ffa_row8_col9\" class=\"data row8 col9\" >0.078567</td>\n                        <td id=\"T_41ffa_row8_col10\" class=\"data row8 col10\" >0.333333</td>\n                        <td id=\"T_41ffa_row8_col11\" class=\"data row8 col11\" >0.471405</td>\n                        <td id=\"T_41ffa_row8_col12\" class=\"data row8 col12\" >88.333333</td>\n                        <td id=\"T_41ffa_row8_col13\" class=\"data row8 col13\" >1.699673</td>\n                        <td id=\"T_41ffa_row8_col14\" class=\"data row8 col14\" >2.666667</td>\n                        <td id=\"T_41ffa_row8_col15\" class=\"data row8 col15\" >1.699673</td>\n                        <td id=\"T_41ffa_row8_col16\" class=\"data row8 col16\" >17.666667</td>\n                        <td id=\"T_41ffa_row8_col17\" class=\"data row8 col17\" >0.471405</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>Target Resampled"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fe7630bbf70>",
      "text/html": "<style  type=\"text/css\" >\n#T_2b92f_row0_col0,#T_2b92f_row0_col1,#T_2b92f_row0_col2,#T_2b92f_row0_col3,#T_2b92f_row0_col4,#T_2b92f_row0_col5,#T_2b92f_row0_col6,#T_2b92f_row0_col7,#T_2b92f_row0_col8,#T_2b92f_row0_col9,#T_2b92f_row0_col10,#T_2b92f_row0_col11,#T_2b92f_row0_col12,#T_2b92f_row0_col13,#T_2b92f_row0_col14,#T_2b92f_row0_col15,#T_2b92f_row0_col16,#T_2b92f_row0_col17,#T_2b92f_row1_col0,#T_2b92f_row1_col1,#T_2b92f_row1_col2,#T_2b92f_row1_col3,#T_2b92f_row1_col4,#T_2b92f_row1_col5,#T_2b92f_row1_col6,#T_2b92f_row1_col7,#T_2b92f_row1_col8,#T_2b92f_row1_col9,#T_2b92f_row1_col10,#T_2b92f_row1_col11,#T_2b92f_row1_col12,#T_2b92f_row1_col13,#T_2b92f_row1_col14,#T_2b92f_row1_col15,#T_2b92f_row1_col16,#T_2b92f_row1_col17,#T_2b92f_row2_col0,#T_2b92f_row2_col1,#T_2b92f_row2_col2,#T_2b92f_row2_col3,#T_2b92f_row2_col4,#T_2b92f_row2_col5,#T_2b92f_row2_col6,#T_2b92f_row2_col7,#T_2b92f_row2_col8,#T_2b92f_row2_col9,#T_2b92f_row2_col10,#T_2b92f_row2_col11,#T_2b92f_row2_col12,#T_2b92f_row2_col13,#T_2b92f_row2_col14,#T_2b92f_row2_col15,#T_2b92f_row2_col16,#T_2b92f_row2_col17,#T_2b92f_row3_col0,#T_2b92f_row3_col1,#T_2b92f_row3_col2,#T_2b92f_row3_col3,#T_2b92f_row3_col4,#T_2b92f_row3_col5,#T_2b92f_row3_col6,#T_2b92f_row3_col7,#T_2b92f_row3_col8,#T_2b92f_row3_col9,#T_2b92f_row3_col10,#T_2b92f_row3_col11,#T_2b92f_row3_col12,#T_2b92f_row3_col13,#T_2b92f_row3_col14,#T_2b92f_row3_col15,#T_2b92f_row3_col16,#T_2b92f_row3_col17,#T_2b92f_row4_col0,#T_2b92f_row4_col1,#T_2b92f_row4_col2,#T_2b92f_row4_col3,#T_2b92f_row4_col4,#T_2b92f_row4_col5,#T_2b92f_row4_col6,#T_2b92f_row4_col7,#T_2b92f_row4_col8,#T_2b92f_row4_col9,#T_2b92f_row4_col10,#T_2b92f_row4_col11,#T_2b92f_row4_col12,#T_2b92f_row4_col13,#T_2b92f_row4_col14,#T_2b92f_row4_col15,#T_2b92f_row4_col16,#T_2b92f_row4_col17,#T_2b92f_row5_col0,#T_2b92f_row5_col1,#T_2b92f_row5_col2,#T_2b92f_row5_col3,#T_2b92f_row5_col4,#T_2b92f_row5_col5,#T_2b92f_row5_col6,#T_2b92f_row5_col7,#T_2b92f_row5_col8,#T_2b92f_row5_col9,#T_2b92f_row5_col10,#T_2b92f_row5_col11,#T_2b92f_row5_col12,#T_2b92f_row5_col13,#T_2b92f_row5_col14,#T_2b92f_row5_col15,#T_2b92f_row5_col16,#T_2b92f_row5_col17,#T_2b92f_row6_col0,#T_2b92f_row6_col1,#T_2b92f_row6_col2,#T_2b92f_row6_col3,#T_2b92f_row6_col4,#T_2b92f_row6_col5,#T_2b92f_row6_col6,#T_2b92f_row6_col7,#T_2b92f_row6_col8,#T_2b92f_row6_col9,#T_2b92f_row6_col10,#T_2b92f_row6_col11,#T_2b92f_row6_col12,#T_2b92f_row6_col13,#T_2b92f_row6_col14,#T_2b92f_row6_col15,#T_2b92f_row6_col16,#T_2b92f_row6_col17,#T_2b92f_row7_col0,#T_2b92f_row7_col1,#T_2b92f_row7_col2,#T_2b92f_row7_col3,#T_2b92f_row7_col4,#T_2b92f_row7_col5,#T_2b92f_row7_col6,#T_2b92f_row7_col7,#T_2b92f_row7_col8,#T_2b92f_row7_col9,#T_2b92f_row7_col10,#T_2b92f_row7_col11,#T_2b92f_row7_col12,#T_2b92f_row7_col13,#T_2b92f_row7_col14,#T_2b92f_row7_col15,#T_2b92f_row7_col16,#T_2b92f_row7_col17,#T_2b92f_row8_col0,#T_2b92f_row8_col1,#T_2b92f_row8_col2,#T_2b92f_row8_col3,#T_2b92f_row8_col4,#T_2b92f_row8_col5,#T_2b92f_row8_col6,#T_2b92f_row8_col7,#T_2b92f_row8_col8,#T_2b92f_row8_col9,#T_2b92f_row8_col10,#T_2b92f_row8_col11,#T_2b92f_row8_col12,#T_2b92f_row8_col13,#T_2b92f_row8_col14,#T_2b92f_row8_col15,#T_2b92f_row8_col16,#T_2b92f_row8_col17{\n            background-color: ;\n        }</style><table id=\"T_2b92f_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >mean_f1</th>        <th class=\"col_heading level0 col1\" >std_f1</th>        <th class=\"col_heading level0 col2\" >mean_roc_auc</th>        <th class=\"col_heading level0 col3\" >std_roc_auc</th>        <th class=\"col_heading level0 col4\" >mean_accuracy</th>        <th class=\"col_heading level0 col5\" >std_accuracy</th>        <th class=\"col_heading level0 col6\" >mean_recall</th>        <th class=\"col_heading level0 col7\" >std_recall</th>        <th class=\"col_heading level0 col8\" >mean_precision</th>        <th class=\"col_heading level0 col9\" >std_precision</th>        <th class=\"col_heading level0 col10\" >mean_true_pos</th>        <th class=\"col_heading level0 col11\" >std_true_pos</th>        <th class=\"col_heading level0 col12\" >mean_true_neg</th>        <th class=\"col_heading level0 col13\" >std_true_neg</th>        <th class=\"col_heading level0 col14\" >mean_false_pos</th>        <th class=\"col_heading level0 col15\" >std_false_pos</th>        <th class=\"col_heading level0 col16\" >mean_false_neg</th>        <th class=\"col_heading level0 col17\" >std_false_neg</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_2b92f_level0_row0\" class=\"row_heading level0 row0\" >K Nearest Neighbours</th>\n                        <td id=\"T_2b92f_row0_col0\" class=\"data row0 col0\" >0.045977</td>\n                        <td id=\"T_2b92f_row0_col1\" class=\"data row0 col1\" >0.065021</td>\n                        <td id=\"T_2b92f_row0_col2\" class=\"data row0 col2\" >0.483720</td>\n                        <td id=\"T_2b92f_row0_col3\" class=\"data row0 col3\" >0.015829</td>\n                        <td id=\"T_2b92f_row0_col4\" class=\"data row0 col4\" >0.782875</td>\n                        <td id=\"T_2b92f_row0_col5\" class=\"data row0 col5\" >0.008650</td>\n                        <td id=\"T_2b92f_row0_col6\" class=\"data row0 col6\" >0.037037</td>\n                        <td id=\"T_2b92f_row0_col7\" class=\"data row0 col7\" >0.052378</td>\n                        <td id=\"T_2b92f_row0_col8\" class=\"data row0 col8\" >0.060606</td>\n                        <td id=\"T_2b92f_row0_col9\" class=\"data row0 col9\" >0.085710</td>\n                        <td id=\"T_2b92f_row0_col10\" class=\"data row0 col10\" >0.666667</td>\n                        <td id=\"T_2b92f_row0_col11\" class=\"data row0 col11\" >0.942809</td>\n                        <td id=\"T_2b92f_row0_col12\" class=\"data row0 col12\" >84.666667</td>\n                        <td id=\"T_2b92f_row0_col13\" class=\"data row0 col13\" >1.885618</td>\n                        <td id=\"T_2b92f_row0_col14\" class=\"data row0 col14\" >6.333333</td>\n                        <td id=\"T_2b92f_row0_col15\" class=\"data row0 col15\" >1.885618</td>\n                        <td id=\"T_2b92f_row0_col16\" class=\"data row0 col16\" >17.333333</td>\n                        <td id=\"T_2b92f_row0_col17\" class=\"data row0 col17\" >0.942809</td>\n            </tr>\n            <tr>\n                        <th id=\"T_2b92f_level0_row1\" class=\"row_heading level0 row1\" >Support Vector Machines</th>\n                        <td id=\"T_2b92f_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col1\" class=\"data row1 col1\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col2\" class=\"data row1 col2\" >0.500000</td>\n                        <td id=\"T_2b92f_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col4\" class=\"data row1 col4\" >0.834862</td>\n                        <td id=\"T_2b92f_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col9\" class=\"data row1 col9\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col10\" class=\"data row1 col10\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col11\" class=\"data row1 col11\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col12\" class=\"data row1 col12\" >91.000000</td>\n                        <td id=\"T_2b92f_row1_col13\" class=\"data row1 col13\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col14\" class=\"data row1 col14\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col15\" class=\"data row1 col15\" >0.000000</td>\n                        <td id=\"T_2b92f_row1_col16\" class=\"data row1 col16\" >18.000000</td>\n                        <td id=\"T_2b92f_row1_col17\" class=\"data row1 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_2b92f_level0_row2\" class=\"row_heading level0 row2\" >Gaussian Process</th>\n                        <td id=\"T_2b92f_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n                        <td id=\"T_2b92f_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n                        <td id=\"T_2b92f_row2_col2\" class=\"data row2 col2\" >0.494505</td>\n                        <td id=\"T_2b92f_row2_col3\" class=\"data row2 col3\" >0.004486</td>\n                        <td id=\"T_2b92f_row2_col4\" class=\"data row2 col4\" >0.825688</td>\n                        <td id=\"T_2b92f_row2_col5\" class=\"data row2 col5\" >0.007491</td>\n                        <td id=\"T_2b92f_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n                        <td id=\"T_2b92f_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n                        <td id=\"T_2b92f_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n                        <td id=\"T_2b92f_row2_col9\" class=\"data row2 col9\" >0.000000</td>\n                        <td id=\"T_2b92f_row2_col10\" class=\"data row2 col10\" >0.000000</td>\n                        <td id=\"T_2b92f_row2_col11\" class=\"data row2 col11\" >0.000000</td>\n                        <td id=\"T_2b92f_row2_col12\" class=\"data row2 col12\" >90.000000</td>\n                        <td id=\"T_2b92f_row2_col13\" class=\"data row2 col13\" >0.816497</td>\n                        <td id=\"T_2b92f_row2_col14\" class=\"data row2 col14\" >1.000000</td>\n                        <td id=\"T_2b92f_row2_col15\" class=\"data row2 col15\" >0.816497</td>\n                        <td id=\"T_2b92f_row2_col16\" class=\"data row2 col16\" >18.000000</td>\n                        <td id=\"T_2b92f_row2_col17\" class=\"data row2 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_2b92f_level0_row3\" class=\"row_heading level0 row3\" >Random Forest Classifier</th>\n                        <td id=\"T_2b92f_row3_col0\" class=\"data row3 col0\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col2\" class=\"data row3 col2\" >0.500000</td>\n                        <td id=\"T_2b92f_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col4\" class=\"data row3 col4\" >0.834862</td>\n                        <td id=\"T_2b92f_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col8\" class=\"data row3 col8\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col9\" class=\"data row3 col9\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col10\" class=\"data row3 col10\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col11\" class=\"data row3 col11\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col12\" class=\"data row3 col12\" >91.000000</td>\n                        <td id=\"T_2b92f_row3_col13\" class=\"data row3 col13\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col14\" class=\"data row3 col14\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col15\" class=\"data row3 col15\" >0.000000</td>\n                        <td id=\"T_2b92f_row3_col16\" class=\"data row3 col16\" >18.000000</td>\n                        <td id=\"T_2b92f_row3_col17\" class=\"data row3 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_2b92f_level0_row4\" class=\"row_heading level0 row4\" >Gradient Boosting Classifier</th>\n                        <td id=\"T_2b92f_row4_col0\" class=\"data row4 col0\" >0.031746</td>\n                        <td id=\"T_2b92f_row4_col1\" class=\"data row4 col1\" >0.044896</td>\n                        <td id=\"T_2b92f_row4_col2\" class=\"data row4 col2\" >0.487281</td>\n                        <td id=\"T_2b92f_row4_col3\" class=\"data row4 col3\" >0.022712</td>\n                        <td id=\"T_2b92f_row4_col4\" class=\"data row4 col4\" >0.801223</td>\n                        <td id=\"T_2b92f_row4_col5\" class=\"data row4 col5\" >0.022885</td>\n                        <td id=\"T_2b92f_row4_col6\" class=\"data row4 col6\" >0.018519</td>\n                        <td id=\"T_2b92f_row4_col7\" class=\"data row4 col7\" >0.026189</td>\n                        <td id=\"T_2b92f_row4_col8\" class=\"data row4 col8\" >0.111111</td>\n                        <td id=\"T_2b92f_row4_col9\" class=\"data row4 col9\" >0.157135</td>\n                        <td id=\"T_2b92f_row4_col10\" class=\"data row4 col10\" >0.333333</td>\n                        <td id=\"T_2b92f_row4_col11\" class=\"data row4 col11\" >0.471405</td>\n                        <td id=\"T_2b92f_row4_col12\" class=\"data row4 col12\" >87.000000</td>\n                        <td id=\"T_2b92f_row4_col13\" class=\"data row4 col13\" >2.160247</td>\n                        <td id=\"T_2b92f_row4_col14\" class=\"data row4 col14\" >4.000000</td>\n                        <td id=\"T_2b92f_row4_col15\" class=\"data row4 col15\" >2.160247</td>\n                        <td id=\"T_2b92f_row4_col16\" class=\"data row4 col16\" >17.666667</td>\n                        <td id=\"T_2b92f_row4_col17\" class=\"data row4 col17\" >0.471405</td>\n            </tr>\n            <tr>\n                        <th id=\"T_2b92f_level0_row5\" class=\"row_heading level0 row5\" >Ada Boost classifier</th>\n                        <td id=\"T_2b92f_row5_col0\" class=\"data row5 col0\" >0.000000</td>\n                        <td id=\"T_2b92f_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n                        <td id=\"T_2b92f_row5_col2\" class=\"data row5 col2\" >0.496337</td>\n                        <td id=\"T_2b92f_row5_col3\" class=\"data row5 col3\" >0.005180</td>\n                        <td id=\"T_2b92f_row5_col4\" class=\"data row5 col4\" >0.828746</td>\n                        <td id=\"T_2b92f_row5_col5\" class=\"data row5 col5\" >0.008650</td>\n                        <td id=\"T_2b92f_row5_col6\" class=\"data row5 col6\" >0.000000</td>\n                        <td id=\"T_2b92f_row5_col7\" class=\"data row5 col7\" >0.000000</td>\n                        <td id=\"T_2b92f_row5_col8\" class=\"data row5 col8\" >0.000000</td>\n                        <td id=\"T_2b92f_row5_col9\" class=\"data row5 col9\" >0.000000</td>\n                        <td id=\"T_2b92f_row5_col10\" class=\"data row5 col10\" >0.000000</td>\n                        <td id=\"T_2b92f_row5_col11\" class=\"data row5 col11\" >0.000000</td>\n                        <td id=\"T_2b92f_row5_col12\" class=\"data row5 col12\" >90.333333</td>\n                        <td id=\"T_2b92f_row5_col13\" class=\"data row5 col13\" >0.942809</td>\n                        <td id=\"T_2b92f_row5_col14\" class=\"data row5 col14\" >0.666667</td>\n                        <td id=\"T_2b92f_row5_col15\" class=\"data row5 col15\" >0.942809</td>\n                        <td id=\"T_2b92f_row5_col16\" class=\"data row5 col16\" >18.000000</td>\n                        <td id=\"T_2b92f_row5_col17\" class=\"data row5 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_2b92f_level0_row6\" class=\"row_heading level0 row6\" >Gaussian Naieve Bayes</th>\n                        <td id=\"T_2b92f_row6_col0\" class=\"data row6 col0\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col1\" class=\"data row6 col1\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col2\" class=\"data row6 col2\" >0.500000</td>\n                        <td id=\"T_2b92f_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col4\" class=\"data row6 col4\" >0.834862</td>\n                        <td id=\"T_2b92f_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col6\" class=\"data row6 col6\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col7\" class=\"data row6 col7\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col8\" class=\"data row6 col8\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col9\" class=\"data row6 col9\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col10\" class=\"data row6 col10\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col11\" class=\"data row6 col11\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col12\" class=\"data row6 col12\" >91.000000</td>\n                        <td id=\"T_2b92f_row6_col13\" class=\"data row6 col13\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col14\" class=\"data row6 col14\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col15\" class=\"data row6 col15\" >0.000000</td>\n                        <td id=\"T_2b92f_row6_col16\" class=\"data row6 col16\" >18.000000</td>\n                        <td id=\"T_2b92f_row6_col17\" class=\"data row6 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_2b92f_level0_row7\" class=\"row_heading level0 row7\" >Logistic Regression</th>\n                        <td id=\"T_2b92f_row7_col0\" class=\"data row7 col0\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col1\" class=\"data row7 col1\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col2\" class=\"data row7 col2\" >0.500000</td>\n                        <td id=\"T_2b92f_row7_col3\" class=\"data row7 col3\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col4\" class=\"data row7 col4\" >0.834862</td>\n                        <td id=\"T_2b92f_row7_col5\" class=\"data row7 col5\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col6\" class=\"data row7 col6\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col7\" class=\"data row7 col7\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col8\" class=\"data row7 col8\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col9\" class=\"data row7 col9\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col10\" class=\"data row7 col10\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col11\" class=\"data row7 col11\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col12\" class=\"data row7 col12\" >91.000000</td>\n                        <td id=\"T_2b92f_row7_col13\" class=\"data row7 col13\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col14\" class=\"data row7 col14\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col15\" class=\"data row7 col15\" >0.000000</td>\n                        <td id=\"T_2b92f_row7_col16\" class=\"data row7 col16\" >18.000000</td>\n                        <td id=\"T_2b92f_row7_col17\" class=\"data row7 col17\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_2b92f_level0_row8\" class=\"row_heading level0 row8\" >Quadratic Discriminant Analysis</th>\n                        <td id=\"T_2b92f_row8_col0\" class=\"data row8 col0\" >0.141923</td>\n                        <td id=\"T_2b92f_row8_col1\" class=\"data row8 col1\" >0.077249</td>\n                        <td id=\"T_2b92f_row8_col2\" class=\"data row8 col2\" >0.446581</td>\n                        <td id=\"T_2b92f_row8_col3\" class=\"data row8 col3\" >0.050949</td>\n                        <td id=\"T_2b92f_row8_col4\" class=\"data row8 col4\" >0.559633</td>\n                        <td id=\"T_2b92f_row8_col5\" class=\"data row8 col5\" >0.156950</td>\n                        <td id=\"T_2b92f_row8_col6\" class=\"data row8 col6\" >0.277778</td>\n                        <td id=\"T_2b92f_row8_col7\" class=\"data row8 col7\" >0.240027</td>\n                        <td id=\"T_2b92f_row8_col8\" class=\"data row8 col8\" >0.113808</td>\n                        <td id=\"T_2b92f_row8_col9\" class=\"data row8 col9\" >0.034334</td>\n                        <td id=\"T_2b92f_row8_col10\" class=\"data row8 col10\" >5.000000</td>\n                        <td id=\"T_2b92f_row8_col11\" class=\"data row8 col11\" >4.320494</td>\n                        <td id=\"T_2b92f_row8_col12\" class=\"data row8 col12\" >56.000000</td>\n                        <td id=\"T_2b92f_row8_col13\" class=\"data row8 col13\" >20.928450</td>\n                        <td id=\"T_2b92f_row8_col14\" class=\"data row8 col14\" >35.000000</td>\n                        <td id=\"T_2b92f_row8_col15\" class=\"data row8 col15\" >20.928450</td>\n                        <td id=\"T_2b92f_row8_col16\" class=\"data row8 col16\" >13.000000</td>\n                        <td id=\"T_2b92f_row8_col17\" class=\"data row8 col17\" >4.320494</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def highlight_good_scores_green(df):\n",
    "    good_accuracy = df[\"mean_accuracy\"] > 0.6\n",
    "    good_recall = df[\"mean_recall\"] > 0.4\n",
    "    good_precision = df[\"mean_precision\"] > 0.2\n",
    "    highlight = good_recall & good_accuracy & good_precision\n",
    "    if highlight:\n",
    "        return [f\"background-color: green\"] * 18\n",
    "    else:\n",
    "        return [f\"background-color:\"] * 18\n",
    "\n",
    "\n",
    "for name, df in best_model_scores.items():\n",
    "    display(HTML(f\"<h2>{name.replace('_', ' ').title()}\"))\n",
    "    display(df.style.apply(highlight_good_scores_green, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame([])\n",
    "for name, df in best_model_scores.items():\n",
    "    idx_tuples = [(name, classifier)\n",
    "                  for classifier in df.index]\n",
    "    new_idx = pd.MultiIndex.from_tuples(\n",
    "        idx_tuples, names=[\"data prep method\", \"classifier\"]\n",
    "    )\n",
    "    new_idx_df = df.set_index(new_idx)\n",
    "    all_results = pd.concat([all_results, new_idx_df])\n",
    "all_results.to_excel(\"blah.xls\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    mean_f1    std_f1  \\\ndata prep method  classifier                                            \none_hot_balanced  K Nearest Neighbours             0.144819  0.042427   \n                  Support Vector Machines          0.223449  0.073343   \n                  Gaussian Process                 0.000000  0.000000   \n                  Random Forest Classifier         0.125483  0.091246   \n                  Gradient Boosting Classifier     0.158322  0.042224   \n                  Ada Boost classifier             0.249982  0.043867   \n                  Gaussian Naieve Bayes            0.242619  0.031142   \n                  Logistic Regression              0.218896  0.062146   \n                  Quadratic Discriminant Analysis  0.027778  0.039284   \ntarget_balanced   K Nearest Neighbours             0.045977  0.065021   \n                  Support Vector Machines          0.300911  0.025771   \n                  Gaussian Process                 0.000000  0.000000   \n                  Random Forest Classifier         0.107882  0.027525   \n                  Gradient Boosting Classifier     0.114472  0.046862   \n                  Ada Boost classifier             0.202363  0.028085   \n                  Gaussian Naieve Bayes            0.241628  0.029976   \n                  Logistic Regression              0.295990  0.047610   \n                  Quadratic Discriminant Analysis  0.141923  0.077249   \none_hot_resampled K Nearest Neighbours             0.144819  0.042427   \n                  Support Vector Machines          0.000000  0.000000   \n                  Gaussian Process                 0.000000  0.000000   \n                  Random Forest Classifier         0.000000  0.000000   \n                  Gradient Boosting Classifier     0.000000  0.000000   \n                  Ada Boost classifier             0.000000  0.000000   \n                  Gaussian Naieve Bayes            0.000000  0.000000   \n                  Logistic Regression              0.000000  0.000000   \n                  Quadratic Discriminant Analysis  0.027778  0.039284   \ntarget_resampled  K Nearest Neighbours             0.045977  0.065021   \n                  Support Vector Machines          0.000000  0.000000   \n                  Gaussian Process                 0.000000  0.000000   \n                  Random Forest Classifier         0.000000  0.000000   \n                  Gradient Boosting Classifier     0.031746  0.044896   \n                  Ada Boost classifier             0.000000  0.000000   \n                  Gaussian Naieve Bayes            0.000000  0.000000   \n                  Logistic Regression              0.000000  0.000000   \n                  Quadratic Discriminant Analysis  0.141923  0.077249   \n\n                                                   mean_roc_auc  std_roc_auc  \\\ndata prep method  classifier                                                   \none_hot_balanced  K Nearest Neighbours                 0.518926     0.009520   \n                  Support Vector Machines              0.498575     0.052939   \n                  Gaussian Process                     0.500000     0.000000   \n                  Random Forest Classifier             0.498779     0.036341   \n                  Gradient Boosting Classifier         0.484534     0.022682   \n                  Ada Boost classifier                 0.517399     0.042994   \n                  Gaussian Naieve Bayes                0.508038     0.024087   \n                  Logistic Regression                  0.503765     0.042312   \n                  Quadratic Discriminant Analysis      0.494607     0.004611   \ntarget_balanced   K Nearest Neighbours                 0.483720     0.015829   \n                  Support Vector Machines              0.550061     0.043348   \n                  Gaussian Process                     0.494505     0.004486   \n                  Random Forest Classifier             0.485857     0.014474   \n                  Gradient Boosting Classifier         0.462251     0.019814   \n                  Ada Boost classifier                 0.472629     0.034001   \n                  Gaussian Naieve Bayes                0.506207     0.023269   \n                  Logistic Regression                  0.563492     0.048091   \n                  Quadratic Discriminant Analysis      0.446581     0.050949   \none_hot_resampled K Nearest Neighbours                 0.518926     0.009520   \n                  Support Vector Machines              0.500000     0.000000   \n                  Gaussian Process                     0.500000     0.000000   \n                  Random Forest Classifier             0.496337     0.005180   \n                  Gradient Boosting Classifier         0.498168     0.002590   \n                  Ada Boost classifier                 0.496337     0.005180   \n                  Gaussian Naieve Bayes                0.500000     0.000000   \n                  Logistic Regression                  0.500000     0.000000   \n                  Quadratic Discriminant Analysis      0.494607     0.004611   \ntarget_resampled  K Nearest Neighbours                 0.483720     0.015829   \n                  Support Vector Machines              0.500000     0.000000   \n                  Gaussian Process                     0.494505     0.004486   \n                  Random Forest Classifier             0.500000     0.000000   \n                  Gradient Boosting Classifier         0.487281     0.022712   \n                  Ada Boost classifier                 0.496337     0.005180   \n                  Gaussian Naieve Bayes                0.500000     0.000000   \n                  Logistic Regression                  0.500000     0.000000   \n                  Quadratic Discriminant Analysis      0.446581     0.050949   \n\n                                                   mean_accuracy  \\\ndata prep method  classifier                                       \none_hot_balanced  K Nearest Neighbours                  0.792049   \n                  Support Vector Machines               0.559633   \n                  Gaussian Process                      0.834862   \n                  Random Forest Classifier              0.758410   \n                  Gradient Boosting Classifier          0.685015   \n                  Ada Boost classifier                  0.529052   \n                  Gaussian Naieve Bayes                 0.538226   \n                  Logistic Regression                   0.605505   \n                  Quadratic Discriminant Analysis       0.813456   \ntarget_balanced   K Nearest Neighbours                  0.782875   \n                  Support Vector Machines               0.397554   \n                  Gaussian Process                      0.825688   \n                  Random Forest Classifier              0.749235   \n                  Gradient Boosting Classifier          0.685015   \n                  Ada Boost classifier                  0.553517   \n                  Gaussian Naieve Bayes                 0.535168   \n                  Logistic Regression                   0.568807   \n                  Quadratic Discriminant Analysis       0.559633   \none_hot_resampled K Nearest Neighbours                  0.792049   \n                  Support Vector Machines               0.834862   \n                  Gaussian Process                      0.834862   \n                  Random Forest Classifier              0.828746   \n                  Gradient Boosting Classifier          0.831804   \n                  Ada Boost classifier                  0.828746   \n                  Gaussian Naieve Bayes                 0.834862   \n                  Logistic Regression                   0.834862   \n                  Quadratic Discriminant Analysis       0.813456   \ntarget_resampled  K Nearest Neighbours                  0.782875   \n                  Support Vector Machines               0.834862   \n                  Gaussian Process                      0.825688   \n                  Random Forest Classifier              0.834862   \n                  Gradient Boosting Classifier          0.801223   \n                  Ada Boost classifier                  0.828746   \n                  Gaussian Naieve Bayes                 0.834862   \n                  Logistic Regression                   0.834862   \n                  Quadratic Discriminant Analysis       0.559633   \n\n                                                   std_accuracy  mean_recall  \\\ndata prep method  classifier                                                   \none_hot_balanced  K Nearest Neighbours                 0.021624     0.111111   \n                  Support Vector Machines              0.027008     0.407407   \n                  Gaussian Process                     0.000000     0.000000   \n                  Random Forest Classifier             0.033778     0.111111   \n                  Gradient Boosting Classifier         0.035400     0.185185   \n                  Ada Boost classifier                 0.134487     0.500000   \n                  Gaussian Naieve Bayes                0.114424     0.462963   \n                  Logistic Regression                  0.052436     0.351852   \n                  Quadratic Discriminant Analysis      0.011442     0.018519   \ntarget_balanced   K Nearest Neighbours                 0.008650     0.037037   \n                  Support Vector Machines              0.131960     0.777778   \n                  Gaussian Process                     0.007491     0.000000   \n                  Random Forest Classifier             0.024080     0.092593   \n                  Gradient Boosting Classifier         0.033778     0.129630   \n                  Ada Boost classifier                 0.127637     0.351852   \n                  Gaussian Naieve Bayes                0.118519     0.462963   \n                  Logistic Regression                  0.029963     0.555556   \n                  Quadratic Discriminant Analysis      0.156950     0.277778   \none_hot_resampled K Nearest Neighbours                 0.021624     0.111111   \n                  Support Vector Machines              0.000000     0.000000   \n                  Gaussian Process                     0.000000     0.000000   \n                  Random Forest Classifier             0.008650     0.000000   \n                  Gradient Boosting Classifier         0.004325     0.000000   \n                  Ada Boost classifier                 0.008650     0.000000   \n                  Gaussian Naieve Bayes                0.000000     0.000000   \n                  Logistic Regression                  0.000000     0.000000   \n                  Quadratic Discriminant Analysis      0.011442     0.018519   \ntarget_resampled  K Nearest Neighbours                 0.008650     0.037037   \n                  Support Vector Machines              0.000000     0.000000   \n                  Gaussian Process                     0.007491     0.000000   \n                  Random Forest Classifier             0.000000     0.000000   \n                  Gradient Boosting Classifier         0.022885     0.018519   \n                  Ada Boost classifier                 0.008650     0.000000   \n                  Gaussian Naieve Bayes                0.000000     0.000000   \n                  Logistic Regression                  0.000000     0.000000   \n                  Quadratic Discriminant Analysis      0.156950     0.277778   \n\n                                                   std_recall  mean_precision  \\\ndata prep method  classifier                                                    \none_hot_balanced  K Nearest Neighbours               0.045361        0.233333   \n                  Support Vector Machines            0.171734        0.155368   \n                  Gaussian Process                   0.000000        0.000000   \n                  Random Forest Classifier           0.078567        0.152632   \n                  Gradient Boosting Classifier       0.069290        0.142430   \n                  Ada Boost classifier               0.207870        0.178900   \n                  Gaussian Naieve Bayes              0.183324        0.171074   \n                  Logistic Regression                0.138580        0.162159   \n                  Quadratic Discriminant Analysis    0.026189        0.055556   \ntarget_balanced   K Nearest Neighbours               0.052378        0.060606   \n                  Support Vector Machines            0.136083        0.188462   \n                  Gaussian Process                   0.000000        0.000000   \n                  Random Forest Classifier           0.026189        0.133155   \n                  Gradient Boosting Classifier       0.069290        0.106162   \n                  Ada Boost classifier               0.145815        0.152926   \n                  Gaussian Naieve Bayes              0.183324        0.170314   \n                  Logistic Regression                0.120014        0.202453   \n                  Quadratic Discriminant Analysis    0.240027        0.113808   \none_hot_resampled K Nearest Neighbours               0.045361        0.233333   \n                  Support Vector Machines            0.000000        0.000000   \n                  Gaussian Process                   0.000000        0.000000   \n                  Random Forest Classifier           0.000000        0.000000   \n                  Gradient Boosting Classifier       0.000000        0.000000   \n                  Ada Boost classifier               0.000000        0.000000   \n                  Gaussian Naieve Bayes              0.000000        0.000000   \n                  Logistic Regression                0.000000        0.000000   \n                  Quadratic Discriminant Analysis    0.026189        0.055556   \ntarget_resampled  K Nearest Neighbours               0.052378        0.060606   \n                  Support Vector Machines            0.000000        0.000000   \n                  Gaussian Process                   0.000000        0.000000   \n                  Random Forest Classifier           0.000000        0.000000   \n                  Gradient Boosting Classifier       0.026189        0.111111   \n                  Ada Boost classifier               0.000000        0.000000   \n                  Gaussian Naieve Bayes              0.000000        0.000000   \n                  Logistic Regression                0.000000        0.000000   \n                  Quadratic Discriminant Analysis    0.240027        0.113808   \n\n                                                   std_precision  \\\ndata prep method  classifier                                       \none_hot_balanced  K Nearest Neighbours                  0.037495   \n                  Support Vector Machines               0.043618   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              0.122531   \n                  Gradient Boosting Classifier          0.026586   \n                  Ada Boost classifier                  0.021130   \n                  Gaussian Naieve Bayes                 0.019338   \n                  Logistic Regression                   0.035133   \n                  Quadratic Discriminant Analysis       0.078567   \ntarget_balanced   K Nearest Neighbours                  0.085710   \n                  Support Vector Machines               0.020217   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              0.035156   \n                  Gradient Boosting Classifier          0.036172   \n                  Ada Boost classifier                  0.029276   \n                  Gaussian Naieve Bayes                 0.019279   \n                  Logistic Regression                   0.029643   \n                  Quadratic Discriminant Analysis       0.034334   \none_hot_resampled K Nearest Neighbours                  0.037495   \n                  Support Vector Machines               0.000000   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              0.000000   \n                  Gradient Boosting Classifier          0.000000   \n                  Ada Boost classifier                  0.000000   \n                  Gaussian Naieve Bayes                 0.000000   \n                  Logistic Regression                   0.000000   \n                  Quadratic Discriminant Analysis       0.078567   \ntarget_resampled  K Nearest Neighbours                  0.085710   \n                  Support Vector Machines               0.000000   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              0.000000   \n                  Gradient Boosting Classifier          0.157135   \n                  Ada Boost classifier                  0.000000   \n                  Gaussian Naieve Bayes                 0.000000   \n                  Logistic Regression                   0.000000   \n                  Quadratic Discriminant Analysis       0.034334   \n\n                                                   mean_true_pos  \\\ndata prep method  classifier                                       \none_hot_balanced  K Nearest Neighbours                  2.000000   \n                  Support Vector Machines               7.333333   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              2.000000   \n                  Gradient Boosting Classifier          3.333333   \n                  Ada Boost classifier                  9.000000   \n                  Gaussian Naieve Bayes                 8.333333   \n                  Logistic Regression                   6.333333   \n                  Quadratic Discriminant Analysis       0.333333   \ntarget_balanced   K Nearest Neighbours                  0.666667   \n                  Support Vector Machines              14.000000   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              1.666667   \n                  Gradient Boosting Classifier          2.333333   \n                  Ada Boost classifier                  6.333333   \n                  Gaussian Naieve Bayes                 8.333333   \n                  Logistic Regression                  10.000000   \n                  Quadratic Discriminant Analysis       5.000000   \none_hot_resampled K Nearest Neighbours                  2.000000   \n                  Support Vector Machines               0.000000   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              0.000000   \n                  Gradient Boosting Classifier          0.000000   \n                  Ada Boost classifier                  0.000000   \n                  Gaussian Naieve Bayes                 0.000000   \n                  Logistic Regression                   0.000000   \n                  Quadratic Discriminant Analysis       0.333333   \ntarget_resampled  K Nearest Neighbours                  0.666667   \n                  Support Vector Machines               0.000000   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              0.000000   \n                  Gradient Boosting Classifier          0.333333   \n                  Ada Boost classifier                  0.000000   \n                  Gaussian Naieve Bayes                 0.000000   \n                  Logistic Regression                   0.000000   \n                  Quadratic Discriminant Analysis       5.000000   \n\n                                                   std_true_pos  \\\ndata prep method  classifier                                      \none_hot_balanced  K Nearest Neighbours                 0.816497   \n                  Support Vector Machines              3.091206   \n                  Gaussian Process                     0.000000   \n                  Random Forest Classifier             1.414214   \n                  Gradient Boosting Classifier         1.247219   \n                  Ada Boost classifier                 3.741657   \n                  Gaussian Naieve Bayes                3.299832   \n                  Logistic Regression                  2.494438   \n                  Quadratic Discriminant Analysis      0.471405   \ntarget_balanced   K Nearest Neighbours                 0.942809   \n                  Support Vector Machines              2.449490   \n                  Gaussian Process                     0.000000   \n                  Random Forest Classifier             0.471405   \n                  Gradient Boosting Classifier         1.247219   \n                  Ada Boost classifier                 2.624669   \n                  Gaussian Naieve Bayes                3.299832   \n                  Logistic Regression                  2.160247   \n                  Quadratic Discriminant Analysis      4.320494   \none_hot_resampled K Nearest Neighbours                 0.816497   \n                  Support Vector Machines              0.000000   \n                  Gaussian Process                     0.000000   \n                  Random Forest Classifier             0.000000   \n                  Gradient Boosting Classifier         0.000000   \n                  Ada Boost classifier                 0.000000   \n                  Gaussian Naieve Bayes                0.000000   \n                  Logistic Regression                  0.000000   \n                  Quadratic Discriminant Analysis      0.471405   \ntarget_resampled  K Nearest Neighbours                 0.942809   \n                  Support Vector Machines              0.000000   \n                  Gaussian Process                     0.000000   \n                  Random Forest Classifier             0.000000   \n                  Gradient Boosting Classifier         0.471405   \n                  Ada Boost classifier                 0.000000   \n                  Gaussian Naieve Bayes                0.000000   \n                  Logistic Regression                  0.000000   \n                  Quadratic Discriminant Analysis      4.320494   \n\n                                                   mean_true_neg  \\\ndata prep method  classifier                                       \none_hot_balanced  K Nearest Neighbours                 84.333333   \n                  Support Vector Machines              53.666667   \n                  Gaussian Process                     91.000000   \n                  Random Forest Classifier             80.666667   \n                  Gradient Boosting Classifier         71.333333   \n                  Ada Boost classifier                 48.666667   \n                  Gaussian Naieve Bayes                50.333333   \n                  Logistic Regression                  59.666667   \n                  Quadratic Discriminant Analysis      88.333333   \ntarget_balanced   K Nearest Neighbours                 84.666667   \n                  Support Vector Machines              29.333333   \n                  Gaussian Process                     90.000000   \n                  Random Forest Classifier             80.000000   \n                  Gradient Boosting Classifier         72.333333   \n                  Ada Boost classifier                 54.000000   \n                  Gaussian Naieve Bayes                50.000000   \n                  Logistic Regression                  52.000000   \n                  Quadratic Discriminant Analysis      56.000000   \none_hot_resampled K Nearest Neighbours                 84.333333   \n                  Support Vector Machines              91.000000   \n                  Gaussian Process                     91.000000   \n                  Random Forest Classifier             90.333333   \n                  Gradient Boosting Classifier         90.666667   \n                  Ada Boost classifier                 90.333333   \n                  Gaussian Naieve Bayes                91.000000   \n                  Logistic Regression                  91.000000   \n                  Quadratic Discriminant Analysis      88.333333   \ntarget_resampled  K Nearest Neighbours                 84.666667   \n                  Support Vector Machines              91.000000   \n                  Gaussian Process                     90.000000   \n                  Random Forest Classifier             91.000000   \n                  Gradient Boosting Classifier         87.000000   \n                  Ada Boost classifier                 90.333333   \n                  Gaussian Naieve Bayes                91.000000   \n                  Logistic Regression                  91.000000   \n                  Quadratic Discriminant Analysis      56.000000   \n\n                                                   std_true_neg  \\\ndata prep method  classifier                                      \none_hot_balanced  K Nearest Neighbours                 3.091206   \n                  Support Vector Machines              6.018490   \n                  Gaussian Process                     0.000000   \n                  Random Forest Classifier             4.027682   \n                  Gradient Boosting Classifier         4.714045   \n                  Ada Boost classifier                17.987650   \n                  Gaussian Naieve Bayes               15.627611   \n                  Logistic Regression                  7.586538   \n                  Quadratic Discriminant Analysis      1.699673   \ntarget_balanced   K Nearest Neighbours                 1.885618   \n                  Support Vector Machines             16.519349   \n                  Gaussian Process                     0.816497   \n                  Random Forest Classifier             2.828427   \n                  Gradient Boosting Classifier         4.642796   \n                  Ada Boost classifier                16.329932   \n                  Gaussian Naieve Bayes               16.083117   \n                  Logistic Regression                  4.242641   \n                  Quadratic Discriminant Analysis     20.928450   \none_hot_resampled K Nearest Neighbours                 3.091206   \n                  Support Vector Machines              0.000000   \n                  Gaussian Process                     0.000000   \n                  Random Forest Classifier             0.942809   \n                  Gradient Boosting Classifier         0.471405   \n                  Ada Boost classifier                 0.942809   \n                  Gaussian Naieve Bayes                0.000000   \n                  Logistic Regression                  0.000000   \n                  Quadratic Discriminant Analysis      1.699673   \ntarget_resampled  K Nearest Neighbours                 1.885618   \n                  Support Vector Machines              0.000000   \n                  Gaussian Process                     0.816497   \n                  Random Forest Classifier             0.000000   \n                  Gradient Boosting Classifier         2.160247   \n                  Ada Boost classifier                 0.942809   \n                  Gaussian Naieve Bayes                0.000000   \n                  Logistic Regression                  0.000000   \n                  Quadratic Discriminant Analysis     20.928450   \n\n                                                   mean_false_pos  \\\ndata prep method  classifier                                        \none_hot_balanced  K Nearest Neighbours                   6.666667   \n                  Support Vector Machines               37.333333   \n                  Gaussian Process                       0.000000   \n                  Random Forest Classifier              10.333333   \n                  Gradient Boosting Classifier          19.666667   \n                  Ada Boost classifier                  42.333333   \n                  Gaussian Naieve Bayes                 40.666667   \n                  Logistic Regression                   31.333333   \n                  Quadratic Discriminant Analysis        2.666667   \ntarget_balanced   K Nearest Neighbours                   6.333333   \n                  Support Vector Machines               61.666667   \n                  Gaussian Process                       1.000000   \n                  Random Forest Classifier              11.000000   \n                  Gradient Boosting Classifier          18.666667   \n                  Ada Boost classifier                  37.000000   \n                  Gaussian Naieve Bayes                 41.000000   \n                  Logistic Regression                   39.000000   \n                  Quadratic Discriminant Analysis       35.000000   \none_hot_resampled K Nearest Neighbours                   6.666667   \n                  Support Vector Machines                0.000000   \n                  Gaussian Process                       0.000000   \n                  Random Forest Classifier               0.666667   \n                  Gradient Boosting Classifier           0.333333   \n                  Ada Boost classifier                   0.666667   \n                  Gaussian Naieve Bayes                  0.000000   \n                  Logistic Regression                    0.000000   \n                  Quadratic Discriminant Analysis        2.666667   \ntarget_resampled  K Nearest Neighbours                   6.333333   \n                  Support Vector Machines                0.000000   \n                  Gaussian Process                       1.000000   \n                  Random Forest Classifier               0.000000   \n                  Gradient Boosting Classifier           4.000000   \n                  Ada Boost classifier                   0.666667   \n                  Gaussian Naieve Bayes                  0.000000   \n                  Logistic Regression                    0.000000   \n                  Quadratic Discriminant Analysis       35.000000   \n\n                                                   std_false_pos  \\\ndata prep method  classifier                                       \none_hot_balanced  K Nearest Neighbours                  3.091206   \n                  Support Vector Machines               6.018490   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              4.027682   \n                  Gradient Boosting Classifier          4.714045   \n                  Ada Boost classifier                 17.987650   \n                  Gaussian Naieve Bayes                15.627611   \n                  Logistic Regression                   7.586538   \n                  Quadratic Discriminant Analysis       1.699673   \ntarget_balanced   K Nearest Neighbours                  1.885618   \n                  Support Vector Machines              16.519349   \n                  Gaussian Process                      0.816497   \n                  Random Forest Classifier              2.828427   \n                  Gradient Boosting Classifier          4.642796   \n                  Ada Boost classifier                 16.329932   \n                  Gaussian Naieve Bayes                16.083117   \n                  Logistic Regression                   4.242641   \n                  Quadratic Discriminant Analysis      20.928450   \none_hot_resampled K Nearest Neighbours                  3.091206   \n                  Support Vector Machines               0.000000   \n                  Gaussian Process                      0.000000   \n                  Random Forest Classifier              0.942809   \n                  Gradient Boosting Classifier          0.471405   \n                  Ada Boost classifier                  0.942809   \n                  Gaussian Naieve Bayes                 0.000000   \n                  Logistic Regression                   0.000000   \n                  Quadratic Discriminant Analysis       1.699673   \ntarget_resampled  K Nearest Neighbours                  1.885618   \n                  Support Vector Machines               0.000000   \n                  Gaussian Process                      0.816497   \n                  Random Forest Classifier              0.000000   \n                  Gradient Boosting Classifier          2.160247   \n                  Ada Boost classifier                  0.942809   \n                  Gaussian Naieve Bayes                 0.000000   \n                  Logistic Regression                   0.000000   \n                  Quadratic Discriminant Analysis      20.928450   \n\n                                                   mean_false_neg  \\\ndata prep method  classifier                                        \none_hot_balanced  K Nearest Neighbours                  16.000000   \n                  Support Vector Machines               10.666667   \n                  Gaussian Process                      18.000000   \n                  Random Forest Classifier              16.000000   \n                  Gradient Boosting Classifier          14.666667   \n                  Ada Boost classifier                   9.000000   \n                  Gaussian Naieve Bayes                  9.666667   \n                  Logistic Regression                   11.666667   \n                  Quadratic Discriminant Analysis       17.666667   \ntarget_balanced   K Nearest Neighbours                  17.333333   \n                  Support Vector Machines                4.000000   \n                  Gaussian Process                      18.000000   \n                  Random Forest Classifier              16.333333   \n                  Gradient Boosting Classifier          15.666667   \n                  Ada Boost classifier                  11.666667   \n                  Gaussian Naieve Bayes                  9.666667   \n                  Logistic Regression                    8.000000   \n                  Quadratic Discriminant Analysis       13.000000   \none_hot_resampled K Nearest Neighbours                  16.000000   \n                  Support Vector Machines               18.000000   \n                  Gaussian Process                      18.000000   \n                  Random Forest Classifier              18.000000   \n                  Gradient Boosting Classifier          18.000000   \n                  Ada Boost classifier                  18.000000   \n                  Gaussian Naieve Bayes                 18.000000   \n                  Logistic Regression                   18.000000   \n                  Quadratic Discriminant Analysis       17.666667   \ntarget_resampled  K Nearest Neighbours                  17.333333   \n                  Support Vector Machines               18.000000   \n                  Gaussian Process                      18.000000   \n                  Random Forest Classifier              18.000000   \n                  Gradient Boosting Classifier          17.666667   \n                  Ada Boost classifier                  18.000000   \n                  Gaussian Naieve Bayes                 18.000000   \n                  Logistic Regression                   18.000000   \n                  Quadratic Discriminant Analysis       13.000000   \n\n                                                   std_false_neg  \ndata prep method  classifier                                      \none_hot_balanced  K Nearest Neighbours                  0.816497  \n                  Support Vector Machines               3.091206  \n                  Gaussian Process                      0.000000  \n                  Random Forest Classifier              1.414214  \n                  Gradient Boosting Classifier          1.247219  \n                  Ada Boost classifier                  3.741657  \n                  Gaussian Naieve Bayes                 3.299832  \n                  Logistic Regression                   2.494438  \n                  Quadratic Discriminant Analysis       0.471405  \ntarget_balanced   K Nearest Neighbours                  0.942809  \n                  Support Vector Machines               2.449490  \n                  Gaussian Process                      0.000000  \n                  Random Forest Classifier              0.471405  \n                  Gradient Boosting Classifier          1.247219  \n                  Ada Boost classifier                  2.624669  \n                  Gaussian Naieve Bayes                 3.299832  \n                  Logistic Regression                   2.160247  \n                  Quadratic Discriminant Analysis       4.320494  \none_hot_resampled K Nearest Neighbours                  0.816497  \n                  Support Vector Machines               0.000000  \n                  Gaussian Process                      0.000000  \n                  Random Forest Classifier              0.000000  \n                  Gradient Boosting Classifier          0.000000  \n                  Ada Boost classifier                  0.000000  \n                  Gaussian Naieve Bayes                 0.000000  \n                  Logistic Regression                   0.000000  \n                  Quadratic Discriminant Analysis       0.471405  \ntarget_resampled  K Nearest Neighbours                  0.942809  \n                  Support Vector Machines               0.000000  \n                  Gaussian Process                      0.000000  \n                  Random Forest Classifier              0.000000  \n                  Gradient Boosting Classifier          0.471405  \n                  Ada Boost classifier                  0.000000  \n                  Gaussian Naieve Bayes                 0.000000  \n                  Logistic Regression                   0.000000  \n                  Quadratic Discriminant Analysis       4.320494  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>mean_f1</th>\n      <th>std_f1</th>\n      <th>mean_roc_auc</th>\n      <th>std_roc_auc</th>\n      <th>mean_accuracy</th>\n      <th>std_accuracy</th>\n      <th>mean_recall</th>\n      <th>std_recall</th>\n      <th>mean_precision</th>\n      <th>std_precision</th>\n      <th>mean_true_pos</th>\n      <th>std_true_pos</th>\n      <th>mean_true_neg</th>\n      <th>std_true_neg</th>\n      <th>mean_false_pos</th>\n      <th>std_false_pos</th>\n      <th>mean_false_neg</th>\n      <th>std_false_neg</th>\n    </tr>\n    <tr>\n      <th>data prep method</th>\n      <th>classifier</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"9\" valign=\"top\">one_hot_balanced</th>\n      <th>K Nearest Neighbours</th>\n      <td>0.144819</td>\n      <td>0.042427</td>\n      <td>0.518926</td>\n      <td>0.009520</td>\n      <td>0.792049</td>\n      <td>0.021624</td>\n      <td>0.111111</td>\n      <td>0.045361</td>\n      <td>0.233333</td>\n      <td>0.037495</td>\n      <td>2.000000</td>\n      <td>0.816497</td>\n      <td>84.333333</td>\n      <td>3.091206</td>\n      <td>6.666667</td>\n      <td>3.091206</td>\n      <td>16.000000</td>\n      <td>0.816497</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines</th>\n      <td>0.223449</td>\n      <td>0.073343</td>\n      <td>0.498575</td>\n      <td>0.052939</td>\n      <td>0.559633</td>\n      <td>0.027008</td>\n      <td>0.407407</td>\n      <td>0.171734</td>\n      <td>0.155368</td>\n      <td>0.043618</td>\n      <td>7.333333</td>\n      <td>3.091206</td>\n      <td>53.666667</td>\n      <td>6.018490</td>\n      <td>37.333333</td>\n      <td>6.018490</td>\n      <td>10.666667</td>\n      <td>3.091206</td>\n    </tr>\n    <tr>\n      <th>Gaussian Process</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Random Forest Classifier</th>\n      <td>0.125483</td>\n      <td>0.091246</td>\n      <td>0.498779</td>\n      <td>0.036341</td>\n      <td>0.758410</td>\n      <td>0.033778</td>\n      <td>0.111111</td>\n      <td>0.078567</td>\n      <td>0.152632</td>\n      <td>0.122531</td>\n      <td>2.000000</td>\n      <td>1.414214</td>\n      <td>80.666667</td>\n      <td>4.027682</td>\n      <td>10.333333</td>\n      <td>4.027682</td>\n      <td>16.000000</td>\n      <td>1.414214</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting Classifier</th>\n      <td>0.158322</td>\n      <td>0.042224</td>\n      <td>0.484534</td>\n      <td>0.022682</td>\n      <td>0.685015</td>\n      <td>0.035400</td>\n      <td>0.185185</td>\n      <td>0.069290</td>\n      <td>0.142430</td>\n      <td>0.026586</td>\n      <td>3.333333</td>\n      <td>1.247219</td>\n      <td>71.333333</td>\n      <td>4.714045</td>\n      <td>19.666667</td>\n      <td>4.714045</td>\n      <td>14.666667</td>\n      <td>1.247219</td>\n    </tr>\n    <tr>\n      <th>Ada Boost classifier</th>\n      <td>0.249982</td>\n      <td>0.043867</td>\n      <td>0.517399</td>\n      <td>0.042994</td>\n      <td>0.529052</td>\n      <td>0.134487</td>\n      <td>0.500000</td>\n      <td>0.207870</td>\n      <td>0.178900</td>\n      <td>0.021130</td>\n      <td>9.000000</td>\n      <td>3.741657</td>\n      <td>48.666667</td>\n      <td>17.987650</td>\n      <td>42.333333</td>\n      <td>17.987650</td>\n      <td>9.000000</td>\n      <td>3.741657</td>\n    </tr>\n    <tr>\n      <th>Gaussian Naieve Bayes</th>\n      <td>0.242619</td>\n      <td>0.031142</td>\n      <td>0.508038</td>\n      <td>0.024087</td>\n      <td>0.538226</td>\n      <td>0.114424</td>\n      <td>0.462963</td>\n      <td>0.183324</td>\n      <td>0.171074</td>\n      <td>0.019338</td>\n      <td>8.333333</td>\n      <td>3.299832</td>\n      <td>50.333333</td>\n      <td>15.627611</td>\n      <td>40.666667</td>\n      <td>15.627611</td>\n      <td>9.666667</td>\n      <td>3.299832</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.218896</td>\n      <td>0.062146</td>\n      <td>0.503765</td>\n      <td>0.042312</td>\n      <td>0.605505</td>\n      <td>0.052436</td>\n      <td>0.351852</td>\n      <td>0.138580</td>\n      <td>0.162159</td>\n      <td>0.035133</td>\n      <td>6.333333</td>\n      <td>2.494438</td>\n      <td>59.666667</td>\n      <td>7.586538</td>\n      <td>31.333333</td>\n      <td>7.586538</td>\n      <td>11.666667</td>\n      <td>2.494438</td>\n    </tr>\n    <tr>\n      <th>Quadratic Discriminant Analysis</th>\n      <td>0.027778</td>\n      <td>0.039284</td>\n      <td>0.494607</td>\n      <td>0.004611</td>\n      <td>0.813456</td>\n      <td>0.011442</td>\n      <td>0.018519</td>\n      <td>0.026189</td>\n      <td>0.055556</td>\n      <td>0.078567</td>\n      <td>0.333333</td>\n      <td>0.471405</td>\n      <td>88.333333</td>\n      <td>1.699673</td>\n      <td>2.666667</td>\n      <td>1.699673</td>\n      <td>17.666667</td>\n      <td>0.471405</td>\n    </tr>\n    <tr>\n      <th rowspan=\"9\" valign=\"top\">target_balanced</th>\n      <th>K Nearest Neighbours</th>\n      <td>0.045977</td>\n      <td>0.065021</td>\n      <td>0.483720</td>\n      <td>0.015829</td>\n      <td>0.782875</td>\n      <td>0.008650</td>\n      <td>0.037037</td>\n      <td>0.052378</td>\n      <td>0.060606</td>\n      <td>0.085710</td>\n      <td>0.666667</td>\n      <td>0.942809</td>\n      <td>84.666667</td>\n      <td>1.885618</td>\n      <td>6.333333</td>\n      <td>1.885618</td>\n      <td>17.333333</td>\n      <td>0.942809</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines</th>\n      <td>0.300911</td>\n      <td>0.025771</td>\n      <td>0.550061</td>\n      <td>0.043348</td>\n      <td>0.397554</td>\n      <td>0.131960</td>\n      <td>0.777778</td>\n      <td>0.136083</td>\n      <td>0.188462</td>\n      <td>0.020217</td>\n      <td>14.000000</td>\n      <td>2.449490</td>\n      <td>29.333333</td>\n      <td>16.519349</td>\n      <td>61.666667</td>\n      <td>16.519349</td>\n      <td>4.000000</td>\n      <td>2.449490</td>\n    </tr>\n    <tr>\n      <th>Gaussian Process</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494505</td>\n      <td>0.004486</td>\n      <td>0.825688</td>\n      <td>0.007491</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>90.000000</td>\n      <td>0.816497</td>\n      <td>1.000000</td>\n      <td>0.816497</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Random Forest Classifier</th>\n      <td>0.107882</td>\n      <td>0.027525</td>\n      <td>0.485857</td>\n      <td>0.014474</td>\n      <td>0.749235</td>\n      <td>0.024080</td>\n      <td>0.092593</td>\n      <td>0.026189</td>\n      <td>0.133155</td>\n      <td>0.035156</td>\n      <td>1.666667</td>\n      <td>0.471405</td>\n      <td>80.000000</td>\n      <td>2.828427</td>\n      <td>11.000000</td>\n      <td>2.828427</td>\n      <td>16.333333</td>\n      <td>0.471405</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting Classifier</th>\n      <td>0.114472</td>\n      <td>0.046862</td>\n      <td>0.462251</td>\n      <td>0.019814</td>\n      <td>0.685015</td>\n      <td>0.033778</td>\n      <td>0.129630</td>\n      <td>0.069290</td>\n      <td>0.106162</td>\n      <td>0.036172</td>\n      <td>2.333333</td>\n      <td>1.247219</td>\n      <td>72.333333</td>\n      <td>4.642796</td>\n      <td>18.666667</td>\n      <td>4.642796</td>\n      <td>15.666667</td>\n      <td>1.247219</td>\n    </tr>\n    <tr>\n      <th>Ada Boost classifier</th>\n      <td>0.202363</td>\n      <td>0.028085</td>\n      <td>0.472629</td>\n      <td>0.034001</td>\n      <td>0.553517</td>\n      <td>0.127637</td>\n      <td>0.351852</td>\n      <td>0.145815</td>\n      <td>0.152926</td>\n      <td>0.029276</td>\n      <td>6.333333</td>\n      <td>2.624669</td>\n      <td>54.000000</td>\n      <td>16.329932</td>\n      <td>37.000000</td>\n      <td>16.329932</td>\n      <td>11.666667</td>\n      <td>2.624669</td>\n    </tr>\n    <tr>\n      <th>Gaussian Naieve Bayes</th>\n      <td>0.241628</td>\n      <td>0.029976</td>\n      <td>0.506207</td>\n      <td>0.023269</td>\n      <td>0.535168</td>\n      <td>0.118519</td>\n      <td>0.462963</td>\n      <td>0.183324</td>\n      <td>0.170314</td>\n      <td>0.019279</td>\n      <td>8.333333</td>\n      <td>3.299832</td>\n      <td>50.000000</td>\n      <td>16.083117</td>\n      <td>41.000000</td>\n      <td>16.083117</td>\n      <td>9.666667</td>\n      <td>3.299832</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.295990</td>\n      <td>0.047610</td>\n      <td>0.563492</td>\n      <td>0.048091</td>\n      <td>0.568807</td>\n      <td>0.029963</td>\n      <td>0.555556</td>\n      <td>0.120014</td>\n      <td>0.202453</td>\n      <td>0.029643</td>\n      <td>10.000000</td>\n      <td>2.160247</td>\n      <td>52.000000</td>\n      <td>4.242641</td>\n      <td>39.000000</td>\n      <td>4.242641</td>\n      <td>8.000000</td>\n      <td>2.160247</td>\n    </tr>\n    <tr>\n      <th>Quadratic Discriminant Analysis</th>\n      <td>0.141923</td>\n      <td>0.077249</td>\n      <td>0.446581</td>\n      <td>0.050949</td>\n      <td>0.559633</td>\n      <td>0.156950</td>\n      <td>0.277778</td>\n      <td>0.240027</td>\n      <td>0.113808</td>\n      <td>0.034334</td>\n      <td>5.000000</td>\n      <td>4.320494</td>\n      <td>56.000000</td>\n      <td>20.928450</td>\n      <td>35.000000</td>\n      <td>20.928450</td>\n      <td>13.000000</td>\n      <td>4.320494</td>\n    </tr>\n    <tr>\n      <th rowspan=\"9\" valign=\"top\">one_hot_resampled</th>\n      <th>K Nearest Neighbours</th>\n      <td>0.144819</td>\n      <td>0.042427</td>\n      <td>0.518926</td>\n      <td>0.009520</td>\n      <td>0.792049</td>\n      <td>0.021624</td>\n      <td>0.111111</td>\n      <td>0.045361</td>\n      <td>0.233333</td>\n      <td>0.037495</td>\n      <td>2.000000</td>\n      <td>0.816497</td>\n      <td>84.333333</td>\n      <td>3.091206</td>\n      <td>6.666667</td>\n      <td>3.091206</td>\n      <td>16.000000</td>\n      <td>0.816497</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Gaussian Process</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Random Forest Classifier</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.496337</td>\n      <td>0.005180</td>\n      <td>0.828746</td>\n      <td>0.008650</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>90.333333</td>\n      <td>0.942809</td>\n      <td>0.666667</td>\n      <td>0.942809</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting Classifier</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.498168</td>\n      <td>0.002590</td>\n      <td>0.831804</td>\n      <td>0.004325</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>90.666667</td>\n      <td>0.471405</td>\n      <td>0.333333</td>\n      <td>0.471405</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Ada Boost classifier</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.496337</td>\n      <td>0.005180</td>\n      <td>0.828746</td>\n      <td>0.008650</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>90.333333</td>\n      <td>0.942809</td>\n      <td>0.666667</td>\n      <td>0.942809</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Gaussian Naieve Bayes</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Quadratic Discriminant Analysis</th>\n      <td>0.027778</td>\n      <td>0.039284</td>\n      <td>0.494607</td>\n      <td>0.004611</td>\n      <td>0.813456</td>\n      <td>0.011442</td>\n      <td>0.018519</td>\n      <td>0.026189</td>\n      <td>0.055556</td>\n      <td>0.078567</td>\n      <td>0.333333</td>\n      <td>0.471405</td>\n      <td>88.333333</td>\n      <td>1.699673</td>\n      <td>2.666667</td>\n      <td>1.699673</td>\n      <td>17.666667</td>\n      <td>0.471405</td>\n    </tr>\n    <tr>\n      <th rowspan=\"9\" valign=\"top\">target_resampled</th>\n      <th>K Nearest Neighbours</th>\n      <td>0.045977</td>\n      <td>0.065021</td>\n      <td>0.483720</td>\n      <td>0.015829</td>\n      <td>0.782875</td>\n      <td>0.008650</td>\n      <td>0.037037</td>\n      <td>0.052378</td>\n      <td>0.060606</td>\n      <td>0.085710</td>\n      <td>0.666667</td>\n      <td>0.942809</td>\n      <td>84.666667</td>\n      <td>1.885618</td>\n      <td>6.333333</td>\n      <td>1.885618</td>\n      <td>17.333333</td>\n      <td>0.942809</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Gaussian Process</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.494505</td>\n      <td>0.004486</td>\n      <td>0.825688</td>\n      <td>0.007491</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>90.000000</td>\n      <td>0.816497</td>\n      <td>1.000000</td>\n      <td>0.816497</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Random Forest Classifier</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting Classifier</th>\n      <td>0.031746</td>\n      <td>0.044896</td>\n      <td>0.487281</td>\n      <td>0.022712</td>\n      <td>0.801223</td>\n      <td>0.022885</td>\n      <td>0.018519</td>\n      <td>0.026189</td>\n      <td>0.111111</td>\n      <td>0.157135</td>\n      <td>0.333333</td>\n      <td>0.471405</td>\n      <td>87.000000</td>\n      <td>2.160247</td>\n      <td>4.000000</td>\n      <td>2.160247</td>\n      <td>17.666667</td>\n      <td>0.471405</td>\n    </tr>\n    <tr>\n      <th>Ada Boost classifier</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.496337</td>\n      <td>0.005180</td>\n      <td>0.828746</td>\n      <td>0.008650</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>90.333333</td>\n      <td>0.942809</td>\n      <td>0.666667</td>\n      <td>0.942809</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Gaussian Naieve Bayes</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.000000</td>\n      <td>0.834862</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>91.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>18.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Quadratic Discriminant Analysis</th>\n      <td>0.141923</td>\n      <td>0.077249</td>\n      <td>0.446581</td>\n      <td>0.050949</td>\n      <td>0.559633</td>\n      <td>0.156950</td>\n      <td>0.277778</td>\n      <td>0.240027</td>\n      <td>0.113808</td>\n      <td>0.034334</td>\n      <td>5.000000</td>\n      <td>4.320494</td>\n      <td>56.000000</td>\n      <td>20.928450</td>\n      <td>35.000000</td>\n      <td>20.928450</td>\n      <td>13.000000</td>\n      <td>4.320494</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = pd.DataFrame([])\n",
    "for data_prep_method, scores_df in best_model_scores.items():\n",
    "\n",
    "    results_idx_tuples = [(data_prep_method, classifier)\n",
    "                          for classifier in scores_df.index]\n",
    "    new_results_idx = pd.MultiIndex.from_tuples(\n",
    "        results_idx_tuples,\n",
    "        names=[\"data prep method\", \"classifier\"]\n",
    "    )\n",
    "    new_idx_scores_df = scores_df.set_index(new_results_idx)\n",
    "    all_results = pd.concat([all_results, new_idx_scores_df])\n",
    "all_results\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                                  one_hot_balanced  \\\nK Nearest Neighbours             {'metric': 'minkowski', 'n_jobs': -2, 'n_neigh...   \nSupport Vector Machines          {'C': 0.1, 'class_weight': 'balanced', 'gamma'...   \nGaussian Process                 {'kernel': 0.707**2 * RBF(length_scale=0.5), '...   \nRandom Forest Classifier         {'class_weight': 'balanced', 'max_depth': 4, '...   \nGradient Boosting Classifier     {'learning_rate': 0.1, 'max_depth': 4, 'max_fe...   \nAda Boost classifier                  {'learning_rate': 0.001, 'n_estimators': 30}   \nGaussian Naieve Bayes                                       {'var_smoothing': 0.3}   \nLogistic Regression              {'C': 0.1, 'class_weight': 'balanced', 'penalt...   \nQuadratic Discriminant Analysis                                 {'reg_param': 0.0}   \n\n                                                                   target_balanced  \\\nK Nearest Neighbours             {'metric': 'minkowski', 'n_jobs': -2, 'n_neigh...   \nSupport Vector Machines          {'C': 0.1, 'class_weight': 'balanced', 'gamma'...   \nGaussian Process                 {'kernel': 0.707**2 * RBF(length_scale=0.5), '...   \nRandom Forest Classifier         {'class_weight': 'balanced', 'max_depth': 4, '...   \nGradient Boosting Classifier     {'learning_rate': 0.1, 'max_depth': 4, 'max_fe...   \nAda Boost classifier                  {'learning_rate': 0.001, 'n_estimators': 30}   \nGaussian Naieve Bayes                                       {'var_smoothing': 0.3}   \nLogistic Regression              {'C': 0.1, 'class_weight': 'balanced', 'penalt...   \nQuadratic Discriminant Analysis                                 {'reg_param': 0.0}   \n\n                                                                 one_hot_resampled  \\\nK Nearest Neighbours             {'metric': 'minkowski', 'n_jobs': -2, 'n_neigh...   \nSupport Vector Machines               {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}   \nGaussian Process                 {'kernel': 0.707**2 * RBF(length_scale=0.5), '...   \nRandom Forest Classifier         {'max_depth': 4, 'max_features': 0.3, 'min_sam...   \nGradient Boosting Classifier     {'learning_rate': 0.1, 'max_depth': 4, 'max_fe...   \nAda Boost classifier                  {'learning_rate': 0.001, 'n_estimators': 30}   \nGaussian Naieve Bayes                                       {'var_smoothing': 0.3}   \nLogistic Regression              {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...   \nQuadratic Discriminant Analysis                                 {'reg_param': 0.0}   \n\n                                                                  target_resampled  \nK Nearest Neighbours             {'metric': 'minkowski', 'n_jobs': -2, 'n_neigh...  \nSupport Vector Machines               {'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}  \nGaussian Process                 {'kernel': 0.707**2 * RBF(length_scale=0.5), '...  \nRandom Forest Classifier         {'max_depth': 4, 'max_features': 0.3, 'min_sam...  \nGradient Boosting Classifier     {'learning_rate': 0.1, 'max_depth': 4, 'max_fe...  \nAda Boost classifier                  {'learning_rate': 0.001, 'n_estimators': 30}  \nGaussian Naieve Bayes                                       {'var_smoothing': 0.3}  \nLogistic Regression              {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...  \nQuadratic Discriminant Analysis                                 {'reg_param': 0.0}  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>one_hot_balanced</th>\n      <th>target_balanced</th>\n      <th>one_hot_resampled</th>\n      <th>target_resampled</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>K Nearest Neighbours</th>\n      <td>{'metric': 'minkowski', 'n_jobs': -2, 'n_neigh...</td>\n      <td>{'metric': 'minkowski', 'n_jobs': -2, 'n_neigh...</td>\n      <td>{'metric': 'minkowski', 'n_jobs': -2, 'n_neigh...</td>\n      <td>{'metric': 'minkowski', 'n_jobs': -2, 'n_neigh...</td>\n    </tr>\n    <tr>\n      <th>Support Vector Machines</th>\n      <td>{'C': 0.1, 'class_weight': 'balanced', 'gamma'...</td>\n      <td>{'C': 0.1, 'class_weight': 'balanced', 'gamma'...</td>\n      <td>{'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}</td>\n      <td>{'C': 0.1, 'gamma': 0.1, 'kernel': 'linear'}</td>\n    </tr>\n    <tr>\n      <th>Gaussian Process</th>\n      <td>{'kernel': 0.707**2 * RBF(length_scale=0.5), '...</td>\n      <td>{'kernel': 0.707**2 * RBF(length_scale=0.5), '...</td>\n      <td>{'kernel': 0.707**2 * RBF(length_scale=0.5), '...</td>\n      <td>{'kernel': 0.707**2 * RBF(length_scale=0.5), '...</td>\n    </tr>\n    <tr>\n      <th>Random Forest Classifier</th>\n      <td>{'class_weight': 'balanced', 'max_depth': 4, '...</td>\n      <td>{'class_weight': 'balanced', 'max_depth': 4, '...</td>\n      <td>{'max_depth': 4, 'max_features': 0.3, 'min_sam...</td>\n      <td>{'max_depth': 4, 'max_features': 0.3, 'min_sam...</td>\n    </tr>\n    <tr>\n      <th>Gradient Boosting Classifier</th>\n      <td>{'learning_rate': 0.1, 'max_depth': 4, 'max_fe...</td>\n      <td>{'learning_rate': 0.1, 'max_depth': 4, 'max_fe...</td>\n      <td>{'learning_rate': 0.1, 'max_depth': 4, 'max_fe...</td>\n      <td>{'learning_rate': 0.1, 'max_depth': 4, 'max_fe...</td>\n    </tr>\n    <tr>\n      <th>Ada Boost classifier</th>\n      <td>{'learning_rate': 0.001, 'n_estimators': 30}</td>\n      <td>{'learning_rate': 0.001, 'n_estimators': 30}</td>\n      <td>{'learning_rate': 0.001, 'n_estimators': 30}</td>\n      <td>{'learning_rate': 0.001, 'n_estimators': 30}</td>\n    </tr>\n    <tr>\n      <th>Gaussian Naieve Bayes</th>\n      <td>{'var_smoothing': 0.3}</td>\n      <td>{'var_smoothing': 0.3}</td>\n      <td>{'var_smoothing': 0.3}</td>\n      <td>{'var_smoothing': 0.3}</td>\n    </tr>\n    <tr>\n      <th>Logistic Regression</th>\n      <td>{'C': 0.1, 'class_weight': 'balanced', 'penalt...</td>\n      <td>{'C': 0.1, 'class_weight': 'balanced', 'penalt...</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'libline...</td>\n      <td>{'C': 0.1, 'penalty': 'l2', 'solver': 'libline...</td>\n    </tr>\n    <tr>\n      <th>Quadratic Discriminant Analysis</th>\n      <td>{'reg_param': 0.0}</td>\n      <td>{'reg_param': 0.0}</td>\n      <td>{'reg_param': 0.0}</td>\n      <td>{'reg_param': 0.0}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(best_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that none of the methods have really \"cracked this nut\" so to\n",
    "speak. Most models identify under 50% of the patients that need hospital\n",
    "treatment and are right far less than 25% of the time when they do predict\n",
    "the need for hospital treatment.\n",
    "\n",
    "Almost all of the best results appear to come from data that is one-hot\n",
    "encoded and when the target is weighted (rather than synthesising new\n",
    "examples). The best performing models appear to be the tree based methods\n",
    " (Random Forest Classifier, Gradient Bossting Classifier, and Ada Boost\n",
    " Classifier), classic Logistic Regression (a version of linear regression\n",
    " optimised for classification tasks) and Support Vector Machines (trained\n",
    " using target encoded data).\n",
    "\n",
    "We can test how well these models perform with new data by evaluating their predictions\n",
    "on the holdout test set: a dataset that has not been used at any point\n",
    "during training and thus is a good indicator of a\n",
    "model's ability to generalise. I'll only evaluate the models identified\n",
    "above, as indiscriminately evaluating every possible model against the holdout\n",
    "test set risks biasing our selection - the more models we test the more\n",
    "likely a result is to have occurred by chance rather than accurate modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_ohe, X_test_ohe = encode_and_scale(X_train, y_train, X_test,\n",
    "                                           cat_encoder=\"one_hot\")\n",
    "X_train_ohe_scaled, X_test_ohe_scaled = encode_and_scale(X_train, y_train,\n",
    "                                                         X_test,\n",
    "                                                         cat_encoder=\"one_hot\",\n",
    "                                                         scaled=True)\n",
    "X_train_target_scaled, X_test_target_scaled = encode_and_scale(X_train, y_train,\n",
    "                                                               X_test,\n",
    "                                                               cat_encoder=\"one_hot\",\n",
    "                                                               scaled=True)\n",
    "\n",
    "best_performing_models = [\n",
    "    (\"one_hot_balanced\", 'Random Forest Classifier'),\n",
    "    (\"one_hot_balanced\", 'Gradient Boosting Classifier'),\n",
    "    (\"one_hot_balanced\", 'Ada Boost classifier'),\n",
    "    (\"one_hot_balanced\",  'Logistic Regression'),\n",
    "    (\"target_balanced\", \"Support Vector Machines\")\n",
    "]\n",
    "test_scores = {}\n",
    "for data_prep_type, model in best_performing_models:\n",
    "    model_args = techniques_dict[model](balanced=True)\n",
    "    clf = model_args[\"clf\"]\n",
    "    params = best_params[data_prep_type][model]\n",
    "    if \"weight_y\" in model_args.keys():\n",
    "        weight_y = model_args[\"weight_y\"]\n",
    "    else:\n",
    "        weight_y = False\n",
    "    if model_args[\"scaled\"]:\n",
    "        if weight_y:\n",
    "            # calculate array of weights for y labels\n",
    "            pos_weight, neg_weight = compute_class_weight(\n",
    "                class_weight=\"balanced\",\n",
    "                classes=[1,0],\n",
    "                y=y_train)\n",
    "            y_weights = y_train.apply(lambda y: pos_weight if y else neg_weight)\n",
    "            # train model using parameters, weights and cv loop data\n",
    "            clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(X_train_ohe_scaled,\n",
    "                           y_train,\n",
    "                           sample_weight=y_weights))\n",
    "        else:\n",
    "            # train model using parameters and cv loop data\n",
    "            clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(X_train_ohe_scaled, y_train))\n",
    "    else:\n",
    "        if weight_y:\n",
    "            # calculate array of weights for y labels\n",
    "            pos_weight, neg_weight = compute_class_weight(\n",
    "                class_weight=\"balanced\",\n",
    "                classes=[1,0],\n",
    "                y=y_train)\n",
    "            y_weights = y_train.apply(lambda y: pos_weight if y else neg_weight)\n",
    "            # train model using parameters, weights and cv loop data\n",
    "            clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(X_train_ohe,\n",
    "                           y_train,\n",
    "                           sample_weight=y_weights))\n",
    "        else:\n",
    "            # train model using parameters and cv loop data\n",
    "            clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(X_train_ohe, y_train))\n",
    "    if model_args[\"scaled\"]:\n",
    "        scores = score_classifier(clf, X_test_ohe_scaled, y_test)\n",
    "    else:\n",
    "        scores = score_classifier(clf, X_test_ohe, y_test)\n",
    "    test_scores[model] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_scores).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test scores seem to agree reasonably with the validation scores, so we\n",
    "seem to have reasonable estimates of the performance of these models (which is\n",
    " a relief given the amount of time it took to write the custom cv / grid\n",
    " search loop!)\n",
    "\n",
    "## Closing Comments:\n",
    "\n",
    "As I've mentioned a few times, I'm underway with a notebook to follow this\n",
    "that includes a more in-depth analysis of the above models, with:\n",
    "  * discussion of the model alogrithms\n",
    "  * an explanations of the features the models are using to make predictions\n",
    "  * analysis of individual examples the models are getting right / wrong\n",
    "\n",
    "As things stand, none of these models are exhibiting a level of accuracy that\n",
    "would leave us confident they could provide much in the way of useful\n",
    "inference, or be taken forward into production. Even with better data, we\n",
    "have a very long way to go to see really high levels of accuracy (the likes\n",
    "of 80-90% precision and recall).\n",
    "\n",
    " It's because of this that I'm keen on moving away from the \"rigid\" single\n",
    " estimate of probability approach, to a model that can say more about the\n",
    " uncertainty of a given estimate. That way, even if the model is only right\n",
    " one third of the time, if it's confident about that third and unconfident\n",
    " the rest of the time then we can provide some useful inference when taking\n",
    " ACE referrals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "interesting_models = [\n",
    "    (\"one_hot_balanced\", 'Gradient Boosting Classifier'),\n",
    "    (\"one_hot_balanced\",  'Logistic Regression'),\n",
    "    (\"target_balanced\", \"Support Vector Machines\")\n",
    "]\n",
    "\n",
    "for data_prep_type, model in interesting_models:\n",
    "    print('=' * 50)\n",
    "    print(model)\n",
    "    print('=' * 50)\n",
    "    print(\"Params:\")\n",
    "    for param, value in best_params[data_prep_type][model].items():\n",
    "        print(f\"{param}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}