{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, precision_score\n",
    "from data_prep.data_prep import *\n",
    "from sklearn.model_selection import StratifiedKFold"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "true_neg = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0][0])\n",
    "false_neg = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[1][0])\n",
    "true_pos = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[1][1])\n",
    "false_pos = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0][1])\n",
    "precision = make_scorer(precision_score, zero_division=0)\n",
    "\n",
    "SCORING = {\n",
    "    \"roc_auc\":\"roc_auc\",\n",
    "    \"accuracy\":\"accuracy\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"precision\": precision,\n",
    "    \"true_pos\": true_pos,\n",
    "    \"true_neg\": true_neg,\n",
    "    \"false_pos\": false_pos,\n",
    "    \"false_neg\": false_neg\n",
    "}\n",
    "\n",
    "SCORE_FEATURES = [\n",
    "       'mean_test_roc_auc', 'std_test_roc_auc',\n",
    "       'mean_train_roc_auc', 'std_train_roc_auc',\n",
    "       'mean_test_accuracy', 'std_test_accuracy',\n",
    "       'mean_train_accuracy', 'std_train_accuracy',\n",
    "       'mean_test_recall', 'std_test_recall',\n",
    "       'mean_train_recall', 'std_train_recall',\n",
    "       'mean_test_precision', 'std_test_precision',\n",
    "       'mean_train_precision', 'std_train_precision',\n",
    "       'mean_test_true_pos', 'std_test_true_pos',\n",
    "       'mean_train_true_pos', 'std_train_true_pos',\n",
    "       'mean_test_true_neg', 'std_test_true_neg',\n",
    "       'mean_train_true_neg', 'std_train_true_neg',\n",
    "       'mean_test_false_pos', 'std_test_false_pos',\n",
    "       'mean_train_false_pos', 'std_train_false_pos',\n",
    "       'mean_test_false_neg', 'std_test_false_neg',\n",
    "       'mean_train_false_neg', 'std_train_false_neg'\n",
    "]\n",
    "\n",
    "N_ORIG_EXAMPLES = 345\n",
    "\n",
    "def test_model(clf, X, y, param_grid, **kwargs):\n",
    "\n",
    "    # custom splits for cross validation\n",
    "    # remove synthetic examples from validation splits for more accurate val scoring\n",
    "    splitter = StratifiedKFold(n_splits=3)\n",
    "    contains_synthetic_examples = len(X) > N_ORIG_EXAMPLES\n",
    "    if contains_synthetic_examples:\n",
    "\n",
    "        X_orig, y_orig = X.iloc[:N_ORIG_EXAMPLES], y[:N_ORIG_EXAMPLES]\n",
    "        X_synth, y_synth = X.iloc[N_ORIG_EXAMPLES:], y[N_ORIG_EXAMPLES:]\n",
    "        orig_data_splits = [cv_split for cv_split in splitter.split(X_orig, y_orig)]\n",
    "        synth_data_splits = [cv_split for cv_split in splitter.split(X_synth, y_synth)]\n",
    "\n",
    "        cv_splits = []\n",
    "        for orig_data_split, synth_data_split in zip(orig_data_splits,\n",
    "                                                     synth_data_splits):\n",
    "            orig_train_idxs, orig_val_idxs = orig_data_split\n",
    "            synth_train_idxs, _ = synth_data_split\n",
    "            synth_train_idxs += N_ORIG_EXAMPLES\n",
    "            cv_train_idxs = np.append(orig_train_idxs, synth_train_idxs)\n",
    "            cv_splits.append((cv_train_idxs, orig_val_idxs))\n",
    "    else:\n",
    "        cv_splits = [cv_split for cv_split in splitter.split(X, y)]\n",
    "\n",
    "    search_fit = GridSearchCV(clf,\n",
    "                              param_grid,\n",
    "                              cv=cv_splits,\n",
    "                              scoring=SCORING,\n",
    "                              refit=\"roc_auc\",\n",
    "                              return_train_score=True).fit(X, y, **kwargs)\n",
    "    search_results = pd.DataFrame(search_fit.cv_results_)[SCORE_FEATURES]\n",
    "    return search_fit.best_params_, search_results.iloc[search_fit.best_index_]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def nns(x, y, balanced=False):\n",
    "    clf = KNeighborsClassifier(n_jobs=-2)\n",
    "    param_grid = {'n_neighbors': np.arange(1,10),\n",
    "                    'weights': ['uniform','distance'],\n",
    "                    'metric':['euclidean','manhattan']}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for nearest neighbours\")\n",
    "    return test_model(clf, x, y, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm(X, y, balanced=False):\n",
    "    clf = SVC()\n",
    "    param_grid = {'kernel': ['linear','rbf'],\n",
    "                  'C': np.logspace(2,4,2), # np.logspace(2,5,6)\n",
    "                  'gamma': np.logspace(-4,0.5,1)} # np.logspace(-4,0.5,10)}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return test_model(clf, X, y, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "def gp(X, y, balanced=False):\n",
    "    clf = GaussianProcessClassifier(random_state=0, n_jobs=-2)\n",
    "    param_grid = {'kernel': [1.0 * RBF(1.0)]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for Gaussian Process\")\n",
    "    return test_model(clf, X, y, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rfc(X, y, balanced=False):\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    param_grid = {'max_depth': [4, 6],\n",
    "                  'min_samples_leaf': [3,5,9,17],\n",
    "                  'max_features': [0.3]}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return test_model(clf, X, y, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def gbc(X, y, balanced=False):\n",
    "    clf = GradientBoostingClassifier(n_estimators=100,random_state=0)\n",
    "    param_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "                    'max_depth': [3,4,6],\n",
    "                    'min_samples_leaf': [3,5,9,17],\n",
    "                    'max_features': [x for x in np.linspace(0.2,0.4,4)]}\n",
    "    if balanced:\n",
    "        pos_weight, neg_weight = compute_class_weight(class_weight=\"balanced\",\n",
    "                                                      classes=[1,0],\n",
    "                                                      y=y)\n",
    "        y_weights = y.apply(lambda y: pos_weight if y else neg_weight)\n",
    "    else:\n",
    "        y_weights = np.ones(y.shape)\n",
    "\n",
    "    return test_model(clf, X, y, param_grid, sample_weight=y_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def ab(X, y, balanced=False):\n",
    "    clf = AdaBoostClassifier(random_state=0)\n",
    "    param_grid = {'n_estimators': [100,200],\n",
    "                  'learning_rate': [0.001,0.01,0.1,0.2,0.5]}\n",
    "    if balanced:\n",
    "        pos_weight, neg_weight = compute_class_weight(class_weight=\"balanced\",\n",
    "                                                      classes=[1,0],\n",
    "                                                      y=y)\n",
    "        y_weights = y.apply(lambda y: pos_weight if y else neg_weight)\n",
    "    else:\n",
    "        y_weights = np.ones(y.shape)\n",
    "\n",
    "    return test_model(clf, X, y, param_grid, sample_weight=y_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def nb(X, y, balanced=False):\n",
    "    clf = GaussianNB()\n",
    "    param_grid = {'var_smoothing':  np.logspace(-11,-3,9,base=10)}\n",
    "    if balanced:\n",
    "        pos_weight, neg_weight = compute_class_weight(class_weight=\"balanced\",\n",
    "                                                      classes=[1,0],\n",
    "                                                      y=y)\n",
    "        y_weights = y.apply(lambda y: pos_weight if y else neg_weight)\n",
    "    else:\n",
    "        y_weights = np.ones(y.shape)\n",
    "    return test_model(clf, X, y, param_grid, sample_weight=y_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def lr(X, y, balanced=False):\n",
    "    clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "    param_grid = {'penalty' : ['l2'],\n",
    "                  'solver': [\"liblinear\"],\n",
    "                  'C' : np.logspace(-4, 4, 20)}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return test_model(clf, X, y, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "def qda(X, y, balanced=False):\n",
    "    clf = QuadraticDiscriminantAnalysis()\n",
    "    param_grid = {'reg_param':  [0.0]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for QDA\")\n",
    "    return test_model(clf, X, y, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def lda(X, y, balanced=False):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    param_grid = {'solver':  [\"svd\", \"lsqr\", \"eigen\"],\n",
    "                  \"shrinkage\": [None, \"auto\", 0.1, 0.3, 0.8, 1]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for LDA\")\n",
    "    return test_model(clf, X, y, param_grid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ohe Test\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "done.\n",
      "==================================================\n",
      "target Test\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "done.\n",
      "==================================================\n",
      "ohe_balanced Test\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "no available balancing technique for nearest neighbours\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "no available balancing technique for Gaussian Process\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "no available balancing technique for QDA\n",
      "done.\n",
      "==================================================\n",
      "target_resampled Test\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "no available balancing technique for nearest neighbours\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "no available balancing technique for Gaussian Process\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "no available balancing technique for QDA\n",
      "done.\n",
      "==================================================\n",
      "ohe_resampled Test\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "done.\n",
      "==================================================\n",
      "target_resampled Test\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "ohe_test = {\n",
    "    \"name\": \"ohe\",\n",
    "    \"balanced\": False,\n",
    "    \"cat_encoder\": \"one_hot\",\n",
    "    \"resampled\": False,\n",
    "}\n",
    "\n",
    "target_test = {\n",
    "    \"name\": \"target\",\n",
    "    \"balanced\": False,\n",
    "    \"cat_encoder\": \"target\",\n",
    "    \"resampled\": False,\n",
    "}\n",
    "\n",
    "ohe_balanced_test = {\n",
    "    \"name\": \"ohe_balanced\",\n",
    "    \"balanced\": True,\n",
    "    \"cat_encoder\": \"one_hot\",\n",
    "    \"resampled\": False,\n",
    "}\n",
    "\n",
    "target_balanced_test = {\n",
    "    \"name\": \"target_resampled\",\n",
    "    \"balanced\": True,\n",
    "    \"cat_encoder\": \"target\",\n",
    "    \"resampled\": False,\n",
    "}\n",
    "\n",
    "ohe_resampled_test = {\n",
    "    \"name\": \"ohe_resampled\",\n",
    "    \"balanced\": False,\n",
    "    \"cat_encoder\": \"one_hot\",\n",
    "    \"resampled\": True,\n",
    "}\n",
    "\n",
    "target_resampled_test = {\n",
    "    \"name\": \"target_resampled\",\n",
    "    \"balanced\": False,\n",
    "    \"cat_encoder\": \"target\",\n",
    "    \"resampled\": True,\n",
    "}\n",
    "\n",
    "tests = [ohe_test, target_test, ohe_balanced_test, target_balanced_test,\n",
    "         ohe_resampled_test, target_resampled_test]\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "techniques_dict = {'K Nearest Neighbours': nns, 'Support Vector Machines': svm,\n",
    "                   'Gaussian Process': gp, 'Random Forest Classifier': rfc,\n",
    "                   'Gradient Boosting Classifier': gbc,  'Ada Boost classifier': ab,\n",
    "                   'Gaussian Naieve Bayes': nb, 'Logistic Regression': lr,\n",
    "                   'Quadratic Discriminant Analysis': qda}\n",
    "\n",
    "for test in tests:\n",
    "    print(50* '=')\n",
    "    print(f\"{test['name']} Test\")\n",
    "    print(50* '=')\n",
    "    data_loc = \"../data/ace_data_orig.csv\"\n",
    "    X_train, _, _, _ = return_train_test(loc=data_loc,\n",
    "                                         cat_encoder=test[\"cat_encoder\"],\n",
    "                                         resampled=test[\"resampled\"])\n",
    "    X_train_scaled, y_train, _, _ = return_train_test(loc=data_loc,\n",
    "                                                      cat_encoder=test[\"cat_encoder\"],\n",
    "                                                      resampled=test[\"resampled\"])\n",
    "\n",
    "    results_list = []\n",
    "    for model_type, cv_model_func in techniques_dict.items():\n",
    "        print(f\"fitting {model_type}......\")\n",
    "        if cv_model_func in [nns, svm, gp, lr, lda, qda]:\n",
    "            _, cv_results = cv_model_func(X_train_scaled, y_train, \n",
    "                                          balanced=test[\"balanced\"])\n",
    "            results_list.append(cv_results)\n",
    "        else: # don't normalise x\n",
    "            _, cv_results = cv_model_func(X_train, y_train,\n",
    "                                          balanced=test[\"balanced\"])\n",
    "            results_list.append(cv_results)\n",
    "        print(\"done.\")\n",
    "\n",
    "    test_results[test[\"name\"]] = pd.DataFrame(results_list,\n",
    "                                              index=techniques_dict.keys())\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}