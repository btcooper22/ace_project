{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samrelins/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samrelins/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (make_scorer, confusion_matrix, precision_score,\n",
    "                             f1_score, roc_auc_score, accuracy_score,\n",
    "                             recall_score)\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/samrelins/Documents/LIDA/ace_project/')\n",
    "from src.data_prep import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Models: Analysis and Features\n",
    "\n",
    "The below is a short(ish) script and summary of the data preparation and\n",
    "modelling experiments I have performed. This is to be followed by a more in-depth analysis of a small selection of the best performing models - although, as we'll see, none of the models show impressive predictive accuracy at this point.\n",
    "\n",
    "## Data Preparation Methods\n",
    "\n",
    "The ACE dataset presents a couple of prominent considerations that need to be\n",
    " accounted for when preparing the data for modeling:\n",
    "\n",
    "### 1. Categorical Encoding Methods:\n",
    "\n",
    "Machine learning methods require categorical data to be represented\n",
    "numerically for it to be interpretable. There are a number of ways to\n",
    "approach\n",
    "this, some of which are not possible in this setting because of the small amount of training data. I've focussed on two approaches:\n",
    "\n",
    "**One Hot Encoding** - Each categorical feature is split into\n",
    "individual categories and these categories are assigned a binary value, a\n",
    "1 indicating the feature is present and 0 not present. For example, if we had\n",
    " the following data on the time of referral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Referral Time\n1       morning\n2     afternoon\n3       morning\n4       evening",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>morning</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afternoon</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>morning</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>evening</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time\": [\"morning\", \"afternoon\", \"morning\", \"evening\"],\n",
    "}, index=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "could be one-hot encoded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Referral Time Morning  Referral Time Afternoon  Referral Time Evening\n1                      1                        0                      0\n2                      0                        1                      0\n3                      1                        0                      0\n4                      0                        0                      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time Morning</th>\n      <th>Referral Time Afternoon</th>\n      <th>Referral Time Evening</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time Morning\": [1, 0, 1, 0],\n",
    "    \"Referral Time Afternoon\": [0, 1, 0, 0],\n",
    "    \"Referral Time Evening\": [0, 0, 0, 1],\n",
    "}, index=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue with one-hot encoding is the creation of a large number of extra\n",
    "features (one for each level of each categorical feature) i.e. the above took\n",
    " one category and made it into three. This crates a very \"sparse\" dataset\n",
    " (contains a lot of zeros that don't add much info) and can result in a very\n",
    " sparse model of the data i.e. a tree model that has to make hundreds\n",
    " of yes / no decisions on different binary categories before it can make a\n",
    " prediction. An alternative to this approach is to encode each category with\n",
    " a numerical representation of its value.\n",
    "\n",
    "**Mean encoding / Feature encoding**\n",
    "\n",
    "\n",
    " Target encoding takes the target feature, in this case the need for hospital\n",
    "  treatment, and encodes each categorical feature with the mean / proportion\n",
    "  that applies to the individual \"levels\" of that category. Using the above example, we would calculate the proportion of referrals made in the morning / afternoon / evening that required hospital treatment, and use those proportions as numerical representations of the features. For example, if 15% of children referred in the morning required hospital treatment, and 5% and 18% for the kids referred in the afternoon and evening required hospital treatment, then the feature would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Referral Time  Target Encoded Referral Time\n1       morning                          0.15\n2     afternoon                          0.05\n3       morning                          0.18\n4       evening                          0.15",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Referral Time</th>\n      <th>Target Encoded Referral Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>morning</td>\n      <td>0.15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>afternoon</td>\n      <td>0.05</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>morning</td>\n      <td>0.18</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>evening</td>\n      <td>0.15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Referral Time\": [\"morning\", \"afternoon\", \"morning\", \"evening\"],\n",
    "    \"Target Encoded Referral Time\": [.15, .05, .18,\n",
    "                                   .15],\n",
    "}, index=[1,2,3,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: One must be careful when using this approach, that \"leakage\" isn't\n",
    "introduced\n",
    " into the dataset - that is, information about the target feature for that\n",
    " example being included in the explanatory variables for the same example.\n",
    " This can be avoided by ensuring that the target value for each example is\n",
    " left out when calculating its encodings.\n",
    "\n",
    "Target encoding fixes the sparcity issue - each categorical feature remains\n",
    "one feature rather than expanding, but it often results in overfitting - that\n",
    " is, when the model maps too closely to the examples it has seen in training\n",
    " and doesn't then generalise well when given new data.\n",
    "\n",
    "### 2. Balancing Positive / Negative Examples:\n",
    "\n",
    "The ACE dataset is heavily imbalanced i.e. only 16.5% of examples require\n",
    "hospital treatment. Left as is, models can easily achieve high (83.5%)\n",
    "accuracy by simply predicting ALL children can be treated by ACE. This\n",
    "wouldn't be a very useful model!\n",
    "\n",
    "To avoid this, efforts need to be made to balance the predictions made by\n",
    "each model. Again, I have used two basic approaches to achieve this:\n",
    "\n",
    "**1. Weighting Labels**:\n",
    "\n",
    "The penalty a model is given for making an incorrect prediction can be\n",
    "weighted to penalise the minority label incorrect guesses more\n",
    "heavily. This discourages the model from simply guessing the majority label\n",
    "over and over, as it gets a heavier penalty when it gets one of the minority\n",
    "examples wrong.\n",
    " The\n",
    "weight is usually chosen to be proportional to the imbalance i.e. if there\n",
    "are 5 times more negative examples (children that can be treated by ACE) than\n",
    "positive (children that require hospital treatment), then\n",
    " an incorrect\n",
    "negative\n",
    "guess is penalised 5 times more than an incorrect positive.\n",
    "\n",
    "**2. SMOTE - Synthetic Minority Oversampling TEchnique**\n",
    "\n",
    "This uses a statistical model to create synthetic examples from the minority\n",
    "label to balance the number of positive / negative examples to 50/50. The\n",
    "simplest form of oversampling is to simply duplicate the minority examples\n",
    "over and over. SMOTE uses interpolation between the different minority\n",
    "examples to create synthetic examples that roughly preserve the distribution of\n",
    " the\n",
    "original examples.\n",
    "\n",
    "### Data Preparation Pipeline\n",
    "\n",
    "I've spent some time developing a \"pipeline\" or group of functions that can\n",
    "automatically apply the above encoding and balancing techniques to the\n",
    "data \"at the flick of a switch\". This means that, during training, the\n",
    "different data preparation methods can be easily and consistently\n",
    "applied to the data at runtime, without having to store many different versions\n",
    " of the ace dataset. The importance of this will become clear in the discussion of model\n",
    "evaluation and cross validation below. Given the general utility of these\n",
    "functions, and the fact they are fairly verbose, I have extracted them into a\n",
    " separate module - `data_prep.py` in which script and detailed documentation\n",
    " can be found.\n",
    "\n",
    "A quick summary of the pipeline functions is as follows:\n",
    "\n",
    "* `clean_data`: converts raw excel / csv data into a more python friendly format\n",
    "* `fill_nas`: fills missing values in dataset with group means\n",
    "* `add_features`: add the extra categorical features discussed in the data\n",
    "analysis\n",
    "* `return_train_test`: divides the dataset into consistent train and test\n",
    "dataframes\n",
    "* `add_synthetic_examples`: generates SMOTE (synthetic) examples and adds to\n",
    "dataset\n",
    "* `encode_and_scale`: applies various categorical encoding techniques and\n",
    "min/max scales the data (for modelling techniques that require scaled data)\n",
    "\n",
    "## Model Training and Evaluating Performance:\n",
    "\n",
    "Having considered data preparation, we now need to define models to predict\n",
    "the\n",
    "hospital /\n",
    " community outcomes.\n",
    "\n",
    "### Models and Parameters\n",
    "\n",
    "The modelling techniques used are too numerous to attempt any discussion\n",
    "here, but further exploration of the most successful techniques will be\n",
    "included in\n",
    " the\n",
    "more detailed discussion that will follow. Each modelling technique\n",
    "includes a number of\n",
    "parameters or \"assumptions\" that need to be specified when defining the model\n",
    " , and have a downstream effect on performance and prediction\n",
    " accuracy. To simplify and compartmentalise each of the models we wish to\n",
    " test along with its parameters (an extension of Ruaridh's work), we have\n",
    " a number of functions that return a model and a \"parameter grid\" of\n",
    "  each of the parameters we wish to test.\n",
    "\n",
    "Note: each function has a \"balanced\" argument allowing for the\n",
    "calculation and use of balanced weights, which is not implemented in the\n",
    "parameters of some models (hence the separate `scaled` keyword argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def return_knn_params(balanced=False):\n",
    "    clf = KNeighborsClassifier()\n",
    "    param_grid = {'n_neighbors': np.arange(1,10),\n",
    "                    'weights': ['uniform','distance'],\n",
    "                    'p': [1,2],\n",
    "                    'metric':['minkowski','euclidean','manhattan'],\n",
    "                    'n_jobs':[-2]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for nearest neighbours\")\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "def return_svm_params(balanced=False):\n",
    "    clf = SVC()\n",
    "    param_grid = {'kernel': ['linear','rbf'],\n",
    "                  'C': np.logspace(2,4,2), # np.logspace(2,5,6)\n",
    "                  'gamma': np.logspace(-4,0.5,1)} # np.logspace(-4,0.5,10)}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "def return_gaussian_process_params(balanced=False):\n",
    "    clf = GaussianProcessClassifier(random_state=0, n_jobs=-2)\n",
    "    kernels = [mul * RBF(length_scale)\n",
    "                    for mul in np.arange(0.5, 2.5, 0.5)\n",
    "                    for length_scale in np.arange(0.5, 2.5, 0.5)]\n",
    "    param_grid = {'kernel': kernels,\n",
    "                  'n_jobs': [-2]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for Gaussian Process\")\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def return_random_forest_params(balanced=False):\n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    param_grid = {'max_depth': [4, 6, 10, 14, 20],\n",
    "                  'n_estimators': [30, 100, 130, 300],\n",
    "                  'min_samples_split': [2, 3, 10, 13, 30],\n",
    "                  'max_features': [0.3, 0.4, 0.5, \"auto\"],\n",
    "                  'n_jobs': [-2]}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def return_grad_boost_params(balanced=False):\n",
    "    clf = GradientBoostingClassifier(n_estimators=100,random_state=0)\n",
    "    param_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "                  'n_estimators': [30, 100, 130, 300],\n",
    "                  'max_depth': [4, 6, 10, 14, 20],\n",
    "                  'min_samples_split': [3, 10, 13, 30],\n",
    "                  'max_features': [x for x in np.linspace(0.2,0.4,4)]}\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": False,\n",
    "            \"weight_y\": balanced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "def return_ada_boost_params(balanced=False):\n",
    "    clf = AdaBoostClassifier(random_state=0)\n",
    "    param_grid = {'n_estimators': [30, 100, 130, 300],\n",
    "                  'learning_rate': [0.001,0.01,0.1,0.2,0.5]}\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": False,\n",
    "            \"weight_y\":balanced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def return_naive_bayes_params(balanced=False):\n",
    "    clf = GaussianNB()\n",
    "    param_grid = {'var_smoothing':  np.logspace(-11,-3,9,base=10)}\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": False,\n",
    "            \"weight_y\":balanced}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def return_lr_params(balanced=False):\n",
    "    clf = LogisticRegression(random_state=0, max_iter=10000)\n",
    "    param_grid = {'penalty' : ['l2'],\n",
    "                  'solver': [\"liblinear\"],\n",
    "                  'C' : np.logspace(-4, 4, 20)}\n",
    "    if balanced:\n",
    "        param_grid[\"class_weight\"] = \"balanced\",\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "def return_qda_params(balanced=False):\n",
    "    clf = QuadraticDiscriminantAnalysis()\n",
    "    param_grid = {'reg_param':  [0.0, 0.01, 0.03, 0.1, 0.3]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for QDA\")\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def return_lda_params(balanced=False):\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    param_grid = {'solver':  [\"svd\", \"lsqr\", \"eigen\"],\n",
    "                  \"shrinkage\": [None, \"auto\", 0.1, 0.3, 0.8, 1]}\n",
    "    if balanced:\n",
    "        print(\"no available balancing technique for LDA\")\n",
    "    return {\"clf\": clf, \"param_grid\": param_grid, \"scaled\": True}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cross Validation and Grid Search\n",
    "\n",
    "For those keeping score, we now have several different data preparation techniques,\n",
    "models. We can't use them all simultaneously (well - technically that is\n",
    "actually possible but would be a needlessly complex solution) and so we need\n",
    "to decide on the best combination. To evaluate each possible permutation of\n",
    "data preparation / model / parameters we\n",
    "can use a technique called \"cross validation\": dividing the\n",
    "training data into k\n",
    "groups, training the model on k-1 of these groups leaving one group aside, and\n",
    " evaluating the model's predictions against the group held aside. Doing this\n",
    " ensures the model is never evaluated on examples it has already seen. This\n",
    " technique is used in conjunction with a parameter optimisation method called\n",
    "  \"grid search\" - this iterates through each possible combination of the\n",
    "  specified parameters, scoring each individually. The best combination of\n",
    "  parameters can then be established.\n",
    "\n",
    "There are several \"out of the box\" implementations of these methods that can\n",
    "be applied in most use cases. However, these functions require you to specify\n",
    " a pipeline that can be compartmentalised into distinct stages and applied\n",
    " across the whole training dataset. This isn't practical in this case as:\n",
    "\n",
    " * The synthesizing and encoding stages can't be divorced from one another,\n",
    " otherwise the SMOTE examples may extrapolate between target-encoded\n",
    " features to produce nonsense examples\n",
    " * Only the training splits can include synthetic data - otherwise model\n",
    " performance will be based in part on its ability to predict synthetic data\n",
    " and will result in a biased cross validation score\n",
    "\n",
    "Therefore, I've spent some time developing a custom cross validation and grid\n",
    " search loop that produces unbiased validation scores. The function includes\n",
    " the different data preparation techniques and pipeline functions discussed\n",
    " above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import (make_scorer, confusion_matrix, precision_score,\n",
    "                             f1_score, roc_auc_score, accuracy_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/samrelins/Documents/LIDA/ace_project/')\n",
    "from src.data_prep import *\n",
    "\n",
    "# custom scoring functions for CV loop\n",
    "true_neg = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0][0])\n",
    "false_neg = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[1][0])\n",
    "true_pos = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[1][1])\n",
    "false_pos = make_scorer(lambda y, y_pred: confusion_matrix(y, y_pred)[0][1])\n",
    "precision = make_scorer(precision_score, zero_division=0)\n",
    "\n",
    "# dict of scoring functions\n",
    "SCORING = {\n",
    "    \"f1\": make_scorer(f1_score),\n",
    "    \"roc_auc\": make_scorer(roc_auc_score),\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"true_pos\": true_pos,\n",
    "    \"true_neg\": true_neg,\n",
    "    \"false_pos\": false_pos,\n",
    "    \"false_neg\": false_neg\n",
    "}\n",
    "\n",
    "\n",
    "def score_classifier(clf, X, y):\n",
    "    \"\"\"\n",
    "    Scores a classifier against metrics in SCORING dict\n",
    "\n",
    "    :param clf: (object: sklearn classifier) classifier to be scored\n",
    "    :param X: (object: pandas DataFrame) matrix of training vectors\n",
    "    :param y: (object: pandas Series) vector of target labels\n",
    "    :return: (dict) group of {score function name: score} pairs\n",
    "    \"\"\"\n",
    "\n",
    "    scores = {}\n",
    "    for name, scorer in SCORING.items():\n",
    "        scores[name] = scorer(clf, X, y)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def cv_score_classifier(clf, X_train, y_train, params,\n",
    "                        cat_encoder=\"one_hot\",\n",
    "                        add_synthetic=False,\n",
    "                        scaled=False,\n",
    "                        n_splits=3,\n",
    "                        weight_y=False):\n",
    "    \"\"\"\n",
    "    Custom CV loop to score classifier functions\n",
    "\n",
    "    Implemented to account for SMOTE example generation and target encoding -\n",
    "    both should only be performed on training data and not validation data -\n",
    "    not possible to achieve this separation using the sklearn pipeline and\n",
    "    GridSearchCV.\n",
    "\n",
    "    :param clf: (object: sklearn classifier) classifier to train and score\n",
    "    :param X_train: (object: pandas DataFrame) Explanatory Training data\n",
    "    :param y_train: (object: pandas Series) Training data labels\n",
    "    :param params: (dict) parameters for classifier\n",
    "    :param cat_encoder: (str: \"one_hot\") categorical encoder for data\n",
    "    either \"one_hot\" / \"target\"\n",
    "    :param add_synthetic: (bool: False) set True to add SMOTE examples before\n",
    "    training\n",
    "    :param scaled: (bool: False) set True to scale numeric features\n",
    "    :param n_splits: (int: 3) number of splilts for CV loop\n",
    "    :param weight_y: (bool: False) set True if clf requires sample_weights\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # create splits for CV loop\n",
    "    splitter = StratifiedKFold(n_splits=n_splits, random_state=1)\n",
    "    splits = list(splitter.split(X_train, y_train))\n",
    "\n",
    "    total_cv_scores = {} # dict to store cumulative CV scores\n",
    "    for train_idxs, val_idxs  in splits:\n",
    "        # divide data into train and validation sets for this cv loop\n",
    "        cv_X_train, cv_y_train, X_val, y_val = (X_train.iloc[train_idxs],\n",
    "                                                y_train.iloc[train_idxs],\n",
    "                                                X_train.iloc[val_idxs],\n",
    "                                                y_train.iloc[val_idxs])\n",
    "\n",
    "        if add_synthetic: # add SMOTE examples to balance data if required\n",
    "            cv_X_train, cv_y_train = add_synthetic_examples(cv_X_train, cv_y_train)\n",
    "\n",
    "        # encode categorical features and scale numeric if required\n",
    "        cv_X_train, X_val, = encode_and_scale(\n",
    "            cv_X_train, cv_y_train, X_val,\n",
    "            cat_encoder=cat_encoder,\n",
    "            scaled=scaled)\n",
    "\n",
    "        if weight_y:\n",
    "            # calculate array of weights for y labels\n",
    "            pos_weight, neg_weight = compute_class_weight(\n",
    "                class_weight=\"balanced\",\n",
    "                classes=[1,0],\n",
    "                y=cv_y_train)\n",
    "            y_weights = cv_y_train.apply(lambda y: pos_weight if y else neg_weight)\n",
    "            # train model using parameters, weights and cv loop data\n",
    "            cv_clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(cv_X_train, cv_y_train, sample_weight=y_weights))\n",
    "        else:\n",
    "            # train model using parameters and cv loop data\n",
    "            cv_clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(cv_X_train, cv_y_train))\n",
    "\n",
    "        # score classifier on cv validation set and add scores to total\n",
    "        scores = score_classifier(cv_clf, X_val, y_val)\n",
    "        if total_cv_scores:\n",
    "            for key, value in scores.items():\n",
    "                total_cv_scores[key] += value\n",
    "        else:\n",
    "            total_cv_scores = scores\n",
    "\n",
    "    mean_cv_scores = {}\n",
    "    for key, value in total_cv_scores.items():\n",
    "        mean_cv_scores[key] = value / n_splits\n",
    "\n",
    "    return mean_cv_scores\n",
    "\n",
    "\n",
    "def param_search_classifier(param_grid, **kwargs):\n",
    "    \"\"\"\n",
    "    custom param grid search to compliment cv_score_classifier function\n",
    "\n",
    "    :param param_grid: (dict) parameters on which to perform grid search\n",
    "    :param kwargs: arguments for cv_score_classifier function\n",
    "    :return: (dict: best_scores, dict: best_params) scores and parameters for\n",
    "    highest scoring model\n",
    "    \"\"\"\n",
    "    param_grid = ParameterGrid(param_grid)\n",
    "    # variable to store best param combo and relevant scores\n",
    "    best_scores = {}\n",
    "    best_params = {}\n",
    "    for params in param_grid:\n",
    "        mean_cv_scores = cv_score_classifier(params=params,\n",
    "                                             **kwargs)\n",
    "        if not best_scores:\n",
    "            best_scores = mean_cv_scores\n",
    "            best_params = params\n",
    "        elif mean_cv_scores[\"f1\"] > best_scores[\"f1\"]:\n",
    "            best_scores = mean_cv_scores\n",
    "            best_params = params\n",
    "\n",
    "    return best_scores, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance Metrics\n",
    "\n",
    "The cross validation loop outputs the following metrics, used to measure\n",
    "model performance:\n",
    "\n",
    "* **True Positive / False Positive / True Negative / False Negative**: Fairly\n",
    "self\n",
    " explanatory. A true positive in this context is an example a model correctly\n",
    "  states requires hospital treatment, a true negative is an example the model\n",
    "   states needs hospital treatment when it doesn't, and so on....\n",
    "* **Accuracy**: Again fairly self explanatory. The proportion of\n",
    " correct predictions\n",
    "* **Precision**: the proportion of positive guesses that are\n",
    "correct i.e. if a model has a precision of 75%, 3 out of every 4 times it\n",
    "predicts that hospital treatment is needed it is correct.\n",
    "* **Recall**: the proportion of positive examples in the dataset that the\n",
    "model correctly predicts i.e. if there are 50 examples requiring hospital\n",
    "treatment and the model correctly identifies 40 of them, it has an 80% recall.\n",
    "* **ROC/AUC**: this is a measure of the tradeoff between precision and\n",
    "recall, but is a little complex to define here. A 0.5 ROC/AUC is\n",
    "representative of random chance and 1 is a perfect model.\n",
    "* **F1 Score**: the f1 score is another measure of the tradeoff between precision and recall. It is a weighted average of the two and ranges from 0 (worst) to 1 (perfect)\n",
    "\n",
    "## Tests\n",
    "\n",
    "The following is the (perhaps long awaited!) output from the training /\n",
    "validation. Scores are broken down by data preparation methods and then model\n",
    " type - the best performing model for each is selected from the cv loop and\n",
    " displayed in the results:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Data Prep: one_hot_balanced\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "no available balancing technique for nearest neighbours\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "no available balancing technique for Gaussian Process\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "no available balancing technique for QDA\n",
      "done.\n",
      "==================================================\n",
      "Data Prep: target_balanced\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "no available balancing technique for nearest neighbours\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "no available balancing technique for Gaussian Process\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "no available balancing technique for QDA\n",
      "done.\n",
      "==================================================\n",
      "Data Prep: one_hot_resampled\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "done.\n",
      "==================================================\n",
      "Data Prep: target_resampled\n",
      "==================================================\n",
      "fitting K Nearest Neighbours......\n",
      "done.\n",
      "fitting Support Vector Machines......\n",
      "done.\n",
      "fitting Gaussian Process......\n",
      "done.\n",
      "fitting Random Forest Classifier......\n",
      "done.\n",
      "fitting Gradient Boosting Classifier......\n",
      "done.\n",
      "fitting Ada Boost classifier......\n",
      "done.\n",
      "fitting Gaussian Naieve Bayes......\n",
      "done.\n",
      "fitting Logistic Regression......\n",
      "done.\n",
      "fitting Quadratic Discriminant Analysis......\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "techniques_dict = {'K Nearest Neighbours': return_knn_params,\n",
    "                   'Support Vector Machines': return_svm_params,\n",
    "                   'Gaussian Process': return_gaussian_process_params,\n",
    "                   'Random Forest Classifier': return_random_forest_params,\n",
    "                   'Gradient Boosting Classifier': return_grad_boost_params,\n",
    "                   'Ada Boost classifier': return_ada_boost_params,\n",
    "                   'Gaussian Naieve Bayes': return_naive_bayes_params,\n",
    "                   'Logistic Regression': return_lr_params,\n",
    "                   'Quadratic Discriminant Analysis': return_qda_params}\n",
    "\n",
    "data_prep_types = [\"one_hot_balanced\", \"target_balanced\",\n",
    "                    \"one_hot_resampled\", \"target_resampled\"]\n",
    "\n",
    "# data_loc = \"/Users/samrelins/Documents/LIDA/ace_project/data/ace_data_orig.csv\"\n",
    "data_loc = \"/Users/samrelins/Documents/LIDA/ace_project/data/ace_data_extra.csv\"\n",
    "ace_data_orig = pd.read_csv(data_loc)\n",
    "ace_data_orig.drop([\"medical_history\", \"examination_summary\",\n",
    "                    \"recommendation\"],\n",
    "                   axis=1, inplace=True)\n",
    "X_train, y_train, X_test, y_test = return_train_test(ace_data_orig)\n",
    "\n",
    "best_params = {}\n",
    "best_model_scores = {}\n",
    "\n",
    "for data_prep_type in data_prep_types:\n",
    "    ### uncomment this and other print statements for output of loop progress\n",
    "    print(50* '=')\n",
    "    print(f\"Data Prep: {data_prep_type}\")\n",
    "    print(50* '=')\n",
    "\n",
    "    cat_encoder = \"one_hot\" if \"one_hot\" in data_prep_type else \"target\"\n",
    "    balanced = True if \"balanced\" in data_prep_type else False\n",
    "    add_synthetic = True if \"resampled\" in data_prep_type else False\n",
    "\n",
    "    scores_list = []\n",
    "    best_loop_params = {}\n",
    "    for model_name, model_params_f in techniques_dict.items():\n",
    "        print(f\"fitting {model_name}......\")\n",
    "        model_best_scores, model_best_params = param_search_classifier(\n",
    "            **model_params_f(balanced=balanced),\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            cat_encoder=cat_encoder,\n",
    "            add_synthetic=add_synthetic)\n",
    "        scores_list.append(model_best_scores)\n",
    "        best_loop_params[model_name] = model_best_params\n",
    "        print(\"done.\")\n",
    "\n",
    "    best_model_scores[data_prep_type] = pd.DataFrame(scores_list,\n",
    "                                                     index=techniques_dict.keys())\n",
    "    best_params[data_prep_type] = best_loop_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>One Hot Balanced"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fda78e3e6a0>",
      "text/html": "<style  type=\"text/css\" >\n#T_24590_row0_col0,#T_24590_row0_col1,#T_24590_row0_col2,#T_24590_row0_col3,#T_24590_row0_col4,#T_24590_row0_col5,#T_24590_row0_col6,#T_24590_row0_col7,#T_24590_row0_col8,#T_24590_row1_col0,#T_24590_row1_col1,#T_24590_row1_col2,#T_24590_row1_col3,#T_24590_row1_col4,#T_24590_row1_col5,#T_24590_row1_col6,#T_24590_row1_col7,#T_24590_row1_col8,#T_24590_row2_col0,#T_24590_row2_col1,#T_24590_row2_col2,#T_24590_row2_col3,#T_24590_row2_col4,#T_24590_row2_col5,#T_24590_row2_col6,#T_24590_row2_col7,#T_24590_row2_col8,#T_24590_row3_col0,#T_24590_row3_col1,#T_24590_row3_col2,#T_24590_row3_col3,#T_24590_row3_col4,#T_24590_row3_col5,#T_24590_row3_col6,#T_24590_row3_col7,#T_24590_row3_col8,#T_24590_row4_col0,#T_24590_row4_col1,#T_24590_row4_col2,#T_24590_row4_col3,#T_24590_row4_col4,#T_24590_row4_col5,#T_24590_row4_col6,#T_24590_row4_col7,#T_24590_row4_col8,#T_24590_row5_col0,#T_24590_row5_col1,#T_24590_row5_col2,#T_24590_row5_col3,#T_24590_row5_col4,#T_24590_row5_col5,#T_24590_row5_col6,#T_24590_row5_col7,#T_24590_row5_col8,#T_24590_row6_col0,#T_24590_row6_col1,#T_24590_row6_col2,#T_24590_row6_col3,#T_24590_row6_col4,#T_24590_row6_col5,#T_24590_row6_col6,#T_24590_row6_col7,#T_24590_row6_col8,#T_24590_row7_col0,#T_24590_row7_col1,#T_24590_row7_col2,#T_24590_row7_col3,#T_24590_row7_col4,#T_24590_row7_col5,#T_24590_row7_col6,#T_24590_row7_col7,#T_24590_row7_col8,#T_24590_row8_col0,#T_24590_row8_col1,#T_24590_row8_col2,#T_24590_row8_col3,#T_24590_row8_col4,#T_24590_row8_col5,#T_24590_row8_col6,#T_24590_row8_col7,#T_24590_row8_col8{\n            background-color: ;\n        }</style><table id=\"T_24590_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >f1</th>        <th class=\"col_heading level0 col1\" >roc_auc</th>        <th class=\"col_heading level0 col2\" >accuracy</th>        <th class=\"col_heading level0 col3\" >recall</th>        <th class=\"col_heading level0 col4\" >precision</th>        <th class=\"col_heading level0 col5\" >true_pos</th>        <th class=\"col_heading level0 col6\" >true_neg</th>        <th class=\"col_heading level0 col7\" >false_pos</th>        <th class=\"col_heading level0 col8\" >false_neg</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_24590_level0_row0\" class=\"row_heading level0 row0\" >K Nearest Neighbours</th>\n                        <td id=\"T_24590_row0_col0\" class=\"data row0 col0\" >0.101154</td>\n                        <td id=\"T_24590_row0_col1\" class=\"data row0 col1\" >0.469373</td>\n                        <td id=\"T_24590_row0_col2\" class=\"data row0 col2\" >0.721713</td>\n                        <td id=\"T_24590_row0_col3\" class=\"data row0 col3\" >0.092593</td>\n                        <td id=\"T_24590_row0_col4\" class=\"data row0 col4\" >0.119192</td>\n                        <td id=\"T_24590_row0_col5\" class=\"data row0 col5\" >1.666667</td>\n                        <td id=\"T_24590_row0_col6\" class=\"data row0 col6\" >77.000000</td>\n                        <td id=\"T_24590_row0_col7\" class=\"data row0 col7\" >14.000000</td>\n                        <td id=\"T_24590_row0_col8\" class=\"data row0 col8\" >16.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_24590_level0_row1\" class=\"row_heading level0 row1\" >Support Vector Machines</th>\n                        <td id=\"T_24590_row1_col0\" class=\"data row1 col0\" >0.255730</td>\n                        <td id=\"T_24590_row1_col1\" class=\"data row1 col1\" >0.519027</td>\n                        <td id=\"T_24590_row1_col2\" class=\"data row1 col2\" >0.556575</td>\n                        <td id=\"T_24590_row1_col3\" class=\"data row1 col3\" >0.462963</td>\n                        <td id=\"T_24590_row1_col4\" class=\"data row1 col4\" >0.176755</td>\n                        <td id=\"T_24590_row1_col5\" class=\"data row1 col5\" >8.333333</td>\n                        <td id=\"T_24590_row1_col6\" class=\"data row1 col6\" >52.333333</td>\n                        <td id=\"T_24590_row1_col7\" class=\"data row1 col7\" >38.666667</td>\n                        <td id=\"T_24590_row1_col8\" class=\"data row1 col8\" >9.666667</td>\n            </tr>\n            <tr>\n                        <th id=\"T_24590_level0_row2\" class=\"row_heading level0 row2\" >Gaussian Process</th>\n                        <td id=\"T_24590_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n                        <td id=\"T_24590_row2_col1\" class=\"data row2 col1\" >0.500000</td>\n                        <td id=\"T_24590_row2_col2\" class=\"data row2 col2\" >0.834862</td>\n                        <td id=\"T_24590_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n                        <td id=\"T_24590_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n                        <td id=\"T_24590_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n                        <td id=\"T_24590_row2_col6\" class=\"data row2 col6\" >91.000000</td>\n                        <td id=\"T_24590_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n                        <td id=\"T_24590_row2_col8\" class=\"data row2 col8\" >18.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_24590_level0_row3\" class=\"row_heading level0 row3\" >Random Forest Classifier</th>\n                        <td id=\"T_24590_row3_col0\" class=\"data row3 col0\" >0.271862</td>\n                        <td id=\"T_24590_row3_col1\" class=\"data row3 col1\" >0.562068</td>\n                        <td id=\"T_24590_row3_col2\" class=\"data row3 col2\" >0.740061</td>\n                        <td id=\"T_24590_row3_col3\" class=\"data row3 col3\" >0.296296</td>\n                        <td id=\"T_24590_row3_col4\" class=\"data row3 col4\" >0.251299</td>\n                        <td id=\"T_24590_row3_col5\" class=\"data row3 col5\" >5.333333</td>\n                        <td id=\"T_24590_row3_col6\" class=\"data row3 col6\" >75.333333</td>\n                        <td id=\"T_24590_row3_col7\" class=\"data row3 col7\" >15.666667</td>\n                        <td id=\"T_24590_row3_col8\" class=\"data row3 col8\" >12.666667</td>\n            </tr>\n            <tr>\n                        <th id=\"T_24590_level0_row4\" class=\"row_heading level0 row4\" >Gradient Boosting Classifier</th>\n                        <td id=\"T_24590_row4_col0\" class=\"data row4 col0\" >0.227404</td>\n                        <td id=\"T_24590_row4_col1\" class=\"data row4 col1\" >0.518213</td>\n                        <td id=\"T_24590_row4_col2\" class=\"data row4 col2\" >0.654434</td>\n                        <td id=\"T_24590_row4_col3\" class=\"data row4 col3\" >0.314815</td>\n                        <td id=\"T_24590_row4_col4\" class=\"data row4 col4\" >0.179215</td>\n                        <td id=\"T_24590_row4_col5\" class=\"data row4 col5\" >5.666667</td>\n                        <td id=\"T_24590_row4_col6\" class=\"data row4 col6\" >65.666667</td>\n                        <td id=\"T_24590_row4_col7\" class=\"data row4 col7\" >25.333333</td>\n                        <td id=\"T_24590_row4_col8\" class=\"data row4 col8\" >12.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_24590_level0_row5\" class=\"row_heading level0 row5\" >Ada Boost classifier</th>\n                        <td id=\"T_24590_row5_col0\" class=\"data row5 col0\" >0.298739</td>\n                        <td id=\"T_24590_row5_col1\" class=\"data row5 col1\" >0.573464</td>\n                        <td id=\"T_24590_row5_col2\" class=\"data row5 col2\" >0.709480</td>\n                        <td id=\"T_24590_row5_col3\" class=\"data row5 col3\" >0.370370</td>\n                        <td id=\"T_24590_row5_col4\" class=\"data row5 col4\" >0.251095</td>\n                        <td id=\"T_24590_row5_col5\" class=\"data row5 col5\" >6.666667</td>\n                        <td id=\"T_24590_row5_col6\" class=\"data row5 col6\" >70.666667</td>\n                        <td id=\"T_24590_row5_col7\" class=\"data row5 col7\" >20.333333</td>\n                        <td id=\"T_24590_row5_col8\" class=\"data row5 col8\" >11.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_24590_level0_row6\" class=\"row_heading level0 row6\" >Gaussian Naieve Bayes</th>\n                        <td id=\"T_24590_row6_col0\" class=\"data row6 col0\" >0.284968</td>\n                        <td id=\"T_24590_row6_col1\" class=\"data row6 col1\" >0.510582</td>\n                        <td id=\"T_24590_row6_col2\" class=\"data row6 col2\" >0.232416</td>\n                        <td id=\"T_24590_row6_col3\" class=\"data row6 col3\" >0.925926</td>\n                        <td id=\"T_24590_row6_col4\" class=\"data row6 col4\" >0.168418</td>\n                        <td id=\"T_24590_row6_col5\" class=\"data row6 col5\" >16.666667</td>\n                        <td id=\"T_24590_row6_col6\" class=\"data row6 col6\" >8.666667</td>\n                        <td id=\"T_24590_row6_col7\" class=\"data row6 col7\" >82.333333</td>\n                        <td id=\"T_24590_row6_col8\" class=\"data row6 col8\" >1.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_24590_level0_row7\" class=\"row_heading level0 row7\" >Logistic Regression</th>\n                        <td id=\"T_24590_row7_col0\" class=\"data row7 col0\" >0.268232</td>\n                        <td id=\"T_24590_row7_col1\" class=\"data row7 col1\" >0.544668</td>\n                        <td id=\"T_24590_row7_col2\" class=\"data row7 col2\" >0.599388</td>\n                        <td id=\"T_24590_row7_col3\" class=\"data row7 col3\" >0.462963</td>\n                        <td id=\"T_24590_row7_col4\" class=\"data row7 col4\" >0.191047</td>\n                        <td id=\"T_24590_row7_col5\" class=\"data row7 col5\" >8.333333</td>\n                        <td id=\"T_24590_row7_col6\" class=\"data row7 col6\" >57.000000</td>\n                        <td id=\"T_24590_row7_col7\" class=\"data row7 col7\" >34.000000</td>\n                        <td id=\"T_24590_row7_col8\" class=\"data row7 col8\" >9.666667</td>\n            </tr>\n            <tr>\n                        <th id=\"T_24590_level0_row8\" class=\"row_heading level0 row8\" >Quadratic Discriminant Analysis</th>\n                        <td id=\"T_24590_row8_col0\" class=\"data row8 col0\" >0.091270</td>\n                        <td id=\"T_24590_row8_col1\" class=\"data row8 col1\" >0.511294</td>\n                        <td id=\"T_24590_row8_col2\" class=\"data row8 col2\" >0.816514</td>\n                        <td id=\"T_24590_row8_col3\" class=\"data row8 col3\" >0.055556</td>\n                        <td id=\"T_24590_row8_col4\" class=\"data row8 col4\" >0.277778</td>\n                        <td id=\"T_24590_row8_col5\" class=\"data row8 col5\" >1.000000</td>\n                        <td id=\"T_24590_row8_col6\" class=\"data row8 col6\" >88.000000</td>\n                        <td id=\"T_24590_row8_col7\" class=\"data row8 col7\" >3.000000</td>\n                        <td id=\"T_24590_row8_col8\" class=\"data row8 col8\" >17.000000</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>Target Balanced"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fda6eb76250>",
      "text/html": "<style  type=\"text/css\" >\n#T_d3fee_row0_col0,#T_d3fee_row0_col1,#T_d3fee_row0_col2,#T_d3fee_row0_col3,#T_d3fee_row0_col4,#T_d3fee_row0_col5,#T_d3fee_row0_col6,#T_d3fee_row0_col7,#T_d3fee_row0_col8,#T_d3fee_row1_col0,#T_d3fee_row1_col1,#T_d3fee_row1_col2,#T_d3fee_row1_col3,#T_d3fee_row1_col4,#T_d3fee_row1_col5,#T_d3fee_row1_col6,#T_d3fee_row1_col7,#T_d3fee_row1_col8,#T_d3fee_row2_col0,#T_d3fee_row2_col1,#T_d3fee_row2_col2,#T_d3fee_row2_col3,#T_d3fee_row2_col4,#T_d3fee_row2_col5,#T_d3fee_row2_col6,#T_d3fee_row2_col7,#T_d3fee_row2_col8,#T_d3fee_row3_col0,#T_d3fee_row3_col1,#T_d3fee_row3_col2,#T_d3fee_row3_col3,#T_d3fee_row3_col4,#T_d3fee_row3_col5,#T_d3fee_row3_col6,#T_d3fee_row3_col7,#T_d3fee_row3_col8,#T_d3fee_row4_col0,#T_d3fee_row4_col1,#T_d3fee_row4_col2,#T_d3fee_row4_col3,#T_d3fee_row4_col4,#T_d3fee_row4_col5,#T_d3fee_row4_col6,#T_d3fee_row4_col7,#T_d3fee_row4_col8,#T_d3fee_row5_col0,#T_d3fee_row5_col1,#T_d3fee_row5_col2,#T_d3fee_row5_col3,#T_d3fee_row5_col4,#T_d3fee_row5_col5,#T_d3fee_row5_col6,#T_d3fee_row5_col7,#T_d3fee_row5_col8,#T_d3fee_row6_col0,#T_d3fee_row6_col1,#T_d3fee_row6_col2,#T_d3fee_row6_col3,#T_d3fee_row6_col4,#T_d3fee_row6_col5,#T_d3fee_row6_col6,#T_d3fee_row6_col7,#T_d3fee_row6_col8,#T_d3fee_row7_col0,#T_d3fee_row7_col1,#T_d3fee_row7_col2,#T_d3fee_row7_col3,#T_d3fee_row7_col4,#T_d3fee_row7_col5,#T_d3fee_row7_col6,#T_d3fee_row7_col7,#T_d3fee_row7_col8,#T_d3fee_row8_col0,#T_d3fee_row8_col1,#T_d3fee_row8_col2,#T_d3fee_row8_col3,#T_d3fee_row8_col4,#T_d3fee_row8_col5,#T_d3fee_row8_col6,#T_d3fee_row8_col7,#T_d3fee_row8_col8{\n            background-color: ;\n        }</style><table id=\"T_d3fee_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >f1</th>        <th class=\"col_heading level0 col1\" >roc_auc</th>        <th class=\"col_heading level0 col2\" >accuracy</th>        <th class=\"col_heading level0 col3\" >recall</th>        <th class=\"col_heading level0 col4\" >precision</th>        <th class=\"col_heading level0 col5\" >true_pos</th>        <th class=\"col_heading level0 col6\" >true_neg</th>        <th class=\"col_heading level0 col7\" >false_pos</th>        <th class=\"col_heading level0 col8\" >false_neg</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_d3fee_level0_row0\" class=\"row_heading level0 row0\" >K Nearest Neighbours</th>\n                        <td id=\"T_d3fee_row0_col0\" class=\"data row0 col0\" >0.176681</td>\n                        <td id=\"T_d3fee_row0_col1\" class=\"data row0 col1\" >0.513736</td>\n                        <td id=\"T_d3fee_row0_col2\" class=\"data row0 col2\" >0.746177</td>\n                        <td id=\"T_d3fee_row0_col3\" class=\"data row0 col3\" >0.166667</td>\n                        <td id=\"T_d3fee_row0_col4\" class=\"data row0 col4\" >0.188550</td>\n                        <td id=\"T_d3fee_row0_col5\" class=\"data row0 col5\" >3.000000</td>\n                        <td id=\"T_d3fee_row0_col6\" class=\"data row0 col6\" >78.333333</td>\n                        <td id=\"T_d3fee_row0_col7\" class=\"data row0 col7\" >12.666667</td>\n                        <td id=\"T_d3fee_row0_col8\" class=\"data row0 col8\" >15.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_d3fee_level0_row1\" class=\"row_heading level0 row1\" >Support Vector Machines</th>\n                        <td id=\"T_d3fee_row1_col0\" class=\"data row1 col0\" >0.283465</td>\n                        <td id=\"T_d3fee_row1_col1\" class=\"data row1 col1\" >0.500000</td>\n                        <td id=\"T_d3fee_row1_col2\" class=\"data row1 col2\" >0.165138</td>\n                        <td id=\"T_d3fee_row1_col3\" class=\"data row1 col3\" >1.000000</td>\n                        <td id=\"T_d3fee_row1_col4\" class=\"data row1 col4\" >0.165138</td>\n                        <td id=\"T_d3fee_row1_col5\" class=\"data row1 col5\" >18.000000</td>\n                        <td id=\"T_d3fee_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n                        <td id=\"T_d3fee_row1_col7\" class=\"data row1 col7\" >91.000000</td>\n                        <td id=\"T_d3fee_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_d3fee_level0_row2\" class=\"row_heading level0 row2\" >Gaussian Process</th>\n                        <td id=\"T_d3fee_row2_col0\" class=\"data row2 col0\" >0.000000</td>\n                        <td id=\"T_d3fee_row2_col1\" class=\"data row2 col1\" >0.498168</td>\n                        <td id=\"T_d3fee_row2_col2\" class=\"data row2 col2\" >0.831804</td>\n                        <td id=\"T_d3fee_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n                        <td id=\"T_d3fee_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n                        <td id=\"T_d3fee_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n                        <td id=\"T_d3fee_row2_col6\" class=\"data row2 col6\" >90.666667</td>\n                        <td id=\"T_d3fee_row2_col7\" class=\"data row2 col7\" >0.333333</td>\n                        <td id=\"T_d3fee_row2_col8\" class=\"data row2 col8\" >18.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_d3fee_level0_row3\" class=\"row_heading level0 row3\" >Random Forest Classifier</th>\n                        <td id=\"T_d3fee_row3_col0\" class=\"data row3 col0\" >0.238889</td>\n                        <td id=\"T_d3fee_row3_col1\" class=\"data row3 col1\" >0.547212</td>\n                        <td id=\"T_d3fee_row3_col2\" class=\"data row3 col2\" >0.740061</td>\n                        <td id=\"T_d3fee_row3_col3\" class=\"data row3 col3\" >0.259259</td>\n                        <td id=\"T_d3fee_row3_col4\" class=\"data row3 col4\" >0.231041</td>\n                        <td id=\"T_d3fee_row3_col5\" class=\"data row3 col5\" >4.666667</td>\n                        <td id=\"T_d3fee_row3_col6\" class=\"data row3 col6\" >76.000000</td>\n                        <td id=\"T_d3fee_row3_col7\" class=\"data row3 col7\" >15.000000</td>\n                        <td id=\"T_d3fee_row3_col8\" class=\"data row3 col8\" >13.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_d3fee_level0_row4\" class=\"row_heading level0 row4\" >Gradient Boosting Classifier</th>\n                        <td id=\"T_d3fee_row4_col0\" class=\"data row4 col0\" >0.251136</td>\n                        <td id=\"T_d3fee_row4_col1\" class=\"data row4 col1\" >0.534799</td>\n                        <td id=\"T_d3fee_row4_col2\" class=\"data row4 col2\" >0.669725</td>\n                        <td id=\"T_d3fee_row4_col3\" class=\"data row4 col3\" >0.333333</td>\n                        <td id=\"T_d3fee_row4_col4\" class=\"data row4 col4\" >0.203431</td>\n                        <td id=\"T_d3fee_row4_col5\" class=\"data row4 col5\" >6.000000</td>\n                        <td id=\"T_d3fee_row4_col6\" class=\"data row4 col6\" >67.000000</td>\n                        <td id=\"T_d3fee_row4_col7\" class=\"data row4 col7\" >24.000000</td>\n                        <td id=\"T_d3fee_row4_col8\" class=\"data row4 col8\" >12.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_d3fee_level0_row5\" class=\"row_heading level0 row5\" >Ada Boost classifier</th>\n                        <td id=\"T_d3fee_row5_col0\" class=\"data row5 col0\" >0.265125</td>\n                        <td id=\"T_d3fee_row5_col1\" class=\"data row5 col1\" >0.547619</td>\n                        <td id=\"T_d3fee_row5_col2\" class=\"data row5 col2\" >0.691131</td>\n                        <td id=\"T_d3fee_row5_col3\" class=\"data row5 col3\" >0.333333</td>\n                        <td id=\"T_d3fee_row5_col4\" class=\"data row5 col4\" >0.221985</td>\n                        <td id=\"T_d3fee_row5_col5\" class=\"data row5 col5\" >6.000000</td>\n                        <td id=\"T_d3fee_row5_col6\" class=\"data row5 col6\" >69.333333</td>\n                        <td id=\"T_d3fee_row5_col7\" class=\"data row5 col7\" >21.666667</td>\n                        <td id=\"T_d3fee_row5_col8\" class=\"data row5 col8\" >12.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_d3fee_level0_row6\" class=\"row_heading level0 row6\" >Gaussian Naieve Bayes</th>\n                        <td id=\"T_d3fee_row6_col0\" class=\"data row6 col0\" >0.237556</td>\n                        <td id=\"T_d3fee_row6_col1\" class=\"data row6 col1\" >0.511294</td>\n                        <td id=\"T_d3fee_row6_col2\" class=\"data row6 col2\" >0.593272</td>\n                        <td id=\"T_d3fee_row6_col3\" class=\"data row6 col3\" >0.388889</td>\n                        <td id=\"T_d3fee_row6_col4\" class=\"data row6 col4\" >0.171641</td>\n                        <td id=\"T_d3fee_row6_col5\" class=\"data row6 col5\" >7.000000</td>\n                        <td id=\"T_d3fee_row6_col6\" class=\"data row6 col6\" >57.666667</td>\n                        <td id=\"T_d3fee_row6_col7\" class=\"data row6 col7\" >33.333333</td>\n                        <td id=\"T_d3fee_row6_col8\" class=\"data row6 col8\" >11.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_d3fee_level0_row7\" class=\"row_heading level0 row7\" >Logistic Regression</th>\n                        <td id=\"T_d3fee_row7_col0\" class=\"data row7 col0\" >0.326915</td>\n                        <td id=\"T_d3fee_row7_col1\" class=\"data row7 col1\" >0.588930</td>\n                        <td id=\"T_d3fee_row7_col2\" class=\"data row7 col2\" >0.412844</td>\n                        <td id=\"T_d3fee_row7_col3\" class=\"data row7 col3\" >0.851852</td>\n                        <td id=\"T_d3fee_row7_col4\" class=\"data row7 col4\" >0.203575</td>\n                        <td id=\"T_d3fee_row7_col5\" class=\"data row7 col5\" >15.333333</td>\n                        <td id=\"T_d3fee_row7_col6\" class=\"data row7 col6\" >29.666667</td>\n                        <td id=\"T_d3fee_row7_col7\" class=\"data row7 col7\" >61.333333</td>\n                        <td id=\"T_d3fee_row7_col8\" class=\"data row7 col8\" >2.666667</td>\n            </tr>\n            <tr>\n                        <th id=\"T_d3fee_level0_row8\" class=\"row_heading level0 row8\" >Quadratic Discriminant Analysis</th>\n                        <td id=\"T_d3fee_row8_col0\" class=\"data row8 col0\" >0.214322</td>\n                        <td id=\"T_d3fee_row8_col1\" class=\"data row8 col1\" >0.489520</td>\n                        <td id=\"T_d3fee_row8_col2\" class=\"data row8 col2\" >0.532110</td>\n                        <td id=\"T_d3fee_row8_col3\" class=\"data row8 col3\" >0.425926</td>\n                        <td id=\"T_d3fee_row8_col4\" class=\"data row8 col4\" >0.179045</td>\n                        <td id=\"T_d3fee_row8_col5\" class=\"data row8 col5\" >7.666667</td>\n                        <td id=\"T_d3fee_row8_col6\" class=\"data row8 col6\" >50.333333</td>\n                        <td id=\"T_d3fee_row8_col7\" class=\"data row8 col7\" >40.666667</td>\n                        <td id=\"T_d3fee_row8_col8\" class=\"data row8 col8\" >10.333333</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>One Hot Resampled"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fda5d84ec10>",
      "text/html": "<style  type=\"text/css\" >\n#T_cd3ef_row0_col0,#T_cd3ef_row0_col1,#T_cd3ef_row0_col2,#T_cd3ef_row0_col3,#T_cd3ef_row0_col4,#T_cd3ef_row0_col5,#T_cd3ef_row0_col6,#T_cd3ef_row0_col7,#T_cd3ef_row0_col8,#T_cd3ef_row5_col0,#T_cd3ef_row5_col1,#T_cd3ef_row5_col2,#T_cd3ef_row5_col3,#T_cd3ef_row5_col4,#T_cd3ef_row5_col5,#T_cd3ef_row5_col6,#T_cd3ef_row5_col7,#T_cd3ef_row5_col8{\n            background-color:  green;\n        }#T_cd3ef_row1_col0,#T_cd3ef_row1_col1,#T_cd3ef_row1_col2,#T_cd3ef_row1_col3,#T_cd3ef_row1_col4,#T_cd3ef_row1_col5,#T_cd3ef_row1_col6,#T_cd3ef_row1_col7,#T_cd3ef_row1_col8,#T_cd3ef_row2_col0,#T_cd3ef_row2_col1,#T_cd3ef_row2_col2,#T_cd3ef_row2_col3,#T_cd3ef_row2_col4,#T_cd3ef_row2_col5,#T_cd3ef_row2_col6,#T_cd3ef_row2_col7,#T_cd3ef_row2_col8,#T_cd3ef_row3_col0,#T_cd3ef_row3_col1,#T_cd3ef_row3_col2,#T_cd3ef_row3_col3,#T_cd3ef_row3_col4,#T_cd3ef_row3_col5,#T_cd3ef_row3_col6,#T_cd3ef_row3_col7,#T_cd3ef_row3_col8,#T_cd3ef_row4_col0,#T_cd3ef_row4_col1,#T_cd3ef_row4_col2,#T_cd3ef_row4_col3,#T_cd3ef_row4_col4,#T_cd3ef_row4_col5,#T_cd3ef_row4_col6,#T_cd3ef_row4_col7,#T_cd3ef_row4_col8,#T_cd3ef_row6_col0,#T_cd3ef_row6_col1,#T_cd3ef_row6_col2,#T_cd3ef_row6_col3,#T_cd3ef_row6_col4,#T_cd3ef_row6_col5,#T_cd3ef_row6_col6,#T_cd3ef_row6_col7,#T_cd3ef_row6_col8,#T_cd3ef_row7_col0,#T_cd3ef_row7_col1,#T_cd3ef_row7_col2,#T_cd3ef_row7_col3,#T_cd3ef_row7_col4,#T_cd3ef_row7_col5,#T_cd3ef_row7_col6,#T_cd3ef_row7_col7,#T_cd3ef_row7_col8,#T_cd3ef_row8_col0,#T_cd3ef_row8_col1,#T_cd3ef_row8_col2,#T_cd3ef_row8_col3,#T_cd3ef_row8_col4,#T_cd3ef_row8_col5,#T_cd3ef_row8_col6,#T_cd3ef_row8_col7,#T_cd3ef_row8_col8{\n            background-color: ;\n        }</style><table id=\"T_cd3ef_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >f1</th>        <th class=\"col_heading level0 col1\" >roc_auc</th>        <th class=\"col_heading level0 col2\" >accuracy</th>        <th class=\"col_heading level0 col3\" >recall</th>        <th class=\"col_heading level0 col4\" >precision</th>        <th class=\"col_heading level0 col5\" >true_pos</th>        <th class=\"col_heading level0 col6\" >true_neg</th>        <th class=\"col_heading level0 col7\" >false_pos</th>        <th class=\"col_heading level0 col8\" >false_neg</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_cd3ef_level0_row0\" class=\"row_heading level0 row0\" >K Nearest Neighbours</th>\n                        <td id=\"T_cd3ef_row0_col0\" class=\"data row0 col0\" >0.268055</td>\n                        <td id=\"T_cd3ef_row0_col1\" class=\"data row0 col1\" >0.544363</td>\n                        <td id=\"T_cd3ef_row0_col2\" class=\"data row0 col2\" >0.636086</td>\n                        <td id=\"T_cd3ef_row0_col3\" class=\"data row0 col3\" >0.407407</td>\n                        <td id=\"T_cd3ef_row0_col4\" class=\"data row0 col4\" >0.201411</td>\n                        <td id=\"T_cd3ef_row0_col5\" class=\"data row0 col5\" >7.333333</td>\n                        <td id=\"T_cd3ef_row0_col6\" class=\"data row0 col6\" >62.000000</td>\n                        <td id=\"T_cd3ef_row0_col7\" class=\"data row0 col7\" >29.000000</td>\n                        <td id=\"T_cd3ef_row0_col8\" class=\"data row0 col8\" >10.666667</td>\n            </tr>\n            <tr>\n                        <th id=\"T_cd3ef_level0_row1\" class=\"row_heading level0 row1\" >Support Vector Machines</th>\n                        <td id=\"T_cd3ef_row1_col0\" class=\"data row1 col0\" >0.234431</td>\n                        <td id=\"T_cd3ef_row1_col1\" class=\"data row1 col1\" >0.505698</td>\n                        <td id=\"T_cd3ef_row1_col2\" class=\"data row1 col2\" >0.596330</td>\n                        <td id=\"T_cd3ef_row1_col3\" class=\"data row1 col3\" >0.370370</td>\n                        <td id=\"T_cd3ef_row1_col4\" class=\"data row1 col4\" >0.174603</td>\n                        <td id=\"T_cd3ef_row1_col5\" class=\"data row1 col5\" >6.666667</td>\n                        <td id=\"T_cd3ef_row1_col6\" class=\"data row1 col6\" >58.333333</td>\n                        <td id=\"T_cd3ef_row1_col7\" class=\"data row1 col7\" >32.666667</td>\n                        <td id=\"T_cd3ef_row1_col8\" class=\"data row1 col8\" >11.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_cd3ef_level0_row2\" class=\"row_heading level0 row2\" >Gaussian Process</th>\n                        <td id=\"T_cd3ef_row2_col0\" class=\"data row2 col0\" >0.105263</td>\n                        <td id=\"T_cd3ef_row2_col1\" class=\"data row2 col1\" >0.473138</td>\n                        <td id=\"T_cd3ef_row2_col2\" class=\"data row2 col2\" >0.715596</td>\n                        <td id=\"T_cd3ef_row2_col3\" class=\"data row2 col3\" >0.111111</td>\n                        <td id=\"T_cd3ef_row2_col4\" class=\"data row2 col4\" >0.100000</td>\n                        <td id=\"T_cd3ef_row2_col5\" class=\"data row2 col5\" >2.000000</td>\n                        <td id=\"T_cd3ef_row2_col6\" class=\"data row2 col6\" >76.000000</td>\n                        <td id=\"T_cd3ef_row2_col7\" class=\"data row2 col7\" >15.000000</td>\n                        <td id=\"T_cd3ef_row2_col8\" class=\"data row2 col8\" >16.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_cd3ef_level0_row3\" class=\"row_heading level0 row3\" >Random Forest Classifier</th>\n                        <td id=\"T_cd3ef_row3_col0\" class=\"data row3 col0\" >0.279183</td>\n                        <td id=\"T_cd3ef_row3_col1\" class=\"data row3 col1\" >0.560440</td>\n                        <td id=\"T_cd3ef_row3_col2\" class=\"data row3 col2\" >0.712538</td>\n                        <td id=\"T_cd3ef_row3_col3\" class=\"data row3 col3\" >0.333333</td>\n                        <td id=\"T_cd3ef_row3_col4\" class=\"data row3 col4\" >0.241092</td>\n                        <td id=\"T_cd3ef_row3_col5\" class=\"data row3 col5\" >6.000000</td>\n                        <td id=\"T_cd3ef_row3_col6\" class=\"data row3 col6\" >71.666667</td>\n                        <td id=\"T_cd3ef_row3_col7\" class=\"data row3 col7\" >19.333333</td>\n                        <td id=\"T_cd3ef_row3_col8\" class=\"data row3 col8\" >12.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_cd3ef_level0_row4\" class=\"row_heading level0 row4\" >Gradient Boosting Classifier</th>\n                        <td id=\"T_cd3ef_row4_col0\" class=\"data row4 col0\" >0.262821</td>\n                        <td id=\"T_cd3ef_row4_col1\" class=\"data row4 col1\" >0.554640</td>\n                        <td id=\"T_cd3ef_row4_col2\" class=\"data row4 col2\" >0.740061</td>\n                        <td id=\"T_cd3ef_row4_col3\" class=\"data row4 col3\" >0.277778</td>\n                        <td id=\"T_cd3ef_row4_col4\" class=\"data row4 col4\" >0.250361</td>\n                        <td id=\"T_cd3ef_row4_col5\" class=\"data row4 col5\" >5.000000</td>\n                        <td id=\"T_cd3ef_row4_col6\" class=\"data row4 col6\" >75.666667</td>\n                        <td id=\"T_cd3ef_row4_col7\" class=\"data row4 col7\" >15.333333</td>\n                        <td id=\"T_cd3ef_row4_col8\" class=\"data row4 col8\" >13.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_cd3ef_level0_row5\" class=\"row_heading level0 row5\" >Ada Boost classifier</th>\n                        <td id=\"T_cd3ef_row5_col0\" class=\"data row5 col0\" >0.302251</td>\n                        <td id=\"T_cd3ef_row5_col1\" class=\"data row5 col1\" >0.574176</td>\n                        <td id=\"T_cd3ef_row5_col2\" class=\"data row5 col2\" >0.623853</td>\n                        <td id=\"T_cd3ef_row5_col3\" class=\"data row5 col3\" >0.500000</td>\n                        <td id=\"T_cd3ef_row5_col4\" class=\"data row5 col4\" >0.218438</td>\n                        <td id=\"T_cd3ef_row5_col5\" class=\"data row5 col5\" >9.000000</td>\n                        <td id=\"T_cd3ef_row5_col6\" class=\"data row5 col6\" >59.000000</td>\n                        <td id=\"T_cd3ef_row5_col7\" class=\"data row5 col7\" >32.000000</td>\n                        <td id=\"T_cd3ef_row5_col8\" class=\"data row5 col8\" >9.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_cd3ef_level0_row6\" class=\"row_heading level0 row6\" >Gaussian Naieve Bayes</th>\n                        <td id=\"T_cd3ef_row6_col0\" class=\"data row6 col0\" >0.268192</td>\n                        <td id=\"T_cd3ef_row6_col1\" class=\"data row6 col1\" >0.503561</td>\n                        <td id=\"T_cd3ef_row6_col2\" class=\"data row6 col2\" >0.406728</td>\n                        <td id=\"T_cd3ef_row6_col3\" class=\"data row6 col3\" >0.648148</td>\n                        <td id=\"T_cd3ef_row6_col4\" class=\"data row6 col4\" >0.169485</td>\n                        <td id=\"T_cd3ef_row6_col5\" class=\"data row6 col5\" >11.666667</td>\n                        <td id=\"T_cd3ef_row6_col6\" class=\"data row6 col6\" >32.666667</td>\n                        <td id=\"T_cd3ef_row6_col7\" class=\"data row6 col7\" >58.333333</td>\n                        <td id=\"T_cd3ef_row6_col8\" class=\"data row6 col8\" >6.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_cd3ef_level0_row7\" class=\"row_heading level0 row7\" >Logistic Regression</th>\n                        <td id=\"T_cd3ef_row7_col0\" class=\"data row7 col0\" >0.283624</td>\n                        <td id=\"T_cd3ef_row7_col1\" class=\"data row7 col1\" >0.524522</td>\n                        <td id=\"T_cd3ef_row7_col2\" class=\"data row7 col2\" >0.342508</td>\n                        <td id=\"T_cd3ef_row7_col3\" class=\"data row7 col3\" >0.796296</td>\n                        <td id=\"T_cd3ef_row7_col4\" class=\"data row7 col4\" >0.172994</td>\n                        <td id=\"T_cd3ef_row7_col5\" class=\"data row7 col5\" >14.333333</td>\n                        <td id=\"T_cd3ef_row7_col6\" class=\"data row7 col6\" >23.000000</td>\n                        <td id=\"T_cd3ef_row7_col7\" class=\"data row7 col7\" >68.000000</td>\n                        <td id=\"T_cd3ef_row7_col8\" class=\"data row7 col8\" >3.666667</td>\n            </tr>\n            <tr>\n                        <th id=\"T_cd3ef_level0_row8\" class=\"row_heading level0 row8\" >Quadratic Discriminant Analysis</th>\n                        <td id=\"T_cd3ef_row8_col0\" class=\"data row8 col0\" >0.241426</td>\n                        <td id=\"T_cd3ef_row8_col1\" class=\"data row8 col1\" >0.502340</td>\n                        <td id=\"T_cd3ef_row8_col2\" class=\"data row8 col2\" >0.553517</td>\n                        <td id=\"T_cd3ef_row8_col3\" class=\"data row8 col3\" >0.425926</td>\n                        <td id=\"T_cd3ef_row8_col4\" class=\"data row8 col4\" >0.169025</td>\n                        <td id=\"T_cd3ef_row8_col5\" class=\"data row8 col5\" >7.666667</td>\n                        <td id=\"T_cd3ef_row8_col6\" class=\"data row8 col6\" >52.666667</td>\n                        <td id=\"T_cd3ef_row8_col7\" class=\"data row8 col7\" >38.333333</td>\n                        <td id=\"T_cd3ef_row8_col8\" class=\"data row8 col8\" >10.333333</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h2>Target Resampled"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<pandas.io.formats.style.Styler at 0x7fda5d84eca0>",
      "text/html": "<style  type=\"text/css\" >\n#T_f2c84_row0_col0,#T_f2c84_row0_col1,#T_f2c84_row0_col2,#T_f2c84_row0_col3,#T_f2c84_row0_col4,#T_f2c84_row0_col5,#T_f2c84_row0_col6,#T_f2c84_row0_col7,#T_f2c84_row0_col8{\n            background-color:  green;\n        }#T_f2c84_row1_col0,#T_f2c84_row1_col1,#T_f2c84_row1_col2,#T_f2c84_row1_col3,#T_f2c84_row1_col4,#T_f2c84_row1_col5,#T_f2c84_row1_col6,#T_f2c84_row1_col7,#T_f2c84_row1_col8,#T_f2c84_row2_col0,#T_f2c84_row2_col1,#T_f2c84_row2_col2,#T_f2c84_row2_col3,#T_f2c84_row2_col4,#T_f2c84_row2_col5,#T_f2c84_row2_col6,#T_f2c84_row2_col7,#T_f2c84_row2_col8,#T_f2c84_row3_col0,#T_f2c84_row3_col1,#T_f2c84_row3_col2,#T_f2c84_row3_col3,#T_f2c84_row3_col4,#T_f2c84_row3_col5,#T_f2c84_row3_col6,#T_f2c84_row3_col7,#T_f2c84_row3_col8,#T_f2c84_row4_col0,#T_f2c84_row4_col1,#T_f2c84_row4_col2,#T_f2c84_row4_col3,#T_f2c84_row4_col4,#T_f2c84_row4_col5,#T_f2c84_row4_col6,#T_f2c84_row4_col7,#T_f2c84_row4_col8,#T_f2c84_row5_col0,#T_f2c84_row5_col1,#T_f2c84_row5_col2,#T_f2c84_row5_col3,#T_f2c84_row5_col4,#T_f2c84_row5_col5,#T_f2c84_row5_col6,#T_f2c84_row5_col7,#T_f2c84_row5_col8,#T_f2c84_row6_col0,#T_f2c84_row6_col1,#T_f2c84_row6_col2,#T_f2c84_row6_col3,#T_f2c84_row6_col4,#T_f2c84_row6_col5,#T_f2c84_row6_col6,#T_f2c84_row6_col7,#T_f2c84_row6_col8,#T_f2c84_row7_col0,#T_f2c84_row7_col1,#T_f2c84_row7_col2,#T_f2c84_row7_col3,#T_f2c84_row7_col4,#T_f2c84_row7_col5,#T_f2c84_row7_col6,#T_f2c84_row7_col7,#T_f2c84_row7_col8,#T_f2c84_row8_col0,#T_f2c84_row8_col1,#T_f2c84_row8_col2,#T_f2c84_row8_col3,#T_f2c84_row8_col4,#T_f2c84_row8_col5,#T_f2c84_row8_col6,#T_f2c84_row8_col7,#T_f2c84_row8_col8{\n            background-color: ;\n        }</style><table id=\"T_f2c84_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >f1</th>        <th class=\"col_heading level0 col1\" >roc_auc</th>        <th class=\"col_heading level0 col2\" >accuracy</th>        <th class=\"col_heading level0 col3\" >recall</th>        <th class=\"col_heading level0 col4\" >precision</th>        <th class=\"col_heading level0 col5\" >true_pos</th>        <th class=\"col_heading level0 col6\" >true_neg</th>        <th class=\"col_heading level0 col7\" >false_pos</th>        <th class=\"col_heading level0 col8\" >false_neg</th>    </tr></thead><tbody>\n                <tr>\n                        <th id=\"T_f2c84_level0_row0\" class=\"row_heading level0 row0\" >K Nearest Neighbours</th>\n                        <td id=\"T_f2c84_row0_col0\" class=\"data row0 col0\" >0.274305</td>\n                        <td id=\"T_f2c84_row0_col1\" class=\"data row0 col1\" >0.544567</td>\n                        <td id=\"T_f2c84_row0_col2\" class=\"data row0 col2\" >0.611621</td>\n                        <td id=\"T_f2c84_row0_col3\" class=\"data row0 col3\" >0.444444</td>\n                        <td id=\"T_f2c84_row0_col4\" class=\"data row0 col4\" >0.200454</td>\n                        <td id=\"T_f2c84_row0_col5\" class=\"data row0 col5\" >8.000000</td>\n                        <td id=\"T_f2c84_row0_col6\" class=\"data row0 col6\" >58.666667</td>\n                        <td id=\"T_f2c84_row0_col7\" class=\"data row0 col7\" >32.333333</td>\n                        <td id=\"T_f2c84_row0_col8\" class=\"data row0 col8\" >10.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f2c84_level0_row1\" class=\"row_heading level0 row1\" >Support Vector Machines</th>\n                        <td id=\"T_f2c84_row1_col0\" class=\"data row1 col0\" >0.320539</td>\n                        <td id=\"T_f2c84_row1_col1\" class=\"data row1 col1\" >0.580586</td>\n                        <td id=\"T_f2c84_row1_col2\" class=\"data row1 col2\" >0.522936</td>\n                        <td id=\"T_f2c84_row1_col3\" class=\"data row1 col3\" >0.666667</td>\n                        <td id=\"T_f2c84_row1_col4\" class=\"data row1 col4\" >0.214927</td>\n                        <td id=\"T_f2c84_row1_col5\" class=\"data row1 col5\" >12.000000</td>\n                        <td id=\"T_f2c84_row1_col6\" class=\"data row1 col6\" >45.000000</td>\n                        <td id=\"T_f2c84_row1_col7\" class=\"data row1 col7\" >46.000000</td>\n                        <td id=\"T_f2c84_row1_col8\" class=\"data row1 col8\" >6.000000</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f2c84_level0_row2\" class=\"row_heading level0 row2\" >Gaussian Process</th>\n                        <td id=\"T_f2c84_row2_col0\" class=\"data row2 col0\" >0.174929</td>\n                        <td id=\"T_f2c84_row2_col1\" class=\"data row2 col1\" >0.499186</td>\n                        <td id=\"T_f2c84_row2_col2\" class=\"data row2 col2\" >0.709480</td>\n                        <td id=\"T_f2c84_row2_col3\" class=\"data row2 col3\" >0.185185</td>\n                        <td id=\"T_f2c84_row2_col4\" class=\"data row2 col4\" >0.166426</td>\n                        <td id=\"T_f2c84_row2_col5\" class=\"data row2 col5\" >3.333333</td>\n                        <td id=\"T_f2c84_row2_col6\" class=\"data row2 col6\" >74.000000</td>\n                        <td id=\"T_f2c84_row2_col7\" class=\"data row2 col7\" >17.000000</td>\n                        <td id=\"T_f2c84_row2_col8\" class=\"data row2 col8\" >14.666667</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f2c84_level0_row3\" class=\"row_heading level0 row3\" >Random Forest Classifier</th>\n                        <td id=\"T_f2c84_row3_col0\" class=\"data row3 col0\" >0.267909</td>\n                        <td id=\"T_f2c84_row3_col1\" class=\"data row3 col1\" >0.553012</td>\n                        <td id=\"T_f2c84_row3_col2\" class=\"data row3 col2\" >0.712538</td>\n                        <td id=\"T_f2c84_row3_col3\" class=\"data row3 col3\" >0.314815</td>\n                        <td id=\"T_f2c84_row3_col4\" class=\"data row3 col4\" >0.234307</td>\n                        <td id=\"T_f2c84_row3_col5\" class=\"data row3 col5\" >5.666667</td>\n                        <td id=\"T_f2c84_row3_col6\" class=\"data row3 col6\" >72.000000</td>\n                        <td id=\"T_f2c84_row3_col7\" class=\"data row3 col7\" >19.000000</td>\n                        <td id=\"T_f2c84_row3_col8\" class=\"data row3 col8\" >12.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f2c84_level0_row4\" class=\"row_heading level0 row4\" >Gradient Boosting Classifier</th>\n                        <td id=\"T_f2c84_row4_col0\" class=\"data row4 col0\" >0.262721</td>\n                        <td id=\"T_f2c84_row4_col1\" class=\"data row4 col1\" >0.551180</td>\n                        <td id=\"T_f2c84_row4_col2\" class=\"data row4 col2\" >0.709480</td>\n                        <td id=\"T_f2c84_row4_col3\" class=\"data row4 col3\" >0.314815</td>\n                        <td id=\"T_f2c84_row4_col4\" class=\"data row4 col4\" >0.226018</td>\n                        <td id=\"T_f2c84_row4_col5\" class=\"data row4 col5\" >5.666667</td>\n                        <td id=\"T_f2c84_row4_col6\" class=\"data row4 col6\" >71.666667</td>\n                        <td id=\"T_f2c84_row4_col7\" class=\"data row4 col7\" >19.333333</td>\n                        <td id=\"T_f2c84_row4_col8\" class=\"data row4 col8\" >12.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f2c84_level0_row5\" class=\"row_heading level0 row5\" >Ada Boost classifier</th>\n                        <td id=\"T_f2c84_row5_col0\" class=\"data row5 col0\" >0.258278</td>\n                        <td id=\"T_f2c84_row5_col1\" class=\"data row5 col1\" >0.503256</td>\n                        <td id=\"T_f2c84_row5_col2\" class=\"data row5 col2\" >0.443425</td>\n                        <td id=\"T_f2c84_row5_col3\" class=\"data row5 col3\" >0.592593</td>\n                        <td id=\"T_f2c84_row5_col4\" class=\"data row5 col4\" >0.166495</td>\n                        <td id=\"T_f2c84_row5_col5\" class=\"data row5 col5\" >10.666667</td>\n                        <td id=\"T_f2c84_row5_col6\" class=\"data row5 col6\" >37.666667</td>\n                        <td id=\"T_f2c84_row5_col7\" class=\"data row5 col7\" >53.333333</td>\n                        <td id=\"T_f2c84_row5_col8\" class=\"data row5 col8\" >7.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f2c84_level0_row6\" class=\"row_heading level0 row6\" >Gaussian Naieve Bayes</th>\n                        <td id=\"T_f2c84_row6_col0\" class=\"data row6 col0\" >0.257586</td>\n                        <td id=\"T_f2c84_row6_col1\" class=\"data row6 col1\" >0.522690</td>\n                        <td id=\"T_f2c84_row6_col2\" class=\"data row6 col2\" >0.562691</td>\n                        <td id=\"T_f2c84_row6_col3\" class=\"data row6 col3\" >0.462963</td>\n                        <td id=\"T_f2c84_row6_col4\" class=\"data row6 col4\" >0.178796</td>\n                        <td id=\"T_f2c84_row6_col5\" class=\"data row6 col5\" >8.333333</td>\n                        <td id=\"T_f2c84_row6_col6\" class=\"data row6 col6\" >53.000000</td>\n                        <td id=\"T_f2c84_row6_col7\" class=\"data row6 col7\" >38.000000</td>\n                        <td id=\"T_f2c84_row6_col8\" class=\"data row6 col8\" >9.666667</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f2c84_level0_row7\" class=\"row_heading level0 row7\" >Logistic Regression</th>\n                        <td id=\"T_f2c84_row7_col0\" class=\"data row7 col0\" >0.310314</td>\n                        <td id=\"T_f2c84_row7_col1\" class=\"data row7 col1\" >0.573158</td>\n                        <td id=\"T_f2c84_row7_col2\" class=\"data row7 col2\" >0.522936</td>\n                        <td id=\"T_f2c84_row7_col3\" class=\"data row7 col3\" >0.648148</td>\n                        <td id=\"T_f2c84_row7_col4\" class=\"data row7 col4\" >0.205198</td>\n                        <td id=\"T_f2c84_row7_col5\" class=\"data row7 col5\" >11.666667</td>\n                        <td id=\"T_f2c84_row7_col6\" class=\"data row7 col6\" >45.333333</td>\n                        <td id=\"T_f2c84_row7_col7\" class=\"data row7 col7\" >45.666667</td>\n                        <td id=\"T_f2c84_row7_col8\" class=\"data row7 col8\" >6.333333</td>\n            </tr>\n            <tr>\n                        <th id=\"T_f2c84_level0_row8\" class=\"row_heading level0 row8\" >Quadratic Discriminant Analysis</th>\n                        <td id=\"T_f2c84_row8_col0\" class=\"data row8 col0\" >0.308064</td>\n                        <td id=\"T_f2c84_row8_col1\" class=\"data row8 col1\" >0.574990</td>\n                        <td id=\"T_f2c84_row8_col2\" class=\"data row8 col2\" >0.525994</td>\n                        <td id=\"T_f2c84_row8_col3\" class=\"data row8 col3\" >0.648148</td>\n                        <td id=\"T_f2c84_row8_col4\" class=\"data row8 col4\" >0.202288</td>\n                        <td id=\"T_f2c84_row8_col5\" class=\"data row8 col5\" >11.666667</td>\n                        <td id=\"T_f2c84_row8_col6\" class=\"data row8 col6\" >45.666667</td>\n                        <td id=\"T_f2c84_row8_col7\" class=\"data row8 col7\" >45.333333</td>\n                        <td id=\"T_f2c84_row8_col8\" class=\"data row8 col8\" >6.333333</td>\n            </tr>\n    </tbody></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_features = []\n",
    "rename_features = {}\n",
    "for feature in SCORING.keys():\n",
    "    feature_name = \"mean_test_\" + feature\n",
    "    fit_features.append(feature_name)\n",
    "    rename_features[feature_name] = feature\n",
    "\n",
    "fit_features = [fit_features[0]] + [\"std_test_f1\"] + fit_features[1:]\n",
    "rename_features[\"mean_test_f1\"] = \"f1 Mean\"\n",
    "rename_features[\"std_test_f1\"] = \"f1 Std\"\n",
    "\n",
    "def highlight_good_scores_green(df):\n",
    "    good_accuracy = df[\"accuracy\"] > 0.6\n",
    "    good_recall = df[\"recall\"] > 0.4\n",
    "    good_precision = df[\"precision\"] > 0.2\n",
    "    highlight = good_recall & good_accuracy & good_precision\n",
    "    if highlight:\n",
    "        return [f\"background-color: green\"] * 9\n",
    "    else:\n",
    "        return [f\"background-color:\"] * 9\n",
    "\n",
    "\n",
    "for name, df in best_model_scores.items():\n",
    "    display(HTML(f\"<h2>{name.replace('_', ' ').title()}\"))\n",
    "    display(df.rename(rename_features, axis=1)\n",
    "            .style.apply(highlight_good_scores_green, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that none of the methods have really \"cracked this nut\" so to\n",
    "speak. Most models identify under 50% of the patients that need hospital\n",
    "treatment and are right far less than 25% of the time when they do predict\n",
    "the need for hospital treatment.\n",
    "\n",
    "Almost all of the best results appear to come from data that is one-hot\n",
    "encoded and when the target is weighted (rather than synthesising new\n",
    "examples). The best performing models appear to be the tree based methods\n",
    " (Random Forest Classifier, Gradient Bossting Classifier, and Ada Boost\n",
    " Classifier), classic Logistic Regression (a version of linear regression\n",
    " optimised for classification tasks) and Support Vector Machines (trained\n",
    " using target encoded data).\n",
    "\n",
    "We can test how well these models perform with new data by evaluating their predictions\n",
    "on the holdout test set: a dataset that has not been used at any point\n",
    "during training and thus is a good indicator of a\n",
    "model's ability to generalise. I'll only evaluate the models identified\n",
    "above, as indiscriminately evaluating every possible model against the holdout\n",
    "test set risks biasing our selection - the more models we test the more\n",
    "likely a result is to have occurred by chance rather than accurate modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_ohe, X_test_ohe = encode_and_scale(X_train, y_train, X_test,\n",
    "                                           cat_encoder=\"one_hot\")\n",
    "X_train_ohe_scaled, X_test_ohe_scaled = encode_and_scale(X_train, y_train,\n",
    "                                                         X_test,\n",
    "                                                         cat_encoder=\"one_hot\",\n",
    "                                                         scaled=True)\n",
    "X_train_target_scaled, X_test_target_scaled = encode_and_scale(X_train, y_train,\n",
    "                                                               X_test,\n",
    "                                                               cat_encoder=\"one_hot\",\n",
    "                                                               scaled=True)\n",
    "\n",
    "best_performing_models = [\n",
    "    (\"one_hot_balanced\", 'Random Forest Classifier'),\n",
    "    (\"one_hot_balanced\", 'Gradient Boosting Classifier'),\n",
    "    (\"one_hot_balanced\", 'Ada Boost classifier'),\n",
    "    (\"one_hot_balanced\",  'Logistic Regression'),\n",
    "    (\"target_balanced\", \"Support Vector Machines\")\n",
    "]\n",
    "test_scores = {}\n",
    "for data_prep_type, model in best_performing_models:\n",
    "    model_args = techniques_dict[model](balanced=True)\n",
    "    clf = model_args[\"clf\"]\n",
    "    params = best_params[data_prep_type][model]\n",
    "    if \"weight_y\" in model_args.keys():\n",
    "        weight_y = model_args[\"weight_y\"]\n",
    "    else:\n",
    "        weight_y = False\n",
    "    if model_args[\"scaled\"]:\n",
    "        if weight_y:\n",
    "            # calculate array of weights for y labels\n",
    "            pos_weight, neg_weight = compute_class_weight(\n",
    "                class_weight=\"balanced\",\n",
    "                classes=[1,0],\n",
    "                y=y_train)\n",
    "            y_weights = y_train.apply(lambda y: pos_weight if y else neg_weight)\n",
    "            # train model using parameters, weights and cv loop data\n",
    "            clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(X_train_ohe_scaled,\n",
    "                           y_train,\n",
    "                           sample_weight=y_weights))\n",
    "        else:\n",
    "            # train model using parameters and cv loop data\n",
    "            clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(X_train_ohe_scaled, y_train))\n",
    "    else:\n",
    "        if weight_y:\n",
    "            # calculate array of weights for y labels\n",
    "            pos_weight, neg_weight = compute_class_weight(\n",
    "                class_weight=\"balanced\",\n",
    "                classes=[1,0],\n",
    "                y=y_train)\n",
    "            y_weights = y_train.apply(lambda y: pos_weight if y else neg_weight)\n",
    "            # train model using parameters, weights and cv loop data\n",
    "            clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(X_train_ohe,\n",
    "                           y_train,\n",
    "                           sample_weight=y_weights))\n",
    "        else:\n",
    "            # train model using parameters and cv loop data\n",
    "            clf = (clf\n",
    "                      .set_params(**params)\n",
    "                      .fit(X_train_ohe, y_train))\n",
    "    if model_args[\"scaled\"]:\n",
    "        scores = score_classifier(clf, X_test_ohe_scaled, y_test)\n",
    "    else:\n",
    "        scores = score_classifier(clf, X_test_ohe, y_test)\n",
    "    test_scores[model] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>true_neg</th>\n",
       "      <th>false_pos</th>\n",
       "      <th>false_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest Classifier</th>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.547678</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>6.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting Classifier</th>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.532615</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>7.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ada Boost classifier</th>\n",
       "      <td>0.276423</td>\n",
       "      <td>0.520039</td>\n",
       "      <td>0.447205</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>17.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.539663</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>10.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machines</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.598673</td>\n",
       "      <td>0.627329</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>15.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    f1   roc_auc  accuracy    recall  \\\n",
       "Random Forest Classifier      0.240000  0.547678  0.763975  0.222222   \n",
       "Gradient Boosting Classifier  0.233333  0.532615  0.714286  0.259259   \n",
       "Ada Boost classifier          0.276423  0.520039  0.447205  0.629630   \n",
       "Logistic Regression           0.263158  0.539663  0.652174  0.370370   \n",
       "Support Vector Machines       0.333333  0.598673  0.627329  0.555556   \n",
       "\n",
       "                              precision  true_pos  true_neg  false_pos  \\\n",
       "Random Forest Classifier       0.260870       6.0     117.0       17.0   \n",
       "Gradient Boosting Classifier   0.212121       7.0     108.0       26.0   \n",
       "Ada Boost classifier           0.177083      17.0      55.0       79.0   \n",
       "Logistic Regression            0.204082      10.0      95.0       39.0   \n",
       "Support Vector Machines        0.238095      15.0      86.0       48.0   \n",
       "\n",
       "                              false_neg  \n",
       "Random Forest Classifier           21.0  \n",
       "Gradient Boosting Classifier       20.0  \n",
       "Ada Boost classifier               10.0  \n",
       "Logistic Regression                17.0  \n",
       "Support Vector Machines            12.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_scores).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test scores seem to agree reasonably with the validation scores, so we\n",
    "seem to have reasonable estimates of the performance of these models (which is\n",
    " a relief given the amount of time it took to write the custom cv / grid\n",
    " search loop!)\n",
    "\n",
    "## Closing Comments:\n",
    "\n",
    "As I've mentioned a few times, I'm underway with a notebook to follow this\n",
    "that includes a more in-depth analysis of the above models, with:\n",
    "  * discussion of the model alogrithms\n",
    "  * an explanations of the features the models are using to make predictions\n",
    "  * analysis of individual examples the models are getting right / wrong\n",
    "\n",
    "As things stand, none of these models are exhibiting a level of accuracy that\n",
    "would leave us confident they could provide much in the way of useful\n",
    "inference, or be taken forward into production. Even with better data, we\n",
    "have a very long way to go to see really high levels of accuracy (the likes\n",
    "of 80-90% precision and recall).\n",
    "\n",
    " It's because of this that I'm keen on moving away from the \"rigid\" single\n",
    " estimate of probability approach, to a model that can say more about the\n",
    " uncertainty of a given estimate. That way, even if the model is only right\n",
    " one third of the time, if it's confident about that third and unconfident\n",
    " the rest of the time then we can provide some useful inference when taking\n",
    " ACE referrals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Gradient Boosting Classifier\n",
      "==================================================\n",
      "Params:\n",
      "learning_rate: 0.02\n",
      "max_depth: 4\n",
      "max_features: 0.2\n",
      "min_samples_split: 30\n",
      "n_estimators: 130\n",
      "==================================================\n",
      "Logistic Regression\n",
      "==================================================\n",
      "Params:\n",
      "C: 0.03359818286283781\n",
      "class_weight: balanced\n",
      "penalty: l2\n",
      "solver: liblinear\n",
      "==================================================\n",
      "Support Vector Machines\n",
      "==================================================\n",
      "Params:\n",
      "C: 10000.0\n",
      "class_weight: balanced\n",
      "gamma: 0.0001\n",
      "kernel: linear\n"
     ]
    }
   ],
   "source": [
    "interesting_models = [\n",
    "    (\"one_hot_balanced\", 'Gradient Boosting Classifier'),\n",
    "    (\"one_hot_balanced\",  'Logistic Regression'),\n",
    "    (\"target_balanced\", \"Support Vector Machines\")\n",
    "]\n",
    "\n",
    "for data_prep_type, model in interesting_models:\n",
    "    print('=' * 50)\n",
    "    print(model)\n",
    "    print('=' * 50)\n",
    "    print(\"Params:\")\n",
    "    for param, value in best_params[data_prep_type][model].items():\n",
    "        print(f\"{param}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}