\scriptsize{
\renewcommand*{\arraystretch}{1.4}
\begin{longtable}[c]{P{30mm} P{17mm} P{17mm} P{17mm} P{17mm} P{17mm} }
        \toprule
        & \textbf{F$_1$} & \textbf{AUC} & \textbf{Accuracy} & \textbf{Recall} & \textbf{Precision} \\\toprule
        \endhead
        \multicolumn{6}{c}{\textbf{One Hot - Weighted Labels}}\\*[3mm]
        \textbf{K Nearest Neighbours}            & 0.154 (0.071) & 0.502 (0.038) & 0.742 (0.027) & 0.144 (0.073) & 0.169 (0.074) \\*
        \textbf{Support Vector Machines}         & 0.269 (0.056) & 0.535 (0.061) & 0.576 (0.082) & 0.474 (0.140) & 0.191 (0.041) \\*
        \textbf{Random Forest Classifier}        & 0.209 (0.066) & 0.530 (0.035) & 0.745 (0.033) & 0.209 (0.079) & 0.218 (0.065) \\*
        \textbf{Gradient Boosting Classifier}    & 0.153 (0.092) & 0.519 (0.039) & 0.788 (0.029) & 0.119 (0.073) & 0.233 (0.162) \\*
        \textbf{Ada Boost classifier}            & 0.173 (0.081) & 0.518 (0.041) & 0.762 (0.034) & 0.154 (0.078) & 0.212 (0.109) \\*
        \textbf{Gaussian Naieve Bayes}           & 0.238 (0.055) & 0.521 (0.047) & 0.645 (0.066) & 0.335 (0.096) & 0.189 (0.048) \\*
        \textbf{Logistic Regression}             & 0.265 (0.049) & 0.534 (0.050) & 0.596 (0.060) & 0.441 (0.097) & 0.191 (0.037) \\*
        \textbf{Quadratic Discriminant Analysis} & 0.036 (0.048) & 0.497 (0.016) & 0.815 (0.016) & 0.022 (0.031) & 0.106 (0.151) \\\midrule
        \multicolumn{6}{c}{\textbf{One Hot - SMOTE}}\\*[3mm]
        \textbf{K Nearest Neighbours}            & 0.248 (0.052) & 0.512 (0.057) & 0.557 (0.047) & 0.444 (0.111) & 0.173 (0.035) \\*
        \textbf{Support Vector Machines}         & 0.292 (0.048) & 0.552 (0.054) & 0.555 (0.071) & 0.546 (0.076) & 0.200 (0.040) \\*
        \textbf{Random Forest Classifier}        & 0.267 (0.050) & 0.555 (0.035) & 0.717 (0.039) & 0.313 (0.072) & 0.238 (0.051) \\*
        \textbf{Gradient Boosting Classifier}    & 0.255 (0.063) & 0.547 (0.043) & 0.717 (0.041) & 0.293 (0.076) & 0.231 (0.064) \\*
        \textbf{Ada Boost classifier}            & 0.278 (0.059) & 0.537 (0.061) & 0.539 (0.093) & 0.535 (0.130) & 0.192 (0.049) \\*
        \textbf{Gaussian Naieve Bayes}           & 0.196 (0.060) & 0.465 (0.051) & 0.540 (0.059) & 0.352 (0.141) & 0.138 (0.038) \\*
        \textbf{Logistic Regression}             & 0.285 (0.066) & 0.556 (0.062) & 0.634 (0.055) & 0.441 (0.108) & 0.213 (0.051) \\*
        \textbf{Quadratic Discriminant Analysis} & 0.256 (0.058) & 0.519 (0.063) & 0.561 (0.061) & 0.456 (0.112) & 0.179 (0.042) \\\midrule
        \multicolumn{6}{c}{\textbf{One Hot - Undersampling}}\\*[3mm]
        \textbf{K Nearest Neighbours}            & 0.246 (0.063) & 0.505 (0.071) & 0.532 (0.059) & 0.463 (0.126) & 0.169 (0.043) \\*
        \textbf{Support Vector Machines}         & 0.276 (0.046) & 0.535 (0.056) & 0.528 (0.084) & 0.546 (0.138) & 0.187 (0.032) \\*
        \textbf{Random Forest Classifier}        & 0.275 (0.047) & 0.536 (0.054) & 0.544 (0.055) & 0.524 (0.106) & 0.187 (0.032) \\*
        \textbf{Gradient Boosting Classifier}    & 0.262 (0.045) & 0.520 (0.050) & 0.530 (0.066) & 0.506 (0.111) & 0.178 (0.033) \\*
        \textbf{Ada Boost classifier}            & 0.267 (0.050) & 0.528 (0.055) & 0.540 (0.054) & 0.511 (0.126) & 0.182 (0.034) \\*
        \textbf{Gaussian Naieve Bayes}           & 0.269 (0.048) & 0.534 (0.053) & 0.561 (0.069) & 0.493 (0.121) & 0.187 (0.031) \\*
        \textbf{Logistic Regression}             & 0.267 (0.051) & 0.525 (0.062) & 0.531 (0.062) & 0.517 (0.106) & 0.181 (0.035) \\*
        \textbf{Quadratic Discriminant Analysis} & 0.269 (0.067) & 0.529 (0.070) & 0.557 (0.083) & 0.489 (0.125) & 0.189 (0.054) \\\midrule
        \multicolumn{6}{c}{\textbf{Mean Target - Weighted Labels}}\\*[3mm]
        \textbf{K Nearest Neighbours}            & 0.108 (0.069) & 0.475 (0.036) & 0.723 (0.033) & 0.104 (0.068) & 0.115 (0.076) \\*
        \textbf{Support Vector Machines}         & 0.279 (0.039) & 0.545 (0.039) & 0.570 (0.042) & 0.509 (0.107) & 0.193 (0.024) \\*
        \textbf{Random Forest Classifier}        & 0.193 (0.075) & 0.512 (0.045) & 0.715 (0.041) & 0.209 (0.088) & 0.186 (0.081) \\*
        \textbf{Gradient Boosting Classifier}    & 0.135 (0.097) & 0.510 (0.044) & 0.780 (0.028) & 0.106 (0.079) & 0.195 (0.130) \\*
        \textbf{Ada Boost classifier}            & 0.161 (0.076) & 0.512 (0.038) & 0.761 (0.031) & 0.141 (0.071) & 0.197 (0.099) \\*
        \textbf{Gaussian Naieve Bayes}           & 0.205 (0.070) & 0.515 (0.036) & 0.684 (0.093) & 0.263 (0.140) & 0.181 (0.049) \\*
        \textbf{Logistic Regression}             & 0.285 (0.042) & 0.549 (0.046) & 0.569 (0.049) & 0.519 (0.090) & 0.197 (0.030) \\*
        \textbf{Quadratic Discriminant Analysis} & 0.148 (0.086) & 0.440 (0.058) & 0.527 (0.172) & 0.311 (0.226) & 0.113 (0.066) \\\midrule
        \multicolumn{6}{c}{\textbf{Mean Target - SMOTE}}\\*[3mm]
        \textbf{K Nearest Neighbours}            & 0.209 (0.062) & 0.474 (0.064) & 0.547 (0.051) & 0.365 (0.124) & 0.147 (0.043) \\*
        \textbf{Support Vector Machines}         & 0.279 (0.034) & 0.539 (0.040) & 0.529 (0.054) & 0.554 (0.102) & 0.188 (0.023) \\*
        \textbf{Random Forest Classifier}        & 0.235 (0.072) & 0.534 (0.047) & 0.708 (0.039) & 0.274 (0.092) & 0.209 (0.063) \\*
        \textbf{Gradient Boosting Classifier}    & 0.242 (0.072) & 0.540 (0.047) & 0.715 (0.042) & 0.280 (0.100) & 0.219 (0.063) \\*
        \textbf{Ada Boost classifier}            & 0.274 (0.059) & 0.531 (0.065) & 0.538 (0.093) & 0.520 (0.125) & 0.190 (0.050) \\*
        \textbf{Gaussian Naieve Bayes}           & 0.271 (0.041) & 0.532 (0.046) & 0.546 (0.050) & 0.511 (0.089) & 0.185 (0.029) \\*
        \textbf{Logistic Regression}             & 0.271 (0.046) & 0.539 (0.044) & 0.591 (0.049) & 0.463 (0.094) & 0.193 (0.032) \\*
        \textbf{Quadratic Discriminant Analysis} & 0.275 (0.036) & 0.536 (0.038) & 0.535 (0.067) & 0.537 (0.105) & 0.187 (0.024) \\\midrule
        \multicolumn{6}{c}{\textbf{Mean Target - Undersampling}}\\*[3mm]
        \textbf{K Nearest Neighbours}            & 0.240 (0.059) & 0.504 (0.064) & 0.561 (0.061) & 0.420 (0.115) & 0.169 (0.041) \\*
        \textbf{Support Vector Machines}         & 0.266 (0.054) & 0.524 (0.061) & 0.528 (0.065) & 0.519 (0.120) & 0.180 (0.038) \\*
        \textbf{Random Forest Classifier}        & 0.265 (0.045) & 0.525 (0.049) & 0.524 (0.068) & 0.526 (0.132) & 0.179 (0.029) \\*
        \textbf{Gradient Boosting Classifier}    & 0.258 (0.045) & 0.515 (0.053) & 0.525 (0.056) & 0.500 (0.102) & 0.175 (0.032) \\*
        \textbf{Ada Boost classifier}            & 0.264 (0.042) & 0.522 (0.049) & 0.531 (0.056) & 0.509 (0.105) & 0.179 (0.031) \\*
        \textbf{Gaussian Naieve Bayes}           & 0.254 (0.056) & 0.520 (0.052) & 0.562 (0.068) & 0.457 (0.126) & 0.178 (0.040) \\*
        \textbf{Logistic Regression}             & 0.258 (0.045) & 0.512 (0.052) & 0.504 (0.055) & 0.524 (0.109) & 0.172 (0.029) \\*
        \textbf{Quadratic Discriminant Analysis} & 0.252 (0.073) & 0.519 (0.073) & 0.568 (0.069) & 0.446 (0.153) & 0.177 (0.050) \\\toprule
        \caption[Cross-validation results for combinations of classification modelling techniques]{Cross-Validation results for each combination of data preparation/label weighting/modelling technique tested during the classification modelling. Figures in brackets are the standard deviations for the relevant statistic.}
        \label{tab:cv-results}
\end{longtable}
}

\clearpage
